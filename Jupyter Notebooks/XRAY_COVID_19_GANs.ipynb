{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baW9CuSpaGjF",
        "outputId": "93f21bcd-b6b2-44e9-ec6d-d0825fbb5faa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            " COVID\t\t\t      Normal.metadata.xlsx\n",
            " COVID.metadata.xlsx\t      README.md.txt\n",
            " Lung_Opacity.metadata.xlsx  'Viral Pneumonia'\n",
            " Normal\t\t\t     'Viral Pneumonia.metadata.xlsx'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n",
            "Requirement already satisfied: install in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.8/dist-packages (0.19.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls \"/content/gdrive/My Drive/COVID-19_Radiography_Dataset\"\n",
        "!pip install pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BPzxJn1aJAb"
      },
      "outputs": [],
      "source": [
        "import keras \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from enum import Enum\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "# for reproducibility - ref https://machinelearningmastery.com/reproducible-results-neural-networks-keras/ and https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "np.random.seed(9)\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "# loading data from gdrive\n",
        "chest_xray_dataset = os.path.abspath(\"/content/gdrive/My Drive/COVID 19 CHEST XRAY/images\")\n",
        "chest_xray_dataset_annotations = os.path.abspath(\"/content/gdrive/My Drive/COVID 19 CHEST XRAY/metadata.csv\")\n",
        "radiography_dataset = os.path.abspath(\"/content/gdrive/My Drive/COVID-19_Radiography_Dataset/\")\n",
        "xray_covid19_dataset = os.path.abspath(\"/content/gdrive/My Drive/xray_dataset_covid19/\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "c0jJGWTaaLaF",
        "outputId": "7965bb63-c9ba-4868-d986-ec49abde2397"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@markdown #**Anti-Disconnect for Google Colab**\n",
        "#@markdown ## Run this to stop it from disconnecting automatically \n",
        "#@markdown  **(It will anyhow disconnect after 6 - 12 hrs for using the free version of Colab.)**\n",
        "#@markdown  *(Colab Pro users will get about 24 hrs usage time)*\n",
        "#@markdown ---\n",
        "# taken from https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP_%28z%2Bquantize_method_with_augmentations%2C_user_friendly_interface%29.ipynb#scrollTo=XHyPd4oxVp_l stops colab disconnecting\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "IPython.display.Javascript(js_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgL0MXAMaVjv"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBjOipffaWQ0",
        "outputId": "891a2441-633e-4a1e-c22a-432f2cb61817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 94 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (256, 256)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    xray_covid19_dataset + '/Normal/', label_mode=None, image_size=image_size, batch_size=2,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR9rbMroaXyo",
        "outputId": "0c868b43-6174-49bb-baae-984d56953fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_16 (Conv2D)          (None, 128, 128, 64)      3136      \n",
            "                                                                 \n",
            " re_lu_40 (ReLU)             (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 64, 64, 128)       131200    \n",
            "                                                                 \n",
            " re_lu_41 (ReLU)             (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 32, 32, 128)       262272    \n",
            "                                                                 \n",
            " re_lu_42 (ReLU)             (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 131072)            0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 131072)            0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 131073    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 527,681\n",
            "Trainable params: 527,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, 2, 2, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_28 (Conv2D  (None, 4, 4, 128)        262272    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " re_lu_43 (ReLU)             (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_29 (Conv2D  (None, 8, 8, 256)        524544    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " re_lu_44 (ReLU)             (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_30 (Conv2D  (None, 16, 16, 512)      2097664   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " re_lu_45 (ReLU)             (None, 16, 16, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_31 (Conv2D  (None, 32, 32, 1024)     8389632   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " re_lu_46 (ReLU)             (None, 32, 32, 1024)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_32 (Conv2D  (None, 64, 64, 1024)     16778240  \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " re_lu_47 (ReLU)             (None, 64, 64, 1024)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_33 (Conv2D  (None, 128, 128, 1024)   16778240  \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " re_lu_48 (ReLU)             (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_transpose_34 (Conv2D  (None, 256, 256, 1024)   16778240  \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " re_lu_49 (ReLU)             (None, 256, 256, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 256, 256, 3)       27651     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,768,067\n",
            "Trainable params: 61,768,067\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(256, 256, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(2 * 2 * 128),\n",
        "        layers.Reshape((2, 2, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "             layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "             layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(3, kernel_size=3, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSIwmnOVaZp3"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_RADIOLOGY_Pneumonia')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c_vNt8KabMm"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_NORMAL' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Y7n5tJacnv",
        "outputId": "a51be7b6-8c09-4974-982a-988f17c534c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/47\n",
            "1/1 [==============================] - 331s 331s/step - d_loss: 0.6892 - g_loss: 0.8417\n",
            "Epoch 2/47\n",
            "1/1 [==============================] - 315s 315s/step - d_loss: 0.4975 - g_loss: 6.4788\n",
            "Epoch 3/47\n",
            "1/1 [==============================] - 320s 320s/step - d_loss: 1.2337 - g_loss: 0.4354\n",
            "Epoch 4/47\n",
            "1/1 [==============================] - 311s 311s/step - d_loss: 0.7204 - g_loss: 0.4399\n",
            "Epoch 5/47\n",
            "1/1 [==============================] - 313s 313s/step - d_loss: 0.9063 - g_loss: 1.3169\n",
            "Epoch 6/47\n",
            "1/1 [==============================] - 304s 304s/step - d_loss: 0.2555 - g_loss: 2.7727\n",
            "Epoch 7/47\n",
            "1/1 [==============================] - 325s 325s/step - d_loss: 0.3397 - g_loss: 3.6949\n",
            "Epoch 8/47\n",
            "1/1 [==============================] - 314s 314s/step - d_loss: 0.2077 - g_loss: 2.8669\n",
            "Epoch 9/47\n",
            "1/1 [==============================] - 308s 308s/step - d_loss: 0.4385 - g_loss: 1.5864\n",
            "Epoch 10/47\n",
            "1/1 [==============================] - 314s 314s/step - d_loss: 0.4918 - g_loss: 2.8233\n",
            "Epoch 11/47\n",
            "1/1 [==============================] - 308s 308s/step - d_loss: 0.8116 - g_loss: 1.5259\n",
            "Epoch 12/47\n",
            "1/1 [==============================] - 314s 314s/step - d_loss: 0.3687 - g_loss: 0.5005\n",
            "Epoch 13/47\n",
            "1/1 [==============================] - 304s 304s/step - d_loss: 0.5549 - g_loss: 0.7096\n",
            "Epoch 14/47\n",
            "1/1 [==============================] - 309s 309s/step - d_loss: 0.4143 - g_loss: 1.9042\n",
            "Epoch 15/47\n",
            "1/1 [==============================] - 302s 302s/step - d_loss: 0.1486 - g_loss: 3.9860\n",
            "Epoch 16/47\n",
            "1/1 [==============================] - 307s 307s/step - d_loss: 0.2880 - g_loss: 4.4192\n",
            "Epoch 17/47\n",
            "1/1 [==============================] - 302s 302s/step - d_loss: 0.1226 - g_loss: 3.8564\n",
            "Epoch 18/47\n",
            "1/1 [==============================] - 306s 306s/step - d_loss: 0.3439 - g_loss: 1.1435\n",
            "Epoch 19/47\n",
            "1/1 [==============================] - 302s 302s/step - d_loss: 0.2800 - g_loss: 0.6716\n",
            "Epoch 20/47\n",
            "1/1 [==============================] - 306s 306s/step - d_loss: 0.4531 - g_loss: 2.5928\n",
            "Epoch 21/47\n",
            "1/1 [==============================] - 304s 304s/step - d_loss: 0.1354 - g_loss: 6.2467\n",
            "Epoch 22/47\n",
            "1/1 [==============================] - 306s 306s/step - d_loss: 0.0761 - g_loss: 10.2595\n",
            "Epoch 23/47\n",
            "1/1 [==============================] - 302s 302s/step - d_loss: 1.4516 - g_loss: 5.3001\n",
            "Epoch 24/47\n",
            "1/1 [==============================] - 307s 307s/step - d_loss: 0.0741 - g_loss: 1.9408\n",
            "Epoch 25/47\n",
            "1/1 [==============================] - 304s 304s/step - d_loss: 0.1702 - g_loss: 0.6208\n",
            "Epoch 26/47\n",
            "1/1 [==============================] - 306s 306s/step - d_loss: 0.6367 - g_loss: 0.6653\n",
            "Epoch 27/47\n",
            "1/1 [==============================] - 304s 304s/step - d_loss: 0.5699 - g_loss: 1.0287\n",
            "Epoch 28/47\n",
            "1/1 [==============================] - 306s 306s/step - d_loss: 0.5325 - g_loss: 1.1925\n",
            "Epoch 29/47\n",
            "1/1 [==============================] - 306s 306s/step - d_loss: 0.5585 - g_loss: 0.9572\n",
            "Epoch 30/47\n",
            "1/1 [==============================] - 307s 307s/step - d_loss: 0.5338 - g_loss: 0.7700\n",
            "Epoch 31/47\n",
            "1/1 [==============================] - 301s 301s/step - d_loss: 0.7723 - g_loss: 0.8293\n",
            "Epoch 32/47\n",
            "1/1 [==============================] - 309s 309s/step - d_loss: 0.5813 - g_loss: 1.0272\n",
            "Epoch 33/47\n",
            "1/1 [==============================] - 304s 304s/step - d_loss: 0.6877 - g_loss: 1.3115\n",
            "Epoch 34/47\n",
            "1/1 [==============================] - 308s 308s/step - d_loss: 0.5962 - g_loss: 1.7240\n",
            "Epoch 35/47\n",
            "1/1 [==============================] - 304s 304s/step - d_loss: 0.4593 - g_loss: 2.3565\n",
            "Epoch 36/47\n",
            "1/1 [==============================] - 311s 311s/step - d_loss: 0.3470 - g_loss: 3.2180\n",
            "Epoch 37/47\n",
            "1/1 [==============================] - 304s 304s/step - d_loss: 0.3335 - g_loss: 4.1846\n",
            "Epoch 38/47\n",
            "1/1 [==============================] - 306s 306s/step - d_loss: 0.2430 - g_loss: 5.2560\n",
            "Epoch 39/47\n",
            "1/1 [==============================] - 304s 304s/step - d_loss: 0.1874 - g_loss: 6.6557\n",
            "Epoch 40/47\n",
            "1/1 [==============================] - 307s 307s/step - d_loss: 0.0927 - g_loss: 8.7990\n",
            "Epoch 41/47\n",
            "1/1 [==============================] - 309s 309s/step - d_loss: -0.0139 - g_loss: 11.2193\n",
            "Epoch 42/47\n",
            "1/1 [==============================] - 308s 308s/step - d_loss: -0.0436 - g_loss: 17.8422\n",
            "Epoch 43/47\n",
            "1/1 [==============================] - 304s 304s/step - d_loss: -0.3037 - g_loss: 26.1632\n",
            "Epoch 44/47\n",
            "1/1 [==============================] - 311s 311s/step - d_loss: -0.2060 - g_loss: 36.0473\n",
            "Epoch 45/47\n",
            "1/1 [==============================] - 306s 306s/step - d_loss: -0.3501 - g_loss: 54.7981\n",
            "Epoch 46/47\n",
            "1/1 [==============================] - 309s 309s/step - d_loss: -0.7407 - g_loss: 69.9323\n",
            "Epoch 47/47\n",
            "1/1 [==============================] - ETA: 0s - d_loss: -0.8237 - g_loss: 62.3554"
          ]
        }
      ],
      "source": [
        "epochs = 47  # In practice, use ~100 epochs\n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(epsilon=1e-8),\n",
        "    g_optimizer=keras.optimizers.Adam(epsilon=1e-8),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs,steps_per_epoch=1, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        " \n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_NORMALModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_NORMALModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOMuK0OGaeOq",
        "outputId": "82705145-22b8-489e-8629-4a4f05387d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 94 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (64, 64)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    xray_covid19_dataset + '/Pneumonia/', label_mode=None, image_size=image_size, batch_size=2,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_qYPugwaf1Q",
        "outputId": "ac46e5ed-a896-4ce4-fe5a-2704f53ddac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_120 (Conv2D)         (None, 32, 32, 64)        3136      \n",
            "                                                                 \n",
            " re_lu_253 (ReLU)            (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_121 (Conv2D)         (None, 16, 16, 128)       131200    \n",
            "                                                                 \n",
            " re_lu_254 (ReLU)            (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_122 (Conv2D)         (None, 8, 8, 128)         262272    \n",
            "                                                                 \n",
            " re_lu_255 (ReLU)            (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten_30 (Flatten)        (None, 8192)              0         \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 1)                 8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 404,801\n",
            "Trainable params: 404,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_61 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " reshape_30 (Reshape)        (None, 1, 1, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_163 (Conv2  (None, 2, 2, 128)        262272    \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " re_lu_256 (ReLU)            (None, 2, 2, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_164 (Conv2  (None, 4, 4, 256)        524544    \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " re_lu_257 (ReLU)            (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_165 (Conv2  (None, 8, 8, 512)        2097664   \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " re_lu_258 (ReLU)            (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_166 (Conv2  (None, 16, 16, 512)      4194816   \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " re_lu_259 (ReLU)            (None, 16, 16, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_167 (Conv2  (None, 32, 32, 512)      4194816   \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " re_lu_260 (ReLU)            (None, 32, 32, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_168 (Conv2  (None, 64, 64, 1024)     8389632   \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " re_lu_261 (ReLU)            (None, 64, 64, 1024)      0         \n",
            "                                                                 \n",
            " conv2d_123 (Conv2D)         (None, 64, 64, 3)         27651     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,707,907\n",
            "Trainable params: 19,707,907\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(1 * 1 * 128),\n",
        "        layers.Reshape((1, 1, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(3, kernel_size=3, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxeCMnOZag9s"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_RADIOLOGY_Pneumonia')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W21Nv6bzaiZZ"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_PNEUMONIA' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ck_mbZfajwW",
        "outputId": "14061fcf-a9dd-4a1d-ab2a-d440d765c9a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/47\n",
            "1/1 [==============================] - 17s 17s/step - d_loss: 0.6679 - g_loss: 0.5334\n",
            "Epoch 2/47\n",
            "1/1 [==============================] - 14s 14s/step - d_loss: 0.6759 - g_loss: 0.9541\n",
            "Epoch 3/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.5918 - g_loss: 1.2054\n",
            "Epoch 4/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.5569 - g_loss: 1.0290\n",
            "Epoch 5/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.5501 - g_loss: 0.5420\n",
            "Epoch 6/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.5484 - g_loss: 0.4681\n",
            "Epoch 7/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.9397 - g_loss: 0.3734\n",
            "Epoch 8/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 1.0388 - g_loss: 0.4794\n",
            "Epoch 9/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 1.0379 - g_loss: 0.5872\n",
            "Epoch 10/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.9804 - g_loss: 0.6362\n",
            "Epoch 11/47\n",
            "1/1 [==============================] - 14s 14s/step - d_loss: 0.8160 - g_loss: 0.7208\n",
            "Epoch 12/47\n",
            "1/1 [==============================] - 14s 14s/step - d_loss: 0.6456 - g_loss: 0.7539\n",
            "Epoch 13/47\n",
            "1/1 [==============================] - 14s 14s/step - d_loss: 0.6222 - g_loss: 0.7707\n",
            "Epoch 14/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.6270 - g_loss: 0.7769\n",
            "Epoch 15/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.5700 - g_loss: 0.7629\n",
            "Epoch 16/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.6025 - g_loss: 0.6664\n",
            "Epoch 17/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.6713 - g_loss: 0.5259\n",
            "Epoch 18/47\n",
            "1/1 [==============================] - 20s 20s/step - d_loss: 0.7150 - g_loss: 0.6494\n",
            "Epoch 19/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.6131 - g_loss: 0.9988\n",
            "Epoch 20/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.5070 - g_loss: 1.4948\n",
            "Epoch 21/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.4371 - g_loss: 1.8650\n",
            "Epoch 22/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.4846 - g_loss: 1.5455\n",
            "Epoch 23/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.6722 - g_loss: 1.0597\n",
            "Epoch 24/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.8375 - g_loss: 1.4779\n",
            "Epoch 25/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 1.0104 - g_loss: 0.9376\n",
            "Epoch 26/47\n",
            "1/1 [==============================] - 14s 14s/step - d_loss: 1.0772 - g_loss: 0.4385\n",
            "Epoch 27/47\n",
            "1/1 [==============================] - 14s 14s/step - d_loss: 0.9246 - g_loss: 0.5061\n",
            "Epoch 28/47\n",
            "1/1 [==============================] - 14s 14s/step - d_loss: 0.8082 - g_loss: 0.9511\n",
            "Epoch 29/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.5814 - g_loss: 1.7666\n",
            "Epoch 30/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.5341 - g_loss: 2.1604\n",
            "Epoch 31/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.5003 - g_loss: 1.8278\n",
            "Epoch 32/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.4036 - g_loss: 1.2936\n",
            "Epoch 33/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.3810 - g_loss: 0.9104\n",
            "Epoch 34/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.4286 - g_loss: 1.4006\n",
            "Epoch 35/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.3174 - g_loss: 2.3041\n",
            "Epoch 36/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.2485 - g_loss: 2.6483\n",
            "Epoch 37/47\n",
            "1/1 [==============================] - 14s 14s/step - d_loss: 0.1447 - g_loss: 2.9223\n",
            "Epoch 38/47\n",
            "1/1 [==============================] - 14s 14s/step - d_loss: 0.6191 - g_loss: 1.4477\n",
            "Epoch 39/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.1718 - g_loss: 0.8549\n",
            "Epoch 40/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.3403 - g_loss: 1.2923\n",
            "Epoch 41/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.2267 - g_loss: 2.7302\n",
            "Epoch 42/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.0711 - g_loss: 4.6141\n",
            "Epoch 43/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.0254 - g_loss: 6.0637\n",
            "Epoch 44/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.1257 - g_loss: 6.3324\n",
            "Epoch 45/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.0592 - g_loss: 5.5348\n",
            "Epoch 46/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.2122 - g_loss: 3.0685\n",
            "Epoch 47/47\n",
            "1/1 [==============================] - 13s 13s/step - d_loss: 0.0699 - g_loss: 1.5260\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "epochs = 47  # In practice, use ~100 epochs\n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(epsilon=1e-8),\n",
        "    g_optimizer=keras.optimizers.Adam(epsilon=1e-8),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs,steps_per_epoch=1, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        " \n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_PNEUMONIAModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_PNEUMONIAModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsBs6whoI6mq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}