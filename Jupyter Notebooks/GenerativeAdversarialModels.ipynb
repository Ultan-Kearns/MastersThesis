{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-KAqwHSkFqu",
        "outputId": "f1da77c8-e322-496c-c16f-9e6b94c539c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            " COVID\t\t\t      Normal.metadata.xlsx\n",
            " COVID.metadata.xlsx\t      README.md.txt\n",
            " Lung_Opacity.metadata.xlsx  'Viral Pneumonia'\n",
            " Normal\t\t\t     'Viral Pneumonia.metadata.xlsx'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n",
            "Requirement already satisfied: install in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.8/dist-packages (0.19.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls \"/content/gdrive/My Drive/COVID-19_Radiography_Dataset\"\n",
        "!pip install pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u4JUZUnIjArU"
      },
      "outputs": [],
      "source": [
        "import keras \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from enum import Enum\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "# for reproducibility - ref https://machinelearningmastery.com/reproducible-results-neural-networks-keras/ and https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "np.random.seed(9)\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "# loading data from gdrive\n",
        "chest_xray_dataset = os.path.abspath(\"/content/gdrive/My Drive/COVID 19 CHEST XRAY/images\")\n",
        "chest_xray_dataset_annotations = os.path.abspath(\"/content/gdrive/My Drive/COVID 19 CHEST XRAY/metadata.csv\")\n",
        "radiography_dataset = os.path.abspath(\"/content/gdrive/My Drive/COVID-19_Radiography_Dataset/\")\n",
        "xray_covid19_dataset = os.path.abspath(\"/content/gdrive/My Drive/xray_dataset_covid19/\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Gy_GXINDq0-8",
        "outputId": "446f51fe-862e-486b-9ca1-b296d8daf768"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#@markdown #**Anti-Disconnect for Google Colab**\n",
        "#@markdown ## Run this to stop it from disconnecting automatically \n",
        "#@markdown  **(It will anyhow disconnect after 6 - 12 hrs for using the free version of Colab.)**\n",
        "#@markdown  *(Colab Pro users will get about 24 hrs usage time)*\n",
        "#@markdown ---\n",
        "# taken from https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP_%28z%2Bquantize_method_with_augmentations%2C_user_friendly_interface%29.ipynb#scrollTo=XHyPd4oxVp_l stops colab disconnecting\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "IPython.display.Javascript(js_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "qSpMb9aHjQT4",
        "outputId": "990d4f05-c57a-442e-c4cc-08d00ac3e719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7232 files belonging to 1 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-76ac95d0f213>:9: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  ax = plt.subplot(2, 2, i + 1)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (64, 64)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "dataset = keras.utils.image_dataset_from_directory(\n",
        "    radiography_dataset + '/COVID', label_mode=None, image_size=image_size, batch_size=32\n",
        ")\n",
        "for images in dataset.take(4):\n",
        "  for i in range(1):\n",
        "      ax = plt.subplot(2, 2, i + 1)\n",
        "      plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "      plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TndIIeA7R5IK"
      },
      "source": [
        "# Code taken from ref https://keras.io/examples/generative/vae/ and refactored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NKKXZDF3P5o"
      },
      "outputs": [],
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRqfi82oD8q3",
        "outputId": "a1ca30b8-366a-4454-9d9c-29f769d5f36d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 32)   896         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 16, 16, 64)   18496       ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 16384)        0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           262160      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 2)            34          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 2)            34          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 281,620\n",
            "Trainable params: 281,620\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_dim = 2\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(64, 64, 3))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl-lUO8PnvxI",
        "outputId": "c61dab2a-6ef0-470f-d53c-41a45cfe97d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              3072      \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 4, 4, 64)          0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 8, 8, 512)        295424    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 16, 16, 512)      2359808   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 32, 32, 1024)     4719616   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 64, 64, 2048)     18876416  \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 64, 64, 1)        18433     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26,272,769\n",
            "Trainable params: 26,272,769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(4 * 4 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((4, 4, 64))(x)\n",
        "x = layers.Conv2DTranspose(512, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(512, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(1024, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(2048, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crjHnHiplTek"
      },
      "outputs": [],
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.mae(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "    def get_vae():\n",
        "      return VAE(name='VAERADIOLOGYCOVID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "15ivoe-vnzsC",
        "outputId": "14dfd992-cbe7-49f2-acb9-992438cddc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/226\n",
            "1/1 [==============================] - 46s 46s/step - loss: 431955.8125 - reconstruction_loss: 431955.8125 - kl_loss: 1.0532e-04\n",
            "Epoch 2/226\n",
            "1/1 [==============================] - 45s 45s/step - loss: 404551.1562 - reconstruction_loss: 404551.0625 - kl_loss: 0.0935\n",
            "Epoch 3/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 444348.8750 - reconstruction_loss: 444348.8750 - kl_loss: 7.8321e-05\n",
            "Epoch 4/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 388715.4375 - reconstruction_loss: 388715.4375 - kl_loss: 7.0125e-05\n",
            "Epoch 5/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 418791.3125 - reconstruction_loss: 418770.9062 - kl_loss: 20.3913\n",
            "Epoch 6/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 436191.3750 - reconstruction_loss: 436191.3750 - kl_loss: 6.2019e-05\n",
            "Epoch 7/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 342143.1875 - reconstruction_loss: 342143.1875 - kl_loss: 6.0767e-05\n",
            "Epoch 8/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 371388.3750 - reconstruction_loss: 371388.3750 - kl_loss: 5.9068e-05\n",
            "Epoch 9/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 411286.9062 - reconstruction_loss: 411286.9062 - kl_loss: 5.7161e-05\n",
            "Epoch 10/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 372692.4375 - reconstruction_loss: 372692.4375 - kl_loss: 5.4955e-05\n",
            "Epoch 11/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 391460.6875 - reconstruction_loss: 391460.6875 - kl_loss: 5.2691e-05\n",
            "Epoch 12/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 375268.0312 - reconstruction_loss: 375268.0312 - kl_loss: 5.0366e-05\n",
            "Epoch 13/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 393698.1875 - reconstruction_loss: 393698.1875 - kl_loss: 4.8101e-05\n",
            "Epoch 14/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 455202.1875 - reconstruction_loss: 455202.1875 - kl_loss: 4.5896e-05\n",
            "Epoch 15/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 443641.5625 - reconstruction_loss: 443641.5625 - kl_loss: 4.3690e-05\n",
            "Epoch 16/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 424684.2812 - reconstruction_loss: 424684.2812 - kl_loss: 4.1664e-05\n",
            "Epoch 17/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 335445.2188 - reconstruction_loss: 335445.2188 - kl_loss: 3.9697e-05\n",
            "Epoch 18/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 422947.1875 - reconstruction_loss: 422947.1875 - kl_loss: 3.7968e-05\n",
            "Epoch 19/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 398512.1250 - reconstruction_loss: 398512.1250 - kl_loss: 3.6240e-05\n",
            "Epoch 20/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 466311.8438 - reconstruction_loss: 466311.8438 - kl_loss: 3.4809e-05\n",
            "Epoch 21/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 386559.1250 - reconstruction_loss: 386559.1250 - kl_loss: 3.3438e-05\n",
            "Epoch 22/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 412530.6562 - reconstruction_loss: 412530.6562 - kl_loss: 3.2306e-05\n",
            "Epoch 23/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 439084.1250 - reconstruction_loss: 439084.1250 - kl_loss: 3.1352e-05\n",
            "Epoch 24/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 412194.3750 - reconstruction_loss: 412194.3750 - kl_loss: 3.0458e-05\n",
            "Epoch 25/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 395803.0625 - reconstruction_loss: 395803.0625 - kl_loss: 2.9802e-05\n",
            "Epoch 26/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 495234.5000 - reconstruction_loss: 495234.5000 - kl_loss: 2.9266e-05\n",
            "Epoch 27/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 409325.8125 - reconstruction_loss: 409325.8125 - kl_loss: 2.8789e-05\n",
            "Epoch 28/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 411134.7812 - reconstruction_loss: 411134.7812 - kl_loss: 2.8491e-05\n",
            "Epoch 29/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 423047.5625 - reconstruction_loss: 423047.5625 - kl_loss: 2.8253e-05\n",
            "Epoch 30/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 402695.4062 - reconstruction_loss: 402695.4062 - kl_loss: 2.8044e-05\n",
            "Epoch 31/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 416457.5000 - reconstruction_loss: 416457.5000 - kl_loss: 2.7895e-05\n",
            "Epoch 32/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 407469.0000 - reconstruction_loss: 407469.0000 - kl_loss: 2.7835e-05\n",
            "Epoch 33/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 376639.2500 - reconstruction_loss: 376639.2500 - kl_loss: 2.7716e-05\n",
            "Epoch 34/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 421214.0938 - reconstruction_loss: 421214.0938 - kl_loss: 2.7657e-05\n",
            "Epoch 35/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 449859.6562 - reconstruction_loss: 449859.6562 - kl_loss: 2.7597e-05\n",
            "Epoch 36/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 346116.1875 - reconstruction_loss: 346116.1875 - kl_loss: 2.7448e-05\n",
            "Epoch 37/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 445205.1250 - reconstruction_loss: 445205.1250 - kl_loss: 2.7329e-05\n",
            "Epoch 38/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 415843.4688 - reconstruction_loss: 415843.4688 - kl_loss: 2.7090e-05\n",
            "Epoch 39/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 439222.7500 - reconstruction_loss: 439222.7500 - kl_loss: 2.6911e-05\n",
            "Epoch 40/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 432081.0625 - reconstruction_loss: 432081.0625 - kl_loss: 2.6643e-05\n",
            "Epoch 41/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 417743.4062 - reconstruction_loss: 417743.4062 - kl_loss: 2.6315e-05\n",
            "Epoch 42/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 422507.2500 - reconstruction_loss: 422507.2500 - kl_loss: 2.5988e-05\n",
            "Epoch 43/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 378951.9688 - reconstruction_loss: 378951.9688 - kl_loss: 2.5570e-05\n",
            "Epoch 44/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 420072.6562 - reconstruction_loss: 420072.6562 - kl_loss: 2.5123e-05\n",
            "Epoch 45/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 388743.4375 - reconstruction_loss: 388743.4375 - kl_loss: 2.4676e-05\n",
            "Epoch 46/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 402726.4688 - reconstruction_loss: 402726.4688 - kl_loss: 2.4229e-05\n",
            "Epoch 47/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 419438.9062 - reconstruction_loss: 419438.9062 - kl_loss: 2.3693e-05\n",
            "Epoch 48/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 426509.3438 - reconstruction_loss: 426509.3438 - kl_loss: 2.3127e-05\n",
            "Epoch 49/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 423037.7500 - reconstruction_loss: 423037.7500 - kl_loss: 2.2590e-05\n",
            "Epoch 50/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 401527.0312 - reconstruction_loss: 401527.0312 - kl_loss: 2.2084e-05\n",
            "Epoch 51/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 448661.4375 - reconstruction_loss: 448661.4375 - kl_loss: 2.1517e-05\n",
            "Epoch 52/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 415557.4375 - reconstruction_loss: 415557.4375 - kl_loss: 2.1011e-05\n",
            "Epoch 53/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 443959.6250 - reconstruction_loss: 443959.6250 - kl_loss: 2.0474e-05\n",
            "Epoch 54/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 408609.6875 - reconstruction_loss: 408609.6875 - kl_loss: 1.9968e-05\n",
            "Epoch 55/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 431356.7812 - reconstruction_loss: 431356.7812 - kl_loss: 1.9461e-05\n",
            "Epoch 56/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 426154.6875 - reconstruction_loss: 426154.6875 - kl_loss: 1.8984e-05\n",
            "Epoch 57/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 377666.8125 - reconstruction_loss: 377666.8125 - kl_loss: 1.8507e-05\n",
            "Epoch 58/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 405964.1562 - reconstruction_loss: 405964.1562 - kl_loss: 1.8030e-05\n",
            "Epoch 59/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 394668.1562 - reconstruction_loss: 394668.1562 - kl_loss: 1.7643e-05\n",
            "Epoch 60/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 342498.9375 - reconstruction_loss: 342498.9375 - kl_loss: 1.7196e-05\n",
            "Epoch 61/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 446746.0625 - reconstruction_loss: 446746.0625 - kl_loss: 1.6779e-05\n",
            "Epoch 62/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 351808.8125 - reconstruction_loss: 351808.8125 - kl_loss: 1.6361e-05\n",
            "Epoch 63/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 461433.1875 - reconstruction_loss: 461433.1875 - kl_loss: 1.6004e-05\n",
            "Epoch 64/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 445252.7500 - reconstruction_loss: 445252.7500 - kl_loss: 1.5646e-05\n",
            "Epoch 65/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 431003.0000 - reconstruction_loss: 431003.0000 - kl_loss: 1.5259e-05\n",
            "Epoch 66/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 490823.3438 - reconstruction_loss: 490823.3438 - kl_loss: 1.4961e-05\n",
            "Epoch 67/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 429493.3750 - reconstruction_loss: 429493.3750 - kl_loss: 1.4603e-05\n",
            "Epoch 68/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 446507.5625 - reconstruction_loss: 446507.5625 - kl_loss: 1.4246e-05\n",
            "Epoch 69/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 391364.5312 - reconstruction_loss: 391364.5312 - kl_loss: 1.3888e-05\n",
            "Epoch 70/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 463047.6875 - reconstruction_loss: 463047.6875 - kl_loss: 1.3590e-05\n",
            "Epoch 71/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 440449.7812 - reconstruction_loss: 440449.7812 - kl_loss: 1.3232e-05\n",
            "Epoch 72/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 383302.3438 - reconstruction_loss: 383302.3438 - kl_loss: 1.2934e-05\n",
            "Epoch 73/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 410275.2812 - reconstruction_loss: 410275.2812 - kl_loss: 1.2636e-05\n",
            "Epoch 74/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 391642.7500 - reconstruction_loss: 391642.7500 - kl_loss: 1.2338e-05\n",
            "Epoch 75/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 434212.6250 - reconstruction_loss: 434212.6250 - kl_loss: 1.2040e-05\n",
            "Epoch 76/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 439954.4688 - reconstruction_loss: 439954.4688 - kl_loss: 1.1742e-05\n",
            "Epoch 77/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 419389.6875 - reconstruction_loss: 419389.6875 - kl_loss: 1.1444e-05\n",
            "Epoch 78/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 394465.0312 - reconstruction_loss: 394465.0312 - kl_loss: 1.1206e-05\n",
            "Epoch 79/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 400807.3750 - reconstruction_loss: 400807.3750 - kl_loss: 1.0908e-05\n",
            "Epoch 80/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 385498.6875 - reconstruction_loss: 385498.6875 - kl_loss: 1.0610e-05\n",
            "Epoch 81/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 438177.1562 - reconstruction_loss: 438177.1562 - kl_loss: 1.0312e-05\n",
            "Epoch 82/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 397600.9375 - reconstruction_loss: 397600.9375 - kl_loss: 1.0073e-05\n",
            "Epoch 83/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 394257.3125 - reconstruction_loss: 394257.3125 - kl_loss: 9.7752e-06\n",
            "Epoch 84/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 461257.6250 - reconstruction_loss: 461257.6250 - kl_loss: 9.5367e-06\n",
            "Epoch 85/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 443010.0000 - reconstruction_loss: 443010.0000 - kl_loss: 9.3579e-06\n",
            "Epoch 86/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 404381.7500 - reconstruction_loss: 404381.7500 - kl_loss: 9.1195e-06\n",
            "Epoch 87/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 491670.0312 - reconstruction_loss: 491670.0312 - kl_loss: 8.8811e-06\n",
            "Epoch 88/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 357120.8438 - reconstruction_loss: 357120.8438 - kl_loss: 8.7023e-06\n",
            "Epoch 89/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 379669.3125 - reconstruction_loss: 379669.3125 - kl_loss: 8.4639e-06\n",
            "Epoch 90/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 395927.1875 - reconstruction_loss: 395927.1875 - kl_loss: 8.2552e-06\n",
            "Epoch 91/226\n",
            "1/1 [==============================] - 45s 45s/step - loss: 365663.3125 - reconstruction_loss: 365663.3125 - kl_loss: 8.0466e-06\n",
            "Epoch 92/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 426324.3438 - reconstruction_loss: 426324.3438 - kl_loss: 7.8678e-06\n",
            "Epoch 93/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 358330.5625 - reconstruction_loss: 358330.5625 - kl_loss: 7.6592e-06\n",
            "Epoch 94/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 401073.5312 - reconstruction_loss: 401073.5312 - kl_loss: 7.4804e-06\n",
            "Epoch 95/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 336770.6562 - reconstruction_loss: 336770.6562 - kl_loss: 7.3016e-06\n",
            "Epoch 96/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 397909.3750 - reconstruction_loss: 397909.3750 - kl_loss: 7.0930e-06\n",
            "Epoch 97/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 433692.5312 - reconstruction_loss: 433692.5312 - kl_loss: 6.9439e-06\n",
            "Epoch 98/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 458728.4062 - reconstruction_loss: 458728.4062 - kl_loss: 6.7651e-06\n",
            "Epoch 99/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 426404.6875 - reconstruction_loss: 426404.6875 - kl_loss: 6.5863e-06\n",
            "Epoch 100/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 326340.8438 - reconstruction_loss: 326340.8438 - kl_loss: 6.4373e-06\n",
            "Epoch 101/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 388886.0000 - reconstruction_loss: 388886.0000 - kl_loss: 6.2585e-06\n",
            "Epoch 102/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 425816.7812 - reconstruction_loss: 425816.7812 - kl_loss: 6.1393e-06\n",
            "Epoch 103/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 408041.2188 - reconstruction_loss: 408041.2188 - kl_loss: 5.9903e-06\n",
            "Epoch 104/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 444956.1562 - reconstruction_loss: 444956.1562 - kl_loss: 5.8413e-06\n",
            "Epoch 105/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 467756.3125 - reconstruction_loss: 467756.3125 - kl_loss: 5.6922e-06\n",
            "Epoch 106/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 459045.3125 - reconstruction_loss: 459045.3125 - kl_loss: 5.5432e-06\n",
            "Epoch 107/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 387111.2500 - reconstruction_loss: 387111.2500 - kl_loss: 5.3942e-06\n",
            "Epoch 108/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 400801.3125 - reconstruction_loss: 400801.3125 - kl_loss: 5.3048e-06\n",
            "Epoch 109/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 421225.0312 - reconstruction_loss: 421225.0312 - kl_loss: 5.1558e-06\n",
            "Epoch 110/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 465776.3438 - reconstruction_loss: 465776.3438 - kl_loss: 5.0366e-06\n",
            "Epoch 111/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 438178.4375 - reconstruction_loss: 438178.4375 - kl_loss: 4.8876e-06\n",
            "Epoch 112/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 435391.9375 - reconstruction_loss: 435391.9375 - kl_loss: 4.7684e-06\n",
            "Epoch 113/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 450981.2500 - reconstruction_loss: 450981.2500 - kl_loss: 4.6790e-06\n",
            "Epoch 114/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 380323.3438 - reconstruction_loss: 380323.3438 - kl_loss: 4.5598e-06\n",
            "Epoch 115/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 420522.6250 - reconstruction_loss: 420522.6250 - kl_loss: 4.4703e-06\n",
            "Epoch 116/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 339311.5625 - reconstruction_loss: 339311.5625 - kl_loss: 4.2915e-06\n",
            "Epoch 117/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 409560.4375 - reconstruction_loss: 409560.4375 - kl_loss: 4.2319e-06\n",
            "Epoch 118/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 390715.6250 - reconstruction_loss: 390715.6250 - kl_loss: 4.1127e-06\n",
            "Epoch 119/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 445224.6562 - reconstruction_loss: 445224.6562 - kl_loss: 3.9935e-06\n",
            "Epoch 120/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 373180.2188 - reconstruction_loss: 373180.2188 - kl_loss: 3.9339e-06\n",
            "Epoch 121/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 392857.3750 - reconstruction_loss: 392857.3750 - kl_loss: 3.8147e-06\n",
            "Epoch 122/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 440602.9375 - reconstruction_loss: 440602.9375 - kl_loss: 3.7551e-06\n",
            "Epoch 123/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 457396.8125 - reconstruction_loss: 457396.8125 - kl_loss: 3.6359e-06\n",
            "Epoch 124/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 409462.4375 - reconstruction_loss: 409462.4375 - kl_loss: 3.5763e-06\n",
            "Epoch 125/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 452230.4375 - reconstruction_loss: 452230.4375 - kl_loss: 3.4571e-06\n",
            "Epoch 126/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 430511.6250 - reconstruction_loss: 430511.6250 - kl_loss: 3.3975e-06\n",
            "Epoch 127/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 402617.2500 - reconstruction_loss: 402617.2500 - kl_loss: 3.2187e-06\n",
            "Epoch 128/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 421922.3125 - reconstruction_loss: 421922.3125 - kl_loss: 3.1590e-06\n",
            "Epoch 129/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 377008.1875 - reconstruction_loss: 377008.1875 - kl_loss: 3.0994e-06\n",
            "Epoch 130/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 409618.4062 - reconstruction_loss: 409618.4062 - kl_loss: 2.9802e-06\n",
            "Epoch 131/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 469516.2500 - reconstruction_loss: 469516.2500 - kl_loss: 2.9206e-06\n",
            "Epoch 132/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 366043.6562 - reconstruction_loss: 366043.6562 - kl_loss: 2.8610e-06\n",
            "Epoch 133/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 393910.0000 - reconstruction_loss: 393910.0000 - kl_loss: 2.7716e-06\n",
            "Epoch 134/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 431364.1875 - reconstruction_loss: 431364.1875 - kl_loss: 2.7120e-06\n",
            "Epoch 135/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 399734.8750 - reconstruction_loss: 399734.8750 - kl_loss: 2.6524e-06\n",
            "Epoch 136/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 384544.6250 - reconstruction_loss: 384544.6250 - kl_loss: 2.5630e-06\n",
            "Epoch 137/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 430190.0625 - reconstruction_loss: 430190.0625 - kl_loss: 2.5034e-06\n",
            "Epoch 138/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 447919.8125 - reconstruction_loss: 447919.8125 - kl_loss: 2.4438e-06\n",
            "Epoch 139/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 392365.7500 - reconstruction_loss: 392365.7500 - kl_loss: 2.3842e-06\n",
            "Epoch 140/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 387223.5000 - reconstruction_loss: 387223.5000 - kl_loss: 2.3246e-06\n",
            "Epoch 141/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 342073.4375 - reconstruction_loss: 342073.4375 - kl_loss: 2.2650e-06\n",
            "Epoch 142/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 422054.2500 - reconstruction_loss: 422054.2500 - kl_loss: 2.2054e-06\n",
            "Epoch 143/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 419024.2812 - reconstruction_loss: 419024.2812 - kl_loss: 2.1458e-06\n",
            "Epoch 144/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 413971.9062 - reconstruction_loss: 413971.9062 - kl_loss: 2.0862e-06\n",
            "Epoch 145/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 363522.8125 - reconstruction_loss: 363522.8125 - kl_loss: 2.0266e-06\n",
            "Epoch 146/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 419327.5000 - reconstruction_loss: 419327.5000 - kl_loss: 1.9670e-06\n",
            "Epoch 147/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 450216.6875 - reconstruction_loss: 450216.6875 - kl_loss: 1.9073e-06\n",
            "Epoch 148/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 414318.0000 - reconstruction_loss: 414318.0000 - kl_loss: 1.8775e-06\n",
            "Epoch 149/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 351701.2188 - reconstruction_loss: 351701.2188 - kl_loss: 1.8179e-06\n",
            "Epoch 150/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 449763.3750 - reconstruction_loss: 449759.3125 - kl_loss: 4.0590\n",
            "Epoch 151/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 448217.6250 - reconstruction_loss: 448217.6250 - kl_loss: 1.4901e-06\n",
            "Epoch 152/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 423454.9375 - reconstruction_loss: 423454.9375 - kl_loss: 2.2054e-06\n",
            "Epoch 153/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 381627.5625 - reconstruction_loss: 381627.5625 - kl_loss: 3.5167e-06\n",
            "Epoch 154/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 383726.4062 - reconstruction_loss: 383726.4062 - kl_loss: 5.2452e-06\n",
            "Epoch 155/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 413280.4375 - reconstruction_loss: 413280.4375 - kl_loss: 7.2122e-06\n",
            "Epoch 156/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 394702.1562 - reconstruction_loss: 394702.1562 - kl_loss: 9.0599e-06\n",
            "Epoch 157/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 390819.9375 - reconstruction_loss: 390819.9375 - kl_loss: 1.0908e-05\n",
            "Epoch 158/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 430942.7188 - reconstruction_loss: 430942.7188 - kl_loss: 1.2517e-05\n",
            "Epoch 159/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 442136.9375 - reconstruction_loss: 442136.9375 - kl_loss: 1.3947e-05\n",
            "Epoch 160/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 493821.5625 - reconstruction_loss: 493821.5625 - kl_loss: 1.4961e-05\n",
            "Epoch 161/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 394589.6562 - reconstruction_loss: 394589.6562 - kl_loss: 1.5974e-05\n",
            "Epoch 162/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 427228.9375 - reconstruction_loss: 427228.9375 - kl_loss: 1.6570e-05\n",
            "Epoch 163/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 431166.0625 - reconstruction_loss: 431166.0625 - kl_loss: 1.6987e-05\n",
            "Epoch 164/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 412065.9375 - reconstruction_loss: 412065.9375 - kl_loss: 1.7166e-05\n",
            "Epoch 165/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 430596.4062 - reconstruction_loss: 430596.4062 - kl_loss: 1.7405e-05\n",
            "Epoch 166/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 407848.1250 - reconstruction_loss: 407848.1250 - kl_loss: 1.7405e-05\n",
            "Epoch 167/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 438255.6875 - reconstruction_loss: 438255.6875 - kl_loss: 1.7464e-05\n",
            "Epoch 168/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 413696.3125 - reconstruction_loss: 413696.3125 - kl_loss: 1.7345e-05\n",
            "Epoch 169/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 407706.5312 - reconstruction_loss: 407706.5312 - kl_loss: 1.7166e-05\n",
            "Epoch 170/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 346085.4062 - reconstruction_loss: 346085.4062 - kl_loss: 1.7047e-05\n",
            "Epoch 171/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 414556.7188 - reconstruction_loss: 414556.7188 - kl_loss: 1.6868e-05\n",
            "Epoch 172/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 393200.7812 - reconstruction_loss: 393200.7812 - kl_loss: 1.6689e-05\n",
            "Epoch 173/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 391040.5312 - reconstruction_loss: 391040.5312 - kl_loss: 1.6540e-05\n",
            "Epoch 174/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 368819.8125 - reconstruction_loss: 368819.8125 - kl_loss: 1.6302e-05\n",
            "Epoch 175/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 403087.2500 - reconstruction_loss: 403087.2500 - kl_loss: 1.6063e-05\n",
            "Epoch 176/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 432198.3750 - reconstruction_loss: 432198.3750 - kl_loss: 1.5765e-05\n",
            "Epoch 177/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 391461.8125 - reconstruction_loss: 391461.8125 - kl_loss: 1.5438e-05\n",
            "Epoch 178/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 382852.3438 - reconstruction_loss: 382852.3438 - kl_loss: 1.5080e-05\n",
            "Epoch 179/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 472823.4375 - reconstruction_loss: 472823.4375 - kl_loss: 1.4693e-05\n",
            "Epoch 180/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 403527.7500 - reconstruction_loss: 403527.7500 - kl_loss: 1.4275e-05\n",
            "Epoch 181/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 385530.3125 - reconstruction_loss: 385530.3125 - kl_loss: 1.3769e-05\n",
            "Epoch 182/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 335724.1875 - reconstruction_loss: 335724.1875 - kl_loss: 1.3322e-05\n",
            "Epoch 183/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 477815.3750 - reconstruction_loss: 477815.3750 - kl_loss: 1.2785e-05\n",
            "Epoch 184/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 388526.3438 - reconstruction_loss: 388526.3438 - kl_loss: 1.2219e-05\n",
            "Epoch 185/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 368140.8125 - reconstruction_loss: 368140.8125 - kl_loss: 1.1683e-05\n",
            "Epoch 186/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 407036.0625 - reconstruction_loss: 407036.0625 - kl_loss: 1.1116e-05\n",
            "Epoch 187/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 430566.6875 - reconstruction_loss: 430566.6875 - kl_loss: 1.0580e-05\n",
            "Epoch 188/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 375367.0000 - reconstruction_loss: 375367.0000 - kl_loss: 1.0043e-05\n",
            "Epoch 189/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 379536.2188 - reconstruction_loss: 379536.2188 - kl_loss: 9.5069e-06\n",
            "Epoch 190/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 404457.3438 - reconstruction_loss: 404457.3438 - kl_loss: 9.0599e-06\n",
            "Epoch 191/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 439637.0000 - reconstruction_loss: 439637.0000 - kl_loss: 8.5533e-06\n",
            "Epoch 192/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 450259.6250 - reconstruction_loss: 450259.6250 - kl_loss: 8.1062e-06\n",
            "Epoch 193/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 421862.1250 - reconstruction_loss: 421862.1250 - kl_loss: 7.6592e-06\n",
            "Epoch 194/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 396509.0000 - reconstruction_loss: 396509.0000 - kl_loss: 7.2420e-06\n",
            "Epoch 195/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 440827.6875 - reconstruction_loss: 440827.6875 - kl_loss: 6.8545e-06\n",
            "Epoch 196/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 453667.5000 - reconstruction_loss: 453667.5000 - kl_loss: 6.4969e-06\n",
            "Epoch 197/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 445160.2500 - reconstruction_loss: 445160.2500 - kl_loss: 6.1393e-06\n",
            "Epoch 198/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 450009.7500 - reconstruction_loss: 450009.7500 - kl_loss: 5.7817e-06\n",
            "Epoch 199/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 425252.7500 - reconstruction_loss: 425252.7500 - kl_loss: 5.5432e-06\n",
            "Epoch 200/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 434388.5625 - reconstruction_loss: 434388.5625 - kl_loss: 5.2452e-06\n",
            "Epoch 201/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 408752.9375 - reconstruction_loss: 408752.9375 - kl_loss: 4.9472e-06\n",
            "Epoch 202/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 403564.4062 - reconstruction_loss: 403564.4062 - kl_loss: 4.6492e-06\n",
            "Epoch 203/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 386897.3438 - reconstruction_loss: 386897.3438 - kl_loss: 4.3511e-06\n",
            "Epoch 204/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 398120.7188 - reconstruction_loss: 398120.7188 - kl_loss: 4.1127e-06\n",
            "Epoch 205/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 428438.1250 - reconstruction_loss: 428438.1250 - kl_loss: 3.8743e-06\n",
            "Epoch 206/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 391235.9375 - reconstruction_loss: 391235.9375 - kl_loss: 3.6955e-06\n",
            "Epoch 207/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 407374.5625 - reconstruction_loss: 407374.5625 - kl_loss: 3.4571e-06\n",
            "Epoch 208/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 460729.3438 - reconstruction_loss: 460729.3438 - kl_loss: 3.2783e-06\n",
            "Epoch 209/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 406063.6875 - reconstruction_loss: 406063.6875 - kl_loss: 3.0398e-06\n",
            "Epoch 210/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 431786.4062 - reconstruction_loss: 431786.4062 - kl_loss: 2.9206e-06\n",
            "Epoch 211/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 428852.8750 - reconstruction_loss: 428852.8750 - kl_loss: 2.7418e-06\n",
            "Epoch 212/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 398184.5625 - reconstruction_loss: 398184.5625 - kl_loss: 2.6226e-06\n",
            "Epoch 213/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 438797.5625 - reconstruction_loss: 438797.5625 - kl_loss: 2.4438e-06\n",
            "Epoch 214/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 428462.3125 - reconstruction_loss: 428462.3125 - kl_loss: 2.3544e-06\n",
            "Epoch 215/226\n",
            "1/1 [==============================] - 44s 44s/step - loss: 413493.1250 - reconstruction_loss: 413493.1250 - kl_loss: 2.1756e-06\n",
            "Epoch 216/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 424630.0938 - reconstruction_loss: 424630.0938 - kl_loss: 2.0862e-06\n",
            "Epoch 217/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 394988.3438 - reconstruction_loss: 394988.3438 - kl_loss: 1.9670e-06\n",
            "Epoch 218/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 435527.0625 - reconstruction_loss: 435527.0625 - kl_loss: 1.8775e-06\n",
            "Epoch 219/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 414947.5625 - reconstruction_loss: 414947.5625 - kl_loss: 1.7881e-06\n",
            "Epoch 220/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 475764.9375 - reconstruction_loss: 475764.9375 - kl_loss: 1.6689e-06\n",
            "Epoch 221/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 360393.6875 - reconstruction_loss: 360393.6875 - kl_loss: 1.6093e-06\n",
            "Epoch 222/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 455541.0000 - reconstruction_loss: 455541.0000 - kl_loss: 1.5199e-06\n",
            "Epoch 223/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 382653.4062 - reconstruction_loss: 382653.4062 - kl_loss: 1.4305e-06\n",
            "Epoch 224/226\n",
            "1/1 [==============================] - 42s 42s/step - loss: 335466.7812 - reconstruction_loss: 335466.7812 - kl_loss: 1.3709e-06\n",
            "Epoch 225/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 406009.0938 - reconstruction_loss: 406009.0938 - kl_loss: 1.3113e-06\n",
            "Epoch 226/226\n",
            "1/1 [==============================] - 43s 43s/step - loss: 348998.5938 - reconstruction_loss: 348998.5938 - kl_loss: 1.2517e-06\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9501b95cf9f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m226\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/DataAugmentedRadiography_CovidModel/VAE.tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_vae' is not defined"
          ]
        }
      ],
      "source": [
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(dataset, epochs=226 , steps_per_epoch=1)\n",
        "model = vae.get_vae();\n",
        "vae.save('/content/gdrive/My Drive/DataAugmentedRadiography_CovidModel/VAE.tf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZTn2W0YR9WW"
      },
      "source": [
        "# Code taken from https://keras.io/examples/generative/dcgan_overriding_train_step/ and refactored  Radiography DCGANs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LpzVOLGUZHP",
        "outputId": "ce895d8b-752a-4121-db10-5a3662367e44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7232 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128, 128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    radiography_dataset + '/COVID/', label_mode=None, image_size=image_size, batch_size=16,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqBHZFeEvnL1"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpdanrS_Sf3K",
        "outputId": "630982a5-f857-449f-d32a-fe2640ffea84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101 4\n"
          ]
        }
      ],
      "source": [
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "print(generator_in_channels, discriminator_in_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi6lG4V7Siri",
        "outputId": "d7d406a3-183a-4770-f8b3-6586daa72f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 8192)              827392    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 256)      524544    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 1024)     8389632   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 64, 64, 1024)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 128, 128, 64)     1048640   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 3)       4803      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,892,675\n",
            "Trainable params: 12,892,675\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(1, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySIPqDcuSmcD"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_RADIOLOGY_COVID')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELzgXu-lSrZg"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_Radiology_COVID' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJgeeK-GTaM4",
        "outputId": "73159272-9dc7-4214-dcd6-fd467c3c8d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/452\n",
            "1/1 [==============================] - 120s 120s/step - d_loss: 0.6966 - g_loss: 0.9573\n",
            "Epoch 2/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6350 - g_loss: 0.9518\n",
            "Epoch 3/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5998 - g_loss: 1.1624\n",
            "Epoch 4/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6082 - g_loss: 0.7784\n",
            "Epoch 5/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5745 - g_loss: 1.2773\n",
            "Epoch 6/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4690 - g_loss: 1.4349\n",
            "Epoch 7/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6493 - g_loss: 0.6152\n",
            "Epoch 8/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6330 - g_loss: 0.9883\n",
            "Epoch 9/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5064 - g_loss: 1.0945\n",
            "Epoch 10/452\n",
            "1/1 [==============================] - 33s 33s/step - d_loss: 0.4993 - g_loss: 1.1827\n",
            "Epoch 11/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4757 - g_loss: 1.2236\n",
            "Epoch 12/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.4467 - g_loss: 1.1917\n",
            "Epoch 13/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4069 - g_loss: 1.6723\n",
            "Epoch 14/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.4961 - g_loss: 0.5412\n",
            "Epoch 15/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5715 - g_loss: 1.7118\n",
            "Epoch 16/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4378 - g_loss: 1.1979\n",
            "Epoch 17/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.4654 - g_loss: 1.0723\n",
            "Epoch 18/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3815 - g_loss: 1.7724\n",
            "Epoch 19/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4285 - g_loss: 1.0793\n",
            "Epoch 20/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3209 - g_loss: 2.1550\n",
            "Epoch 21/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4300 - g_loss: 1.0629\n",
            "Epoch 22/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3404 - g_loss: 1.9059\n",
            "Epoch 23/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.2841 - g_loss: 1.6369\n",
            "Epoch 24/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3459 - g_loss: 1.4231\n",
            "Epoch 25/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.3234 - g_loss: 2.1391\n",
            "Epoch 26/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4053 - g_loss: 0.7127\n",
            "Epoch 27/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4595 - g_loss: 2.6155\n",
            "Epoch 28/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5035 - g_loss: 1.1971\n",
            "Epoch 29/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3053 - g_loss: 1.8549\n",
            "Epoch 30/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.2449 - g_loss: 1.7672\n",
            "Epoch 31/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.3100 - g_loss: 1.4465\n",
            "Epoch 32/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.2091 - g_loss: 2.7184\n",
            "Epoch 33/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6214 - g_loss: 0.7438\n",
            "Epoch 34/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3999 - g_loss: 2.2493\n",
            "Epoch 35/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4408 - g_loss: 1.4394\n",
            "Epoch 36/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.1978 - g_loss: 2.1661\n",
            "Epoch 37/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3394 - g_loss: 1.4756\n",
            "Epoch 38/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.2184 - g_loss: 2.2454\n",
            "Epoch 39/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.2077 - g_loss: 2.1117\n",
            "Epoch 40/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1605 - g_loss: 2.1962\n",
            "Epoch 41/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.2065 - g_loss: 2.1847\n",
            "Epoch 42/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1684 - g_loss: 2.3102\n",
            "Epoch 43/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1584 - g_loss: 2.3644\n",
            "Epoch 44/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3451 - g_loss: 1.6685\n",
            "Epoch 45/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.2013 - g_loss: 3.0099\n",
            "Epoch 46/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3454 - g_loss: 0.7288\n",
            "Epoch 47/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.3891 - g_loss: 3.3627\n",
            "Epoch 48/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.1939 - g_loss: 2.8147\n",
            "Epoch 49/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3166 - g_loss: 1.6717\n",
            "Epoch 50/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.1893 - g_loss: 2.3772\n",
            "Epoch 51/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.2596 - g_loss: 1.7875\n",
            "Epoch 52/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1910 - g_loss: 2.8995\n",
            "Epoch 53/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1342 - g_loss: 2.3054\n",
            "Epoch 54/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1955 - g_loss: 1.7995\n",
            "Epoch 55/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1812 - g_loss: 2.9164\n",
            "Epoch 56/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1914 - g_loss: 1.9191\n",
            "Epoch 57/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1391 - g_loss: 3.5747\n",
            "Epoch 58/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.1376 - g_loss: 2.5135\n",
            "Epoch 59/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.2570 - g_loss: 2.4018\n",
            "Epoch 60/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.2338 - g_loss: 2.0485\n",
            "Epoch 61/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1178 - g_loss: 3.2264\n",
            "Epoch 62/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4323 - g_loss: 0.8677\n",
            "Epoch 63/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3915 - g_loss: 2.3844\n",
            "Epoch 64/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6099 - g_loss: 1.2272\n",
            "Epoch 65/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3112 - g_loss: 1.7024\n",
            "Epoch 66/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.2546 - g_loss: 1.6564\n",
            "Epoch 67/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.1706 - g_loss: 2.1774\n",
            "Epoch 68/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4127 - g_loss: 1.2121\n",
            "Epoch 69/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.3075 - g_loss: 1.5528\n",
            "Epoch 70/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3953 - g_loss: 1.0808\n",
            "Epoch 71/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.3275 - g_loss: 1.3900\n",
            "Epoch 72/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4465 - g_loss: 0.8909\n",
            "Epoch 73/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4448 - g_loss: 1.6819\n",
            "Epoch 74/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5025 - g_loss: 0.8448\n",
            "Epoch 75/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5935 - g_loss: 0.6939\n",
            "Epoch 76/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6251 - g_loss: 0.8928\n",
            "Epoch 77/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7711 - g_loss: 0.7475\n",
            "Epoch 78/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.8884 - g_loss: 0.4783\n",
            "Epoch 79/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7364 - g_loss: 1.1949\n",
            "Epoch 80/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.7766 - g_loss: 0.6410\n",
            "Epoch 81/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.8036 - g_loss: 0.5733\n",
            "Epoch 82/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.8205 - g_loss: 0.6679\n",
            "Epoch 83/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.7543 - g_loss: 0.6754\n",
            "Epoch 84/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6299 - g_loss: 0.7843\n",
            "Epoch 85/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.8307 - g_loss: 0.5502\n",
            "Epoch 86/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7856 - g_loss: 0.6246\n",
            "Epoch 87/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7760 - g_loss: 0.6648\n",
            "Epoch 88/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6597 - g_loss: 0.6890\n",
            "Epoch 89/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7246 - g_loss: 0.6163\n",
            "Epoch 90/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7000 - g_loss: 0.7204\n",
            "Epoch 91/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6579 - g_loss: 0.6204\n",
            "Epoch 92/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6142 - g_loss: 0.8659\n",
            "Epoch 93/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7610 - g_loss: 0.5982\n",
            "Epoch 94/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6308 - g_loss: 0.7757\n",
            "Epoch 95/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.6098 - g_loss: 0.7469\n",
            "Epoch 96/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6059 - g_loss: 0.8152\n",
            "Epoch 97/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6926 - g_loss: 0.6834\n",
            "Epoch 98/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6424 - g_loss: 0.7759\n",
            "Epoch 99/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5842 - g_loss: 0.9017\n",
            "Epoch 100/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5565 - g_loss: 0.9200\n",
            "Epoch 101/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5964 - g_loss: 0.7552\n",
            "Epoch 102/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6596 - g_loss: 0.9207\n",
            "Epoch 103/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5458 - g_loss: 0.8479\n",
            "Epoch 104/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6157 - g_loss: 0.8168\n",
            "Epoch 105/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5888 - g_loss: 1.0568\n",
            "Epoch 106/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5276 - g_loss: 0.9112\n",
            "Epoch 107/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5771 - g_loss: 0.7368\n",
            "Epoch 108/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6091 - g_loss: 1.2818\n",
            "Epoch 109/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.5758 - g_loss: 0.6773\n",
            "Epoch 110/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5902 - g_loss: 1.2408\n",
            "Epoch 111/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5892 - g_loss: 0.7853\n",
            "Epoch 112/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5634 - g_loss: 0.9050\n",
            "Epoch 113/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6118 - g_loss: 0.7240\n",
            "Epoch 114/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5726 - g_loss: 1.0082\n",
            "Epoch 115/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5687 - g_loss: 0.6500\n",
            "Epoch 116/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5722 - g_loss: 1.1667\n",
            "Epoch 117/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5736 - g_loss: 0.6721\n",
            "Epoch 118/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6286 - g_loss: 1.0771\n",
            "Epoch 119/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6311 - g_loss: 0.6666\n",
            "Epoch 120/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5672 - g_loss: 0.9815\n",
            "Epoch 121/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6013 - g_loss: 0.6587\n",
            "Epoch 122/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5871 - g_loss: 0.9460\n",
            "Epoch 123/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6926 - g_loss: 0.5855\n",
            "Epoch 124/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6116 - g_loss: 0.7574\n",
            "Epoch 125/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5567 - g_loss: 0.6333\n",
            "Epoch 126/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5794 - g_loss: 0.6988\n",
            "Epoch 127/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5151 - g_loss: 0.7934\n",
            "Epoch 128/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4932 - g_loss: 0.7589\n",
            "Epoch 129/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5516 - g_loss: 0.5758\n",
            "Epoch 130/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6156 - g_loss: 0.7838\n",
            "Epoch 131/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5459 - g_loss: 0.7850\n",
            "Epoch 132/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6148 - g_loss: 0.7112\n",
            "Epoch 133/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5150 - g_loss: 0.7080\n",
            "Epoch 134/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5439 - g_loss: 0.7714\n",
            "Epoch 135/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4780 - g_loss: 0.7344\n",
            "Epoch 136/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5489 - g_loss: 0.6615\n",
            "Epoch 137/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5712 - g_loss: 0.8240\n",
            "Epoch 138/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5345 - g_loss: 0.6694\n",
            "Epoch 139/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5511 - g_loss: 1.0170\n",
            "Epoch 140/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5294 - g_loss: 0.5774\n",
            "Epoch 141/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5911 - g_loss: 1.1817\n",
            "Epoch 142/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6209 - g_loss: 0.6265\n",
            "Epoch 143/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5213 - g_loss: 0.7630\n",
            "Epoch 144/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5127 - g_loss: 0.7993\n",
            "Epoch 145/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.4955 - g_loss: 0.8245\n",
            "Epoch 146/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4811 - g_loss: 0.7314\n",
            "Epoch 147/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5578 - g_loss: 0.8917\n",
            "Epoch 148/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5116 - g_loss: 0.7693\n",
            "Epoch 149/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5748 - g_loss: 0.6847\n",
            "Epoch 150/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4921 - g_loss: 1.0816\n",
            "Epoch 151/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6283 - g_loss: 0.4681\n",
            "Epoch 152/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6648 - g_loss: 0.9114\n",
            "Epoch 153/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5339 - g_loss: 0.7596\n",
            "Epoch 154/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5482 - g_loss: 0.9122\n",
            "Epoch 155/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5615 - g_loss: 0.7306\n",
            "Epoch 156/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5166 - g_loss: 1.0429\n",
            "Epoch 157/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5439 - g_loss: 0.7031\n",
            "Epoch 158/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5596 - g_loss: 0.8145\n",
            "Epoch 159/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5340 - g_loss: 0.7491\n",
            "Epoch 160/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5742 - g_loss: 1.0045\n",
            "Epoch 161/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.5155 - g_loss: 0.8258\n",
            "Epoch 162/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5440 - g_loss: 0.9308\n",
            "Epoch 163/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5897 - g_loss: 0.5540\n",
            "Epoch 164/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5904 - g_loss: 1.1570\n",
            "Epoch 165/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5991 - g_loss: 0.6216\n",
            "Epoch 166/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6114 - g_loss: 0.8309\n",
            "Epoch 167/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5338 - g_loss: 0.8911\n",
            "Epoch 168/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4992 - g_loss: 1.0083\n",
            "Epoch 169/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5221 - g_loss: 0.7452\n",
            "Epoch 170/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5874 - g_loss: 0.8250\n",
            "Epoch 171/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.4962 - g_loss: 0.7584\n",
            "Epoch 172/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5008 - g_loss: 0.6731\n",
            "Epoch 173/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5558 - g_loss: 0.8130\n",
            "Epoch 174/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6149 - g_loss: 0.8285\n",
            "Epoch 175/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5403 - g_loss: 0.8953\n",
            "Epoch 176/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.4894 - g_loss: 0.9791\n",
            "Epoch 177/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5011 - g_loss: 0.7758\n",
            "Epoch 178/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5474 - g_loss: 0.7342\n",
            "Epoch 179/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5742 - g_loss: 1.3700\n",
            "Epoch 180/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5658 - g_loss: 0.5588\n",
            "Epoch 181/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6689 - g_loss: 1.8017\n",
            "Epoch 182/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6801 - g_loss: 0.8467\n",
            "Epoch 183/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5951 - g_loss: 0.8143\n",
            "Epoch 184/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5238 - g_loss: 0.8736\n",
            "Epoch 185/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5056 - g_loss: 0.9908\n",
            "Epoch 186/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5152 - g_loss: 1.0140\n",
            "Epoch 187/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6071 - g_loss: 0.7626\n",
            "Epoch 188/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5577 - g_loss: 0.9834\n",
            "Epoch 189/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5318 - g_loss: 0.7753\n",
            "Epoch 190/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5342 - g_loss: 0.6219\n",
            "Epoch 191/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5860 - g_loss: 0.9039\n",
            "Epoch 192/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5655 - g_loss: 0.8162\n",
            "Epoch 193/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5259 - g_loss: 0.5914\n",
            "Epoch 194/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6726 - g_loss: 1.2050\n",
            "Epoch 195/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6234 - g_loss: 0.6658\n",
            "Epoch 196/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5617 - g_loss: 0.7722\n",
            "Epoch 197/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5851 - g_loss: 0.9593\n",
            "Epoch 198/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5682 - g_loss: 0.6009\n",
            "Epoch 199/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.5867 - g_loss: 0.7224\n",
            "Epoch 200/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5728 - g_loss: 1.0430\n",
            "Epoch 201/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6792 - g_loss: 0.6511\n",
            "Epoch 202/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5942 - g_loss: 0.9679\n",
            "Epoch 203/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7093 - g_loss: 0.6530\n",
            "Epoch 204/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6414 - g_loss: 0.8409\n",
            "Epoch 205/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5841 - g_loss: 0.7675\n",
            "Epoch 206/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6058 - g_loss: 0.8037\n",
            "Epoch 207/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6706 - g_loss: 0.5611\n",
            "Epoch 208/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6024 - g_loss: 0.7699\n",
            "Epoch 209/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6168 - g_loss: 0.8059\n",
            "Epoch 210/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6319 - g_loss: 0.8483\n",
            "Epoch 211/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5983 - g_loss: 0.8378\n",
            "Epoch 212/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5205 - g_loss: 0.7784\n",
            "Epoch 213/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6899 - g_loss: 0.8201\n",
            "Epoch 214/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6288 - g_loss: 0.7080\n",
            "Epoch 215/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6246 - g_loss: 0.8641\n",
            "Epoch 216/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6340 - g_loss: 1.0082\n",
            "Epoch 217/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6273 - g_loss: 0.7022\n",
            "Epoch 218/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6164 - g_loss: 0.9001\n",
            "Epoch 219/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5945 - g_loss: 0.6843\n",
            "Epoch 220/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6483 - g_loss: 1.0476\n",
            "Epoch 221/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6542 - g_loss: 0.6233\n",
            "Epoch 222/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6858 - g_loss: 1.6038\n",
            "Epoch 223/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.7957 - g_loss: 0.6086\n",
            "Epoch 224/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6943 - g_loss: 0.7916\n",
            "Epoch 225/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5995 - g_loss: 0.7949\n",
            "Epoch 226/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6214 - g_loss: 0.8954\n",
            "Epoch 227/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6228 - g_loss: 0.7766\n",
            "Epoch 228/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6178 - g_loss: 0.9234\n",
            "Epoch 229/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5949 - g_loss: 0.8850\n",
            "Epoch 230/452\n",
            "1/1 [==============================] - 33s 33s/step - d_loss: 0.5453 - g_loss: 0.9070\n",
            "Epoch 231/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7498 - g_loss: 0.7143\n",
            "Epoch 232/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7108 - g_loss: 0.8982\n",
            "Epoch 233/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5725 - g_loss: 0.9025\n",
            "Epoch 234/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6256 - g_loss: 0.7607\n",
            "Epoch 235/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6545 - g_loss: 0.8352\n",
            "Epoch 236/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6063 - g_loss: 0.7212\n",
            "Epoch 237/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6234 - g_loss: 0.9303\n",
            "Epoch 238/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6865 - g_loss: 0.7181\n",
            "Epoch 239/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5908 - g_loss: 1.1800\n",
            "Epoch 240/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.6272 - g_loss: 0.6704\n",
            "Epoch 241/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6504 - g_loss: 0.9968\n",
            "Epoch 242/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6412 - g_loss: 0.6770\n",
            "Epoch 243/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.7365 - g_loss: 1.0608\n",
            "Epoch 244/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7063 - g_loss: 0.6497\n",
            "Epoch 245/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6891 - g_loss: 0.9924\n",
            "Epoch 246/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5873 - g_loss: 0.8949\n",
            "Epoch 247/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6245 - g_loss: 0.7663\n",
            "Epoch 248/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6745 - g_loss: 0.9425\n",
            "Epoch 249/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6671 - g_loss: 0.8143\n",
            "Epoch 250/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6375 - g_loss: 0.8997\n",
            "Epoch 251/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6268 - g_loss: 1.0538\n",
            "Epoch 252/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5989 - g_loss: 0.6969\n",
            "Epoch 253/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6125 - g_loss: 1.1117\n",
            "Epoch 254/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6171 - g_loss: 0.8839\n",
            "Epoch 255/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6269 - g_loss: 0.8160\n",
            "Epoch 256/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6508 - g_loss: 0.8134\n",
            "Epoch 257/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6405 - g_loss: 0.7497\n",
            "Epoch 258/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6139 - g_loss: 0.9127\n",
            "Epoch 259/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7793 - g_loss: 0.9204\n",
            "Epoch 260/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7157 - g_loss: 0.7045\n",
            "Epoch 261/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6268 - g_loss: 0.8695\n",
            "Epoch 262/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6510 - g_loss: 0.8925\n",
            "Epoch 263/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6750 - g_loss: 0.7356\n",
            "Epoch 264/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6974 - g_loss: 1.0812\n",
            "Epoch 265/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7428 - g_loss: 0.6631\n",
            "Epoch 266/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6473 - g_loss: 0.7958\n",
            "Epoch 267/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.7275 - g_loss: 0.9476\n",
            "Epoch 268/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6278 - g_loss: 0.8934\n",
            "Epoch 269/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7181 - g_loss: 0.7497\n",
            "Epoch 270/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6526 - g_loss: 0.8747\n",
            "Epoch 271/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6911 - g_loss: 0.9358\n",
            "Epoch 272/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5874 - g_loss: 0.9150\n",
            "Epoch 273/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5971 - g_loss: 0.9028\n",
            "Epoch 274/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5799 - g_loss: 0.9199\n",
            "Epoch 275/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6065 - g_loss: 0.5517\n",
            "Epoch 276/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.7041 - g_loss: 1.6660\n",
            "Epoch 277/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7598 - g_loss: 0.4907\n",
            "Epoch 278/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6992 - g_loss: 0.8844\n",
            "Epoch 279/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6523 - g_loss: 0.9566\n",
            "Epoch 280/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5853 - g_loss: 0.9198\n",
            "Epoch 281/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6608 - g_loss: 0.8083\n",
            "Epoch 282/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6864 - g_loss: 1.1535\n",
            "Epoch 283/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6544 - g_loss: 0.8436\n",
            "Epoch 284/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6815 - g_loss: 0.6161\n",
            "Epoch 285/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6692 - g_loss: 1.4574\n",
            "Epoch 286/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6472 - g_loss: 0.8245\n",
            "Epoch 287/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6072 - g_loss: 1.0327\n",
            "Epoch 288/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5586 - g_loss: 0.9850\n",
            "Epoch 289/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7059 - g_loss: 0.7758\n",
            "Epoch 290/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6112 - g_loss: 0.8595\n",
            "Epoch 291/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6322 - g_loss: 1.2264\n",
            "Epoch 292/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6977 - g_loss: 0.6206\n",
            "Epoch 293/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6655 - g_loss: 0.9500\n",
            "Epoch 294/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6131 - g_loss: 0.7920\n",
            "Epoch 295/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6092 - g_loss: 1.1132\n",
            "Epoch 296/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6320 - g_loss: 0.5772\n",
            "Epoch 297/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6875 - g_loss: 1.0649\n",
            "Epoch 298/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6676 - g_loss: 0.6745\n",
            "Epoch 299/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6819 - g_loss: 1.1268\n",
            "Epoch 300/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5630 - g_loss: 0.9757\n",
            "Epoch 301/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7052 - g_loss: 0.8711\n",
            "Epoch 302/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6298 - g_loss: 0.6825\n",
            "Epoch 303/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6464 - g_loss: 0.9285\n",
            "Epoch 304/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6705 - g_loss: 1.2122\n",
            "Epoch 305/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6359 - g_loss: 0.7402\n",
            "Epoch 306/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6437 - g_loss: 0.8214\n",
            "Epoch 307/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6010 - g_loss: 0.9803\n",
            "Epoch 308/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6192 - g_loss: 0.6711\n",
            "Epoch 309/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.7544 - g_loss: 1.5246\n",
            "Epoch 310/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7888 - g_loss: 0.5087\n",
            "Epoch 311/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7853 - g_loss: 0.8758\n",
            "Epoch 312/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6517 - g_loss: 0.9250\n",
            "Epoch 313/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6796 - g_loss: 0.7995\n",
            "Epoch 314/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6216 - g_loss: 0.8742\n",
            "Epoch 315/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6175 - g_loss: 1.3635\n",
            "Epoch 316/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6948 - g_loss: 0.7326\n",
            "Epoch 317/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6445 - g_loss: 1.0451\n",
            "Epoch 318/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6336 - g_loss: 0.9153\n",
            "Epoch 319/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6144 - g_loss: 0.8985\n",
            "Epoch 320/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6109 - g_loss: 1.0311\n",
            "Epoch 321/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6923 - g_loss: 0.7620\n",
            "Epoch 322/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6211 - g_loss: 1.2034\n",
            "Epoch 323/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6170 - g_loss: 0.9125\n",
            "Epoch 324/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7117 - g_loss: 0.7642\n",
            "Epoch 325/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5912 - g_loss: 1.0486\n",
            "Epoch 326/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6238 - g_loss: 0.5840\n",
            "Epoch 327/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.7271 - g_loss: 1.6437\n",
            "Epoch 328/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7112 - g_loss: 0.8433\n",
            "Epoch 329/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6236 - g_loss: 0.8481\n",
            "Epoch 330/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5210 - g_loss: 0.9737\n",
            "Epoch 331/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6354 - g_loss: 0.9724\n",
            "Epoch 332/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.7275 - g_loss: 0.8050\n",
            "Epoch 333/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5942 - g_loss: 1.2018\n",
            "Epoch 334/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6722 - g_loss: 0.6464\n",
            "Epoch 335/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6393 - g_loss: 1.3907\n",
            "Epoch 336/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6461 - g_loss: 0.6822\n",
            "Epoch 337/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6040 - g_loss: 1.2256\n",
            "Epoch 338/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5552 - g_loss: 1.3306\n",
            "Epoch 339/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6309 - g_loss: 0.6812\n",
            "Epoch 340/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6454 - g_loss: 1.1119\n",
            "Epoch 341/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7199 - g_loss: 0.7213\n",
            "Epoch 342/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6112 - g_loss: 0.9871\n",
            "Epoch 343/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6220 - g_loss: 1.1668\n",
            "Epoch 344/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6131 - g_loss: 0.8347\n",
            "Epoch 345/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5844 - g_loss: 1.0916\n",
            "Epoch 346/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6105 - g_loss: 0.7388\n",
            "Epoch 347/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5443 - g_loss: 1.4307\n",
            "Epoch 348/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6754 - g_loss: 0.6186\n",
            "Epoch 349/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6791 - g_loss: 1.3020\n",
            "Epoch 350/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6770 - g_loss: 0.6338\n",
            "Epoch 351/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5794 - g_loss: 1.1008\n",
            "Epoch 352/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6804 - g_loss: 0.7391\n",
            "Epoch 353/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6422 - g_loss: 1.1011\n",
            "Epoch 354/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6586 - g_loss: 0.8193\n",
            "Epoch 355/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6666 - g_loss: 1.1022\n",
            "Epoch 356/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6563 - g_loss: 0.9698\n",
            "Epoch 357/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6512 - g_loss: 0.6494\n",
            "Epoch 358/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6584 - g_loss: 1.3290\n",
            "Epoch 359/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6334 - g_loss: 0.7489\n",
            "Epoch 360/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6453 - g_loss: 1.1310\n",
            "Epoch 361/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6252 - g_loss: 1.0160\n",
            "Epoch 362/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6548 - g_loss: 0.5693\n",
            "Epoch 363/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6782 - g_loss: 1.0363\n",
            "Epoch 364/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6116 - g_loss: 1.0164\n",
            "Epoch 365/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6418 - g_loss: 1.0959\n",
            "Epoch 366/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6417 - g_loss: 0.7712\n",
            "Epoch 367/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6673 - g_loss: 1.2781\n",
            "Epoch 368/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6786 - g_loss: 0.5187\n",
            "Epoch 369/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7803 - g_loss: 1.2161\n",
            "Epoch 370/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6546 - g_loss: 1.0100\n",
            "Epoch 371/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6229 - g_loss: 0.9136\n",
            "Epoch 372/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6467 - g_loss: 1.1120\n",
            "Epoch 373/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5778 - g_loss: 1.0645\n",
            "Epoch 374/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5940 - g_loss: 1.0474\n",
            "Epoch 375/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5700 - g_loss: 1.0858\n",
            "Epoch 376/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6033 - g_loss: 0.9469\n",
            "Epoch 377/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5903 - g_loss: 0.8780\n",
            "Epoch 378/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6282 - g_loss: 0.7344\n",
            "Epoch 379/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5912 - g_loss: 1.0478\n",
            "Epoch 380/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6054 - g_loss: 0.8358\n",
            "Epoch 381/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.6406 - g_loss: 1.3034\n",
            "Epoch 382/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.6943 - g_loss: 0.5637\n",
            "Epoch 383/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.7503 - g_loss: 1.4011\n",
            "Epoch 384/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6398 - g_loss: 0.6816\n",
            "Epoch 385/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6315 - g_loss: 0.9237\n",
            "Epoch 386/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.6361 - g_loss: 0.7669\n",
            "Epoch 387/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5724 - g_loss: 1.2700\n",
            "Epoch 388/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5647 - g_loss: 1.2792\n",
            "Epoch 389/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6748 - g_loss: 0.7851\n",
            "Epoch 390/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6820 - g_loss: 0.8165\n",
            "Epoch 391/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6043 - g_loss: 0.7566\n",
            "Epoch 392/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6337 - g_loss: 1.3747\n",
            "Epoch 393/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5555 - g_loss: 1.0267\n",
            "Epoch 394/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.6601 - g_loss: 0.6790\n",
            "Epoch 395/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6759 - g_loss: 1.2781\n",
            "Epoch 396/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6582 - g_loss: 0.6516\n",
            "Epoch 397/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6374 - g_loss: 0.9124\n",
            "Epoch 398/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5876 - g_loss: 0.9572\n",
            "Epoch 399/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6353 - g_loss: 0.6721\n",
            "Epoch 400/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6684 - g_loss: 1.3707\n",
            "Epoch 401/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6353 - g_loss: 0.6823\n",
            "Epoch 402/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6025 - g_loss: 1.0442\n",
            "Epoch 403/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6094 - g_loss: 0.7632\n",
            "Epoch 404/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6220 - g_loss: 1.0743\n",
            "Epoch 405/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5713 - g_loss: 1.5292\n",
            "Epoch 406/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6692 - g_loss: 0.6553\n",
            "Epoch 407/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6783 - g_loss: 0.8204\n",
            "Epoch 408/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5791 - g_loss: 0.9051\n",
            "Epoch 409/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.6509 - g_loss: 0.9539\n",
            "Epoch 410/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5876 - g_loss: 0.6830\n",
            "Epoch 411/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6309 - g_loss: 1.2473\n",
            "Epoch 412/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5866 - g_loss: 1.0043\n",
            "Epoch 413/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6198 - g_loss: 0.8392\n",
            "Epoch 414/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6228 - g_loss: 0.7538\n",
            "Epoch 415/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6082 - g_loss: 1.2161\n",
            "Epoch 416/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6547 - g_loss: 0.5045\n",
            "Epoch 417/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.7447 - g_loss: 1.2395\n",
            "Epoch 418/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6540 - g_loss: 0.6769\n",
            "Epoch 419/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6125 - g_loss: 0.9937\n",
            "Epoch 420/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5993 - g_loss: 0.8679\n",
            "Epoch 421/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6277 - g_loss: 0.7768\n",
            "Epoch 422/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6396 - g_loss: 1.0716\n",
            "Epoch 423/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6245 - g_loss: 0.7653\n",
            "Epoch 424/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6210 - g_loss: 0.9316\n",
            "Epoch 425/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.6069 - g_loss: 0.7757\n",
            "Epoch 426/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6320 - g_loss: 0.8712\n",
            "Epoch 427/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6011 - g_loss: 0.9425\n",
            "Epoch 428/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6282 - g_loss: 1.2611\n",
            "Epoch 429/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5481 - g_loss: 0.6774\n",
            "Epoch 430/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.6270 - g_loss: 1.1694\n",
            "Epoch 431/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.7183 - g_loss: 0.5608\n",
            "Epoch 432/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.8068 - g_loss: 1.1232\n",
            "Epoch 433/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6253 - g_loss: 0.7999\n",
            "Epoch 434/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5700 - g_loss: 1.0763\n",
            "Epoch 435/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5911 - g_loss: 0.9627\n",
            "Epoch 436/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5767 - g_loss: 0.7878\n",
            "Epoch 437/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6359 - g_loss: 0.9489\n",
            "Epoch 438/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5835 - g_loss: 0.9267\n",
            "Epoch 439/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5855 - g_loss: 1.0724\n",
            "Epoch 440/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5380 - g_loss: 1.2119\n",
            "Epoch 441/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6726 - g_loss: 0.5457\n",
            "Epoch 442/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6250 - g_loss: 1.2989\n",
            "Epoch 443/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6116 - g_loss: 0.8267\n",
            "Epoch 444/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6468 - g_loss: 0.7676\n",
            "Epoch 445/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.5957 - g_loss: 0.9133\n",
            "Epoch 446/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.5711 - g_loss: 0.7335\n",
            "Epoch 447/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6664 - g_loss: 1.2184\n",
            "Epoch 448/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6062 - g_loss: 0.6343\n",
            "Epoch 449/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6675 - g_loss: 1.0055\n",
            "Epoch 450/452\n",
            "1/1 [==============================] - 30s 30s/step - d_loss: 0.6629 - g_loss: 0.8549\n",
            "Epoch 451/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6094 - g_loss: 0.8001\n",
            "Epoch 452/452\n",
            "1/1 [==============================] - 31s 31s/step - d_loss: 0.6196 - g_loss: 0.8695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "epochs = 452  # In practice, use ~100 epochs 452\n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0.01),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0.01),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs,steps_per_epoch=1, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_Radiology_COVIDModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_Radiology_COVIDModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-ZcgZzw-cyJ",
        "outputId": "7f9f7ae1-3f3e-43e4-fd1f-e46c7c638c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2690 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128, 128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    radiography_dataset + '/Viral Pneumonia/', label_mode=None, image_size=image_size, batch_size=32,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WAuQy-L3o_Ch"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrJRP5N-9g7T",
        "outputId": "8e8972c7-3339-4e14-ba7e-f1892fe38761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " re_lu_1 (ReLU)              (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 8192)              2105344   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 256)      524544    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " re_lu_4 (ReLU)              (None, 32, 32, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 512)      4194816   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " re_lu_5 (ReLU)              (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 128, 128, 1024)   8389632   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " re_lu_6 (ReLU)              (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 3)       27651     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,339,651\n",
            "Trainable params: 17,339,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(3, kernel_size=3, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YDd9AMmj9iPD"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_RADIOGRAPHY_Pneumonia')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rnY1UG_r199G"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=100):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_Radiography_PNEUMONIA' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u04njmlW2CUm",
        "outputId": "9f35df8f-a2d9-47da-f158-06d514f66d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/84\n",
            "1/1 [==============================] - 158s 158s/step - d_loss: 0.6940 - g_loss: 3.4332\n",
            "Epoch 2/84\n",
            "1/1 [==============================] - 162s 162s/step - d_loss: 0.8406 - g_loss: 0.0036\n",
            "Epoch 3/84\n"
          ]
        }
      ],
      "source": [
        "epochs = 84  # In practice, use ~100 epochs\n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.001,momentum=0.001),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.001,momentum=0.001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs,steps_per_epoch=1, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        " \n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_Radiography_PNEUMONIAModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_Radiography_PNEUMONIAModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "537hHLU8zYhD"
      },
      "source": [
        "# DC GAN Chest XRAY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfdK97BwzfRb"
      },
      "source": [
        "# DCGAN XRAY COVID 19 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "swHTfr9M7L0a"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5uD1I0L72UZa"
      },
      "outputs": [],
      "source": [
        "# load images\n",
        "image_size = (256, 256)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    xray_covid19_dataset + '/Normal/', label_mode=None, image_size=image_size, batch_size=2,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HCtBA4uM19f7"
      },
      "outputs": [],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(256, 256, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(2048, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=3, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O47faZn5RyWH"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_RADIOLOGY_Pneumonia')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ERn_MDIo1_1r"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_NORMAL' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_cEdmm2k2Cyu"
      },
      "outputs": [],
      "source": [
        "epochs = 47  # In practice, use ~100 epochs\n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.1,momentum=0.1),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.1,momentum=0.1),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs,steps_per_epoch=1, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        " \n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_NORMALModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_NORMALModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s6sof7xV2zA_"
      },
      "outputs": [],
      "source": [
        "# load images\n",
        "image_size = (128, 128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    xray_covid19_dataset + '/Pneumonia/', label_mode=None, image_size=image_size, batch_size=2,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nbnmxHQa2zwt"
      },
      "outputs": [],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.6),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.6),\n",
        "        layers.Conv2DTranspose(2048, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.6),\n",
        "        layers.Conv2DTranspose(4096, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.6),\n",
        "        layers.Conv2D(3, kernel_size=3, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fuvH8kv2Rz5X"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_RADIOLOGY_Pneumonia')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_3nRU_Cx21al"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_PNEUMONIA' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RofVn5zW226q"
      },
      "outputs": [],
      "source": [
        "epochs = 47  # In practice, use ~100 epochs\n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.000001,momentum=0.001),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.000001,momentum=0.001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs,steps_per_epoch=1, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        " \n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_PNEUMONIAModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_xray_covid19_dataset_PNEUMONIAModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI2ALLkCR2vt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}