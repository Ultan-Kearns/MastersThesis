{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xjyfa03-ZQT",
        "outputId": "b60797fd-e13d-4789-81e9-78ccd917ff1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n",
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
            "Installing collected packages: tensorflow-addons, install\n",
            "Successfully installed install-1.3.5 tensorflow-addons-0.19.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install pip install tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOY_iJ28-tjq",
        "outputId": "1dbfbd28-bbf9-4062-f5ff-3552491b6d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CT  X-ray\n"
          ]
        }
      ],
      "source": [
        "import keras \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from enum import Enum\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "# for reproducibility - ref https://machinelearningmastery.com/reproducible-results-neural-networks-keras/ and https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "np.random.seed(9)\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "# loading data from gdrive\n",
        "covid19_dataset = os.path.abspath(\"/content/gdrive/My Drive/COVID-19 Dataset/\")\n",
        "\n",
        "!ls \"/content/gdrive/My Drive/COVID-19 Dataset/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Y1hJwcCMAwmM",
        "outputId": "cb09dcee-c7ee-459f-ce14-18def7855196"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#@markdown #**Anti-Disconnect for Google Colab**\n",
        "#@markdown ## Run this to stop it from disconnecting automatically \n",
        "#@markdown  **(It will anyhow disconnect after 6 - 12 hrs for using the free version of Colab.)**\n",
        "#@markdown  *(Colab Pro users will get about 24 hrs usage time)*\n",
        "#@markdown ---\n",
        "# taken from https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP_%28z%2Bquantize_method_with_augmentations%2C_user_friendly_interface%29.ipynb#scrollTo=XHyPd4oxVp_l stops colab disconnecting\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "IPython.display.Javascript(js_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xray COVID"
      ],
      "metadata": {
        "id": "vdGobPGzow-X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qFzgcC1_AnH",
        "outputId": "fbfc347a-8087-47c7-9688-8b8437291eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4044 files belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128,128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    covid19_dataset + '/X-ray/COVID/', label_mode=None, image_size=image_size, batch_size=16,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M1w4bJp_tAF"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVdcu5MM_vbv",
        "outputId": "8eba8915-d7d6-4398-d771-472f79ae757a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "257 4\n"
          ]
        }
      ],
      "source": [
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "print(generator_in_channels, discriminator_in_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRcx8khQ_1Mg",
        "outputId": "99479aaf-11ca-42a9-f2f5-315a173c4ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_14 (Conv2D)          (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu_22 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_23 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_24 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 32768)             8421376   \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_11 (Conv2D  (None, 32, 32, 256)      524544    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_25 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_12 (Conv2D  (None, 64, 64, 512)      2097664   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_26 (LeakyReLU)  (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_13 (Conv2D  (None, 128, 128, 1024)   8389632   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_27 (LeakyReLU)  (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 128, 128, 3)       49155     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,482,371\n",
            "Trainable params: 19,482,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(16 * 16 * 128),\n",
        "        layers.Reshape((16, 16, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueLYo9Ww_3m1"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_COVID_Xray')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQZGmdVT_8K4"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/COVID-19 Data Augmented COVID X-Ray' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNH_74E2_-Bp",
        "outputId": "e41dd918-4c42-4b69-d752-cf23ef7d676b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "253/253 [==============================] - 31s 109ms/step - d_loss: 0.6906 - g_loss: 0.7612\n",
            "Epoch 2/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6922 - g_loss: 0.7454\n",
            "Epoch 3/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6912 - g_loss: 0.7542\n",
            "Epoch 4/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6895 - g_loss: 0.7526\n",
            "Epoch 5/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6872 - g_loss: 0.7603\n",
            "Epoch 6/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6870 - g_loss: 0.7561\n",
            "Epoch 7/200\n",
            "253/253 [==============================] - 28s 109ms/step - d_loss: 0.6883 - g_loss: 0.7535\n",
            "Epoch 8/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6888 - g_loss: 0.7562\n",
            "Epoch 9/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6900 - g_loss: 0.7492\n",
            "Epoch 10/200\n",
            "253/253 [==============================] - 28s 109ms/step - d_loss: 0.6914 - g_loss: 0.7529\n",
            "Epoch 11/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6915 - g_loss: 0.7492\n",
            "Epoch 12/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6915 - g_loss: 0.7509\n",
            "Epoch 13/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6909 - g_loss: 0.7500\n",
            "Epoch 14/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6919 - g_loss: 0.7491\n",
            "Epoch 15/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6931 - g_loss: 0.7456\n",
            "Epoch 16/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6926 - g_loss: 0.7525\n",
            "Epoch 17/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6925 - g_loss: 0.7497\n",
            "Epoch 18/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6925 - g_loss: 0.7510\n",
            "Epoch 19/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6935 - g_loss: 0.7504\n",
            "Epoch 20/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6939 - g_loss: 0.7491\n",
            "Epoch 21/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6944 - g_loss: 0.7487\n",
            "Epoch 22/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6948 - g_loss: 0.7499\n",
            "Epoch 23/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6942 - g_loss: 0.7516\n",
            "Epoch 24/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6953 - g_loss: 0.7524\n",
            "Epoch 25/200\n",
            "253/253 [==============================] - 28s 109ms/step - d_loss: 0.6951 - g_loss: 0.7522\n",
            "Epoch 26/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6968 - g_loss: 0.7470\n",
            "Epoch 27/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6973 - g_loss: 0.7450\n",
            "Epoch 28/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6965 - g_loss: 0.7469\n",
            "Epoch 29/200\n",
            "253/253 [==============================] - 28s 109ms/step - d_loss: 0.6965 - g_loss: 0.7474\n",
            "Epoch 30/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6973 - g_loss: 0.7444\n",
            "Epoch 31/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6977 - g_loss: 0.7458\n",
            "Epoch 32/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6966 - g_loss: 0.7480\n",
            "Epoch 33/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6964 - g_loss: 0.7493\n",
            "Epoch 34/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6966 - g_loss: 0.7489\n",
            "Epoch 35/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6967 - g_loss: 0.7493\n",
            "Epoch 36/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6965 - g_loss: 0.7496\n",
            "Epoch 37/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6977 - g_loss: 0.7493\n",
            "Epoch 38/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6986 - g_loss: 0.7490\n",
            "Epoch 39/200\n",
            "253/253 [==============================] - 31s 118ms/step - d_loss: 0.6997 - g_loss: 0.7464\n",
            "Epoch 40/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7008 - g_loss: 0.7467\n",
            "Epoch 41/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7013 - g_loss: 0.7448\n",
            "Epoch 42/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7035 - g_loss: 0.7421\n",
            "Epoch 43/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7044 - g_loss: 0.7412\n",
            "Epoch 44/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7052 - g_loss: 0.7418\n",
            "Epoch 45/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7049 - g_loss: 0.7407\n",
            "Epoch 46/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7038 - g_loss: 0.7426\n",
            "Epoch 47/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7031 - g_loss: 0.7447\n",
            "Epoch 48/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7020 - g_loss: 0.7453\n",
            "Epoch 49/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7015 - g_loss: 0.7444\n",
            "Epoch 50/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7009 - g_loss: 0.7463\n",
            "Epoch 51/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7008 - g_loss: 0.7458\n",
            "Epoch 52/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6995 - g_loss: 0.7458\n",
            "Epoch 53/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6998 - g_loss: 0.7486\n",
            "Epoch 54/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7009 - g_loss: 0.7434\n",
            "Epoch 55/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7005 - g_loss: 0.7452\n",
            "Epoch 56/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7000 - g_loss: 0.7474\n",
            "Epoch 57/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6999 - g_loss: 0.7479\n",
            "Epoch 58/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7000 - g_loss: 0.7458\n",
            "Epoch 59/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6994 - g_loss: 0.7461\n",
            "Epoch 60/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6995 - g_loss: 0.7465\n",
            "Epoch 61/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6990 - g_loss: 0.7496\n",
            "Epoch 62/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6985 - g_loss: 0.7480\n",
            "Epoch 63/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6987 - g_loss: 0.7452\n",
            "Epoch 64/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6979 - g_loss: 0.7477\n",
            "Epoch 65/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6986 - g_loss: 0.7471\n",
            "Epoch 66/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6984 - g_loss: 0.7489\n",
            "Epoch 67/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6987 - g_loss: 0.7471\n",
            "Epoch 68/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6987 - g_loss: 0.7491\n",
            "Epoch 69/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6983 - g_loss: 0.7470\n",
            "Epoch 70/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6982 - g_loss: 0.7475\n",
            "Epoch 71/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6975 - g_loss: 0.7482\n",
            "Epoch 72/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6975 - g_loss: 0.7486\n",
            "Epoch 73/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6971 - g_loss: 0.7504\n",
            "Epoch 74/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6966 - g_loss: 0.7491\n",
            "Epoch 75/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6967 - g_loss: 0.7501\n",
            "Epoch 76/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6964 - g_loss: 0.7511\n",
            "Epoch 77/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6960 - g_loss: 0.7496\n",
            "Epoch 78/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6959 - g_loss: 0.7506\n",
            "Epoch 79/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6958 - g_loss: 0.7500\n",
            "Epoch 80/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6957 - g_loss: 0.7522\n",
            "Epoch 81/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6955 - g_loss: 0.7486\n",
            "Epoch 82/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6954 - g_loss: 0.7511\n",
            "Epoch 83/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6955 - g_loss: 0.7493\n",
            "Epoch 84/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6945 - g_loss: 0.7506\n",
            "Epoch 85/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6947 - g_loss: 0.7509\n",
            "Epoch 86/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6948 - g_loss: 0.7497\n",
            "Epoch 87/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6942 - g_loss: 0.7508\n",
            "Epoch 88/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6940 - g_loss: 0.7513\n",
            "Epoch 89/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6937 - g_loss: 0.7507\n",
            "Epoch 90/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6938 - g_loss: 0.7517\n",
            "Epoch 91/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6933 - g_loss: 0.7511\n",
            "Epoch 92/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6936 - g_loss: 0.7513\n",
            "Epoch 93/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6929 - g_loss: 0.7521\n",
            "Epoch 94/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6932 - g_loss: 0.7515\n",
            "Epoch 95/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6928 - g_loss: 0.7496\n",
            "Epoch 96/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6930 - g_loss: 0.7522\n",
            "Epoch 97/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6927 - g_loss: 0.7509\n",
            "Epoch 98/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6924 - g_loss: 0.7525\n",
            "Epoch 99/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6924 - g_loss: 0.7512\n",
            "Epoch 100/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6923 - g_loss: 0.7529\n",
            "Epoch 101/200\n",
            "253/253 [==============================] - 32s 124ms/step - d_loss: 0.6915 - g_loss: 0.7524\n",
            "Epoch 102/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6917 - g_loss: 0.7517\n",
            "Epoch 103/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6910 - g_loss: 0.7537\n",
            "Epoch 104/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6916 - g_loss: 0.7516\n",
            "Epoch 105/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6913 - g_loss: 0.7538\n",
            "Epoch 106/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6910 - g_loss: 0.7533\n",
            "Epoch 107/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6908 - g_loss: 0.7524\n",
            "Epoch 108/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6910 - g_loss: 0.7530\n",
            "Epoch 109/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6903 - g_loss: 0.7538\n",
            "Epoch 110/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6905 - g_loss: 0.7545\n",
            "Epoch 111/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6900 - g_loss: 0.7547\n",
            "Epoch 112/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6896 - g_loss: 0.7529\n",
            "Epoch 113/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6898 - g_loss: 0.7565\n",
            "Epoch 114/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6897 - g_loss: 0.7531\n",
            "Epoch 115/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6897 - g_loss: 0.7536\n",
            "Epoch 116/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6900 - g_loss: 0.7567\n",
            "Epoch 117/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6900 - g_loss: 0.7538\n",
            "Epoch 118/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6895 - g_loss: 0.7548\n",
            "Epoch 119/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6897 - g_loss: 0.7557\n",
            "Epoch 120/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6892 - g_loss: 0.7537\n",
            "Epoch 121/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6892 - g_loss: 0.7544\n",
            "Epoch 122/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6892 - g_loss: 0.7562\n",
            "Epoch 123/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6892 - g_loss: 0.7538\n",
            "Epoch 124/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6885 - g_loss: 0.7561\n",
            "Epoch 125/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6892 - g_loss: 0.7545\n",
            "Epoch 126/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6888 - g_loss: 0.7555\n",
            "Epoch 127/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6886 - g_loss: 0.7564\n",
            "Epoch 128/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6889 - g_loss: 0.7539\n",
            "Epoch 129/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6887 - g_loss: 0.7566\n",
            "Epoch 130/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6882 - g_loss: 0.7551\n",
            "Epoch 131/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6891 - g_loss: 0.7558\n",
            "Epoch 132/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6883 - g_loss: 0.7544\n",
            "Epoch 133/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6886 - g_loss: 0.7578\n",
            "Epoch 134/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6880 - g_loss: 0.7569\n",
            "Epoch 135/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6883 - g_loss: 0.7574\n",
            "Epoch 136/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6882 - g_loss: 0.7559\n",
            "Epoch 137/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6886 - g_loss: 0.7549\n",
            "Epoch 138/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6885 - g_loss: 0.7565\n",
            "Epoch 139/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6883 - g_loss: 0.7571\n",
            "Epoch 140/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6883 - g_loss: 0.7566\n",
            "Epoch 141/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6879 - g_loss: 0.7566\n",
            "Epoch 142/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6877 - g_loss: 0.7561\n",
            "Epoch 143/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6877 - g_loss: 0.7565\n",
            "Epoch 144/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6877 - g_loss: 0.7587\n",
            "Epoch 145/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6875 - g_loss: 0.7570\n",
            "Epoch 146/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6874 - g_loss: 0.7557\n",
            "Epoch 147/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6876 - g_loss: 0.7568\n",
            "Epoch 148/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6875 - g_loss: 0.7595\n",
            "Epoch 149/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6877 - g_loss: 0.7569\n",
            "Epoch 150/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6880 - g_loss: 0.7573\n",
            "Epoch 151/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6878 - g_loss: 0.7576\n",
            "Epoch 152/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6876 - g_loss: 0.7571\n",
            "Epoch 153/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6872 - g_loss: 0.7559\n",
            "Epoch 154/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6876 - g_loss: 0.7575\n",
            "Epoch 155/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6879 - g_loss: 0.7582\n",
            "Epoch 156/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6870 - g_loss: 0.7590\n",
            "Epoch 157/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6869 - g_loss: 0.7592\n",
            "Epoch 158/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6877 - g_loss: 0.7579\n",
            "Epoch 159/200\n",
            "253/253 [==============================] - 31s 119ms/step - d_loss: 0.6871 - g_loss: 0.7582\n",
            "Epoch 160/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6876 - g_loss: 0.7563\n",
            "Epoch 161/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6871 - g_loss: 0.7592\n",
            "Epoch 162/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6872 - g_loss: 0.7593\n",
            "Epoch 163/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6874 - g_loss: 0.7557\n",
            "Epoch 164/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6878 - g_loss: 0.7574\n",
            "Epoch 165/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6871 - g_loss: 0.7589\n",
            "Epoch 166/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6868 - g_loss: 0.7571\n",
            "Epoch 167/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6869 - g_loss: 0.7583\n",
            "Epoch 168/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6872 - g_loss: 0.7607\n",
            "Epoch 169/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6869 - g_loss: 0.7599\n",
            "Epoch 170/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6869 - g_loss: 0.7574\n",
            "Epoch 171/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6867 - g_loss: 0.7601\n",
            "Epoch 172/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6869 - g_loss: 0.7584\n",
            "Epoch 173/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6866 - g_loss: 0.7572\n",
            "Epoch 174/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6870 - g_loss: 0.7596\n",
            "Epoch 175/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6869 - g_loss: 0.7595\n",
            "Epoch 176/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6865 - g_loss: 0.7586\n",
            "Epoch 177/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6869 - g_loss: 0.7595\n",
            "Epoch 178/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6866 - g_loss: 0.7582\n",
            "Epoch 179/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6871 - g_loss: 0.7581\n",
            "Epoch 180/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6865 - g_loss: 0.7585\n",
            "Epoch 181/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6864 - g_loss: 0.7583\n",
            "Epoch 182/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6868 - g_loss: 0.7594\n",
            "Epoch 183/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6862 - g_loss: 0.7575\n",
            "Epoch 184/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6866 - g_loss: 0.7609\n",
            "Epoch 185/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6862 - g_loss: 0.7592\n",
            "Epoch 186/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6869 - g_loss: 0.7629\n",
            "Epoch 187/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6863 - g_loss: 0.7606\n",
            "Epoch 188/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6861 - g_loss: 0.7579\n",
            "Epoch 189/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6861 - g_loss: 0.7600\n",
            "Epoch 190/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6869 - g_loss: 0.7590\n",
            "Epoch 191/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6868 - g_loss: 0.7586\n",
            "Epoch 192/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6867 - g_loss: 0.7586\n",
            "Epoch 193/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6859 - g_loss: 0.7619\n",
            "Epoch 194/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6860 - g_loss: 0.7600\n",
            "Epoch 195/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6860 - g_loss: 0.7591\n",
            "Epoch 196/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6856 - g_loss: 0.7615\n",
            "Epoch 197/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6859 - g_loss: 0.7591\n",
            "Epoch 198/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6853 - g_loss: 0.7607\n",
            "Epoch 199/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6864 - g_loss: 0.7589\n",
            "Epoch 200/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6864 - g_loss: 0.7615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "epochs = 200  # In practice, use ~100 epochs \n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/COVID-19_Augmented_COVID_XRayModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/COVID-19_Augmented_COVID_XRayModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X-Ray Non covid model"
      ],
      "metadata": {
        "id": "nBmLr_gV6rxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load images\n",
        "image_size = (128,128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    covid19_dataset + '/X-ray/Non-COVID/', label_mode=None, image_size=image_size, batch_size=16,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su7ByhNwo7nz",
        "outputId": "397df95d-814d-4511-a4df-294ec43a2dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5493 files belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 256"
      ],
      "metadata": {
        "id": "LaDBxus7qXgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(16 * 16 * 128),\n",
        "        layers.Reshape((16, 16, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2fVtNldqkGq",
        "outputId": "955deddb-4765-4af5-8721-ce23f9d8c70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 32768)             8421376   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 32, 32, 256)      524544    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 64, 64, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 128, 128, 1024)   8389632   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 3)       49155     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,482,371\n",
            "Trainable params: 19,482,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_NON_COVID_Xray')\n",
        "\n"
      ],
      "metadata": {
        "id": "BWPqNyImqdbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_COVID_Xray_Non_COVID' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))\n"
      ],
      "metadata": {
        "id": "ZhTfSLMdpHvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100  # In practice, use ~100 epochs \n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/COVID-Data_Augmented_COVID_XrayModelNonCovid/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/COVID-Data_Augmented_COVID_XrayModelNonCovid/Discriminator',save_format='tf')\n"
      ],
      "metadata": {
        "id": "m3xnZDIa6tb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10b6d40-8adb-41aa-c695-447bf51a36b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "344/344 [==============================] - 41s 109ms/step - d_loss: 0.6459 - g_loss: 0.9787\n",
            "Epoch 2/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6792 - g_loss: 0.8639\n",
            "Epoch 3/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6997 - g_loss: 0.6929\n",
            "Epoch 4/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6943 - g_loss: 0.7507\n",
            "Epoch 5/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6871 - g_loss: 0.7922\n",
            "Epoch 6/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6799 - g_loss: 0.7965\n",
            "Epoch 7/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6955 - g_loss: 0.7607\n",
            "Epoch 8/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6935 - g_loss: 0.7476\n",
            "Epoch 9/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6914 - g_loss: 0.7570\n",
            "Epoch 10/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6938 - g_loss: 0.7571\n",
            "Epoch 11/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6835 - g_loss: 0.7700\n",
            "Epoch 12/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6880 - g_loss: 0.7650\n",
            "Epoch 13/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6852 - g_loss: 0.7675\n",
            "Epoch 14/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6910 - g_loss: 0.7535\n",
            "Epoch 15/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6936 - g_loss: 0.7502\n",
            "Epoch 16/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6927 - g_loss: 0.7596\n",
            "Epoch 17/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6938 - g_loss: 0.7491\n",
            "Epoch 18/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6925 - g_loss: 0.7514\n",
            "Epoch 19/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6974 - g_loss: 0.7390\n",
            "Epoch 20/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6922 - g_loss: 0.7508\n",
            "Epoch 21/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6924 - g_loss: 0.7519\n",
            "Epoch 22/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6926 - g_loss: 0.7501\n",
            "Epoch 23/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6927 - g_loss: 0.7471\n",
            "Epoch 24/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6937 - g_loss: 0.7468\n",
            "Epoch 25/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6950 - g_loss: 0.7445\n",
            "Epoch 26/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6925 - g_loss: 0.7517\n",
            "Epoch 27/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6918 - g_loss: 0.7497\n",
            "Epoch 28/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6921 - g_loss: 0.7461\n",
            "Epoch 29/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6942 - g_loss: 0.7441\n",
            "Epoch 30/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6940 - g_loss: 0.7548\n",
            "Epoch 31/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6934 - g_loss: 0.7502\n",
            "Epoch 32/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6925 - g_loss: 0.7522\n",
            "Epoch 33/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6937 - g_loss: 0.7474\n",
            "Epoch 34/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6948 - g_loss: 0.7445\n",
            "Epoch 35/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6943 - g_loss: 0.7523\n",
            "Epoch 36/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6942 - g_loss: 0.7494\n",
            "Epoch 37/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6931 - g_loss: 0.7515\n",
            "Epoch 38/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6934 - g_loss: 0.7486\n",
            "Epoch 39/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6943 - g_loss: 0.7437\n",
            "Epoch 40/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6946 - g_loss: 0.7466\n",
            "Epoch 41/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6943 - g_loss: 0.7480\n",
            "Epoch 42/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6940 - g_loss: 0.7489\n",
            "Epoch 43/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6929 - g_loss: 0.7491\n",
            "Epoch 44/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6927 - g_loss: 0.7510\n",
            "Epoch 45/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6933 - g_loss: 0.7538\n",
            "Epoch 46/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6936 - g_loss: 0.7532\n",
            "Epoch 47/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6931 - g_loss: 0.7500\n",
            "Epoch 48/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6940 - g_loss: 0.7532\n",
            "Epoch 49/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6940 - g_loss: 0.7521\n",
            "Epoch 50/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6933 - g_loss: 0.7512\n",
            "Epoch 51/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6926 - g_loss: 0.7529\n",
            "Epoch 52/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6909 - g_loss: 0.7549\n",
            "Epoch 53/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6910 - g_loss: 0.7536\n",
            "Epoch 54/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6914 - g_loss: 0.7559\n",
            "Epoch 55/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6927 - g_loss: 0.7533\n",
            "Epoch 56/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6924 - g_loss: 0.7540\n",
            "Epoch 57/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6927 - g_loss: 0.7550\n",
            "Epoch 58/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6944 - g_loss: 0.7522\n",
            "Epoch 59/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6964 - g_loss: 0.7512\n",
            "Epoch 60/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6995 - g_loss: 0.7490\n",
            "Epoch 61/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.7016 - g_loss: 0.7497\n",
            "Epoch 62/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.7021 - g_loss: 0.7454\n",
            "Epoch 63/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.7019 - g_loss: 0.7470\n",
            "Epoch 64/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.7019 - g_loss: 0.7477\n",
            "Epoch 65/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.7009 - g_loss: 0.7502\n",
            "Epoch 66/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6994 - g_loss: 0.7471\n",
            "Epoch 67/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6986 - g_loss: 0.7494\n",
            "Epoch 68/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6987 - g_loss: 0.7480\n",
            "Epoch 69/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6972 - g_loss: 0.7504\n",
            "Epoch 70/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6981 - g_loss: 0.7512\n",
            "Epoch 71/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6971 - g_loss: 0.7526\n",
            "Epoch 72/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6978 - g_loss: 0.7486\n",
            "Epoch 73/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6977 - g_loss: 0.7500\n",
            "Epoch 74/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6991 - g_loss: 0.7503\n",
            "Epoch 75/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6992 - g_loss: 0.7498\n",
            "Epoch 76/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6977 - g_loss: 0.7500\n",
            "Epoch 77/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6981 - g_loss: 0.7508\n",
            "Epoch 78/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6978 - g_loss: 0.7499\n",
            "Epoch 79/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6985 - g_loss: 0.7490\n",
            "Epoch 80/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6983 - g_loss: 0.7495\n",
            "Epoch 81/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6985 - g_loss: 0.7484\n",
            "Epoch 82/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6982 - g_loss: 0.7488\n",
            "Epoch 83/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6977 - g_loss: 0.7479\n",
            "Epoch 84/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6978 - g_loss: 0.7486\n",
            "Epoch 85/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6973 - g_loss: 0.7497\n",
            "Epoch 86/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6966 - g_loss: 0.7489\n",
            "Epoch 87/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6963 - g_loss: 0.7500\n",
            "Epoch 88/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6956 - g_loss: 0.7515\n",
            "Epoch 89/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6949 - g_loss: 0.7499\n",
            "Epoch 90/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6947 - g_loss: 0.7525\n",
            "Epoch 91/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6947 - g_loss: 0.7520\n",
            "Epoch 92/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6940 - g_loss: 0.7501\n",
            "Epoch 93/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6936 - g_loss: 0.7521\n",
            "Epoch 94/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6934 - g_loss: 0.7514\n",
            "Epoch 95/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6926 - g_loss: 0.7556\n",
            "Epoch 96/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6926 - g_loss: 0.7528\n",
            "Epoch 97/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6913 - g_loss: 0.7549\n",
            "Epoch 98/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6914 - g_loss: 0.7521\n",
            "Epoch 99/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6912 - g_loss: 0.7563\n",
            "Epoch 100/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6912 - g_loss: 0.7575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CT COVID"
      ],
      "metadata": {
        "id": "LNcHxahEo9_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIHW_gktBCi3",
        "outputId": "d5a491c5-7487-412f-d2a0-28c0452caca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5427 files belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128,128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    covid19_dataset + '/CT/COVID/', label_mode=None, image_size=image_size, batch_size=64,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRhSaoghZHaO"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "url2vqiYZMdf",
        "outputId": "7bc3791a-664e-4d27-ef1f-e658fca32c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "257 4\n"
          ]
        }
      ],
      "source": [
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "print(generator_in_channels, discriminator_in_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cITyyvXVZPr2",
        "outputId": "3bbadfdc-9319-4cd1-f1d8-e682921d5c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_20 (Conv2D)          (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu_32 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_33 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_34 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_11 (Dense)            (None, 32768)             8421376   \n",
            "                                                                 \n",
            " reshape_5 (Reshape)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_17 (Conv2D  (None, 32, 32, 256)      524544    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_35 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_18 (Conv2D  (None, 64, 64, 512)      2097664   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_36 (LeakyReLU)  (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_19 (Conv2D  (None, 128, 128, 1024)   8389632   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_37 (LeakyReLU)  (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 128, 128, 3)       49155     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,482,371\n",
            "Trainable params: 19,482,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(16 * 16 * 128),\n",
        "        layers.Reshape((16, 16, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go-V8ASVZSua"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_COVID_CT')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o0hdV5GZV_k"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/COVID-19 Data Augmented COVID CT' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnpBNr4AZYf_",
        "outputId": "d2cf1621-9511-45e1-9f79-7a9389260251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "85/85 [==============================] - 38s 402ms/step - d_loss: 0.6481 - g_loss: 0.8751\n",
            "Epoch 2/100\n",
            "85/85 [==============================] - 35s 403ms/step - d_loss: 0.6509 - g_loss: 0.8475\n",
            "Epoch 3/100\n",
            "85/85 [==============================] - 35s 403ms/step - d_loss: 0.6708 - g_loss: 0.8067\n",
            "Epoch 4/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6734 - g_loss: 0.7852\n",
            "Epoch 5/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6714 - g_loss: 0.7970\n",
            "Epoch 6/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6750 - g_loss: 0.7838\n",
            "Epoch 7/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6849 - g_loss: 0.7786\n",
            "Epoch 8/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6793 - g_loss: 0.7816\n",
            "Epoch 9/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6639 - g_loss: 0.8048\n",
            "Epoch 10/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6886 - g_loss: 0.7557\n",
            "Epoch 11/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6868 - g_loss: 0.7444\n",
            "Epoch 12/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6657 - g_loss: 0.7874\n",
            "Epoch 13/100\n",
            "85/85 [==============================] - 35s 403ms/step - d_loss: 0.6882 - g_loss: 0.7419\n",
            "Epoch 14/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6852 - g_loss: 0.7438\n",
            "Epoch 15/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6753 - g_loss: 0.7609\n",
            "Epoch 16/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6866 - g_loss: 0.7483\n",
            "Epoch 17/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6759 - g_loss: 0.7586\n",
            "Epoch 18/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6861 - g_loss: 0.7404\n",
            "Epoch 19/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6802 - g_loss: 0.7592\n",
            "Epoch 20/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6738 - g_loss: 0.7727\n",
            "Epoch 21/100\n",
            "85/85 [==============================] - 35s 403ms/step - d_loss: 0.6765 - g_loss: 0.7711\n",
            "Epoch 22/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6805 - g_loss: 0.7634\n",
            "Epoch 23/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6756 - g_loss: 0.7697\n",
            "Epoch 24/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6816 - g_loss: 0.7530\n",
            "Epoch 25/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6877 - g_loss: 0.7490\n",
            "Epoch 26/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6846 - g_loss: 0.7625\n",
            "Epoch 27/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6850 - g_loss: 0.7600\n",
            "Epoch 28/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6796 - g_loss: 0.7664\n",
            "Epoch 29/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6761 - g_loss: 0.7812\n",
            "Epoch 30/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6785 - g_loss: 0.7726\n",
            "Epoch 31/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6777 - g_loss: 0.7714\n",
            "Epoch 32/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6812 - g_loss: 0.7701\n",
            "Epoch 33/100\n",
            "85/85 [==============================] - 35s 404ms/step - d_loss: 0.6829 - g_loss: 0.7622\n",
            "Epoch 34/100\n",
            "85/85 [==============================] - 42s 493ms/step - d_loss: 0.6848 - g_loss: 0.7620\n",
            "Epoch 35/100\n",
            "85/85 [==============================] - 43s 495ms/step - d_loss: 0.6840 - g_loss: 0.7646\n",
            "Epoch 36/100\n",
            "85/85 [==============================] - 42s 490ms/step - d_loss: 0.6858 - g_loss: 0.7642\n",
            "Epoch 37/100\n",
            "85/85 [==============================] - 42s 487ms/step - d_loss: 0.6788 - g_loss: 0.7754\n",
            "Epoch 38/100\n",
            "85/85 [==============================] - 43s 496ms/step - d_loss: 0.6783 - g_loss: 0.7739\n",
            "Epoch 39/100\n",
            "85/85 [==============================] - 42s 493ms/step - d_loss: 0.6746 - g_loss: 0.7812\n",
            "Epoch 40/100\n",
            "85/85 [==============================] - 42s 491ms/step - d_loss: 0.6781 - g_loss: 0.7716\n",
            "Epoch 41/100\n",
            "85/85 [==============================] - 43s 496ms/step - d_loss: 0.6714 - g_loss: 0.7846\n",
            "Epoch 42/100\n",
            "85/85 [==============================] - 42s 491ms/step - d_loss: 0.6753 - g_loss: 0.7765\n",
            "Epoch 43/100\n",
            "85/85 [==============================] - 42s 492ms/step - d_loss: 0.6755 - g_loss: 0.7737\n",
            "Epoch 44/100\n",
            "85/85 [==============================] - 42s 491ms/step - d_loss: 0.6754 - g_loss: 0.7854\n",
            "Epoch 45/100\n",
            "85/85 [==============================] - 42s 492ms/step - d_loss: 0.6744 - g_loss: 0.7851\n",
            "Epoch 46/100\n",
            "85/85 [==============================] - 42s 490ms/step - d_loss: 0.6816 - g_loss: 0.7707\n",
            "Epoch 47/100\n",
            "85/85 [==============================] - 42s 486ms/step - d_loss: 0.6793 - g_loss: 0.7761\n",
            "Epoch 48/100\n",
            "85/85 [==============================] - 43s 496ms/step - d_loss: 0.6800 - g_loss: 0.7825\n",
            "Epoch 49/100\n",
            "85/85 [==============================] - 42s 491ms/step - d_loss: 0.6802 - g_loss: 0.7643\n",
            "Epoch 50/100\n",
            "85/85 [==============================] - 43s 496ms/step - d_loss: 0.6816 - g_loss: 0.7687\n",
            "Epoch 51/100\n",
            "85/85 [==============================] - 42s 490ms/step - d_loss: 0.6819 - g_loss: 0.7709\n",
            "Epoch 52/100\n",
            "85/85 [==============================] - 42s 489ms/step - d_loss: 0.6791 - g_loss: 0.7743\n",
            "Epoch 53/100\n",
            "85/85 [==============================] - 42s 490ms/step - d_loss: 0.6806 - g_loss: 0.7662\n",
            "Epoch 54/100\n",
            "85/85 [==============================] - 42s 488ms/step - d_loss: 0.6800 - g_loss: 0.7726\n",
            "Epoch 55/100\n",
            "85/85 [==============================] - 42s 493ms/step - d_loss: 0.6779 - g_loss: 0.7688\n",
            "Epoch 56/100\n",
            "85/85 [==============================] - 42s 486ms/step - d_loss: 0.6790 - g_loss: 0.7745\n",
            "Epoch 57/100\n",
            "85/85 [==============================] - 42s 489ms/step - d_loss: 0.6772 - g_loss: 0.7770\n",
            "Epoch 58/100\n",
            "85/85 [==============================] - 42s 490ms/step - d_loss: 0.6788 - g_loss: 0.7724\n",
            "Epoch 59/100\n",
            "85/85 [==============================] - 42s 489ms/step - d_loss: 0.6779 - g_loss: 0.7715\n",
            "Epoch 60/100\n",
            "85/85 [==============================] - 42s 494ms/step - d_loss: 0.6787 - g_loss: 0.7692\n",
            "Epoch 61/100\n",
            "85/85 [==============================] - 42s 489ms/step - d_loss: 0.6778 - g_loss: 0.7774\n",
            "Epoch 62/100\n",
            "85/85 [==============================] - 42s 487ms/step - d_loss: 0.6770 - g_loss: 0.7711\n",
            "Epoch 63/100\n",
            "85/85 [==============================] - 42s 486ms/step - d_loss: 0.6782 - g_loss: 0.7712\n",
            "Epoch 64/100\n",
            "85/85 [==============================] - 42s 491ms/step - d_loss: 0.6793 - g_loss: 0.7689\n",
            "Epoch 65/100\n",
            "85/85 [==============================] - 42s 483ms/step - d_loss: 0.6783 - g_loss: 0.7721\n",
            "Epoch 66/100\n",
            "85/85 [==============================] - 42s 486ms/step - d_loss: 0.6772 - g_loss: 0.7759\n",
            "Epoch 67/100\n",
            "85/85 [==============================] - 42s 490ms/step - d_loss: 0.6762 - g_loss: 0.7748\n",
            "Epoch 68/100\n",
            "85/85 [==============================] - 42s 490ms/step - d_loss: 0.6763 - g_loss: 0.7689\n",
            "Epoch 69/100\n",
            "85/85 [==============================] - 42s 491ms/step - d_loss: 0.6764 - g_loss: 0.7737\n",
            "Epoch 70/100\n",
            "85/85 [==============================] - 43s 496ms/step - d_loss: 0.6767 - g_loss: 0.7754\n",
            "Epoch 71/100\n",
            "85/85 [==============================] - 42s 486ms/step - d_loss: 0.6772 - g_loss: 0.7756\n",
            "Epoch 72/100\n",
            "85/85 [==============================] - 42s 483ms/step - d_loss: 0.6758 - g_loss: 0.7732\n",
            "Epoch 73/100\n",
            "85/85 [==============================] - 42s 491ms/step - d_loss: 0.6750 - g_loss: 0.7731\n",
            "Epoch 74/100\n",
            "85/85 [==============================] - 42s 486ms/step - d_loss: 0.6763 - g_loss: 0.7747\n",
            "Epoch 75/100\n",
            "85/85 [==============================] - 42s 484ms/step - d_loss: 0.6774 - g_loss: 0.7774\n",
            "Epoch 76/100\n",
            "85/85 [==============================] - 43s 497ms/step - d_loss: 0.6769 - g_loss: 0.7658\n",
            "Epoch 77/100\n",
            "85/85 [==============================] - 43s 494ms/step - d_loss: 0.6769 - g_loss: 0.7702\n",
            "Epoch 78/100\n",
            "85/85 [==============================] - 42s 489ms/step - d_loss: 0.6774 - g_loss: 0.7735\n",
            "Epoch 79/100\n",
            "85/85 [==============================] - 43s 497ms/step - d_loss: 0.6778 - g_loss: 0.7728\n",
            "Epoch 80/100\n",
            "85/85 [==============================] - 43s 493ms/step - d_loss: 0.6774 - g_loss: 0.7738\n",
            "Epoch 81/100\n",
            "85/85 [==============================] - 42s 486ms/step - d_loss: 0.6769 - g_loss: 0.7746\n",
            "Epoch 82/100\n",
            "85/85 [==============================] - 43s 495ms/step - d_loss: 0.6779 - g_loss: 0.7682\n",
            "Epoch 83/100\n",
            "85/85 [==============================] - 43s 495ms/step - d_loss: 0.6788 - g_loss: 0.7749\n",
            "Epoch 84/100\n",
            "85/85 [==============================] - 41s 479ms/step - d_loss: 0.6782 - g_loss: 0.7713\n",
            "Epoch 85/100\n",
            "85/85 [==============================] - 42s 488ms/step - d_loss: 0.6810 - g_loss: 0.7692\n",
            "Epoch 86/100\n",
            "85/85 [==============================] - 42s 487ms/step - d_loss: 0.6845 - g_loss: 0.7598\n",
            "Epoch 87/100\n",
            "85/85 [==============================] - 42s 488ms/step - d_loss: 0.6875 - g_loss: 0.7649\n",
            "Epoch 88/100\n",
            "85/85 [==============================] - 42s 486ms/step - d_loss: 0.6899 - g_loss: 0.7594\n",
            "Epoch 89/100\n",
            "85/85 [==============================] - 42s 490ms/step - d_loss: 0.6929 - g_loss: 0.7535\n",
            "Epoch 90/100\n",
            "85/85 [==============================] - 42s 487ms/step - d_loss: 0.6961 - g_loss: 0.7541\n",
            "Epoch 91/100\n",
            "85/85 [==============================] - 42s 493ms/step - d_loss: 0.6970 - g_loss: 0.7534\n",
            "Epoch 92/100\n",
            "85/85 [==============================] - 42s 492ms/step - d_loss: 0.6962 - g_loss: 0.7574\n",
            "Epoch 93/100\n",
            "85/85 [==============================] - 42s 491ms/step - d_loss: 0.6936 - g_loss: 0.7648\n",
            "Epoch 94/100\n",
            "85/85 [==============================] - 42s 494ms/step - d_loss: 0.6905 - g_loss: 0.7665\n",
            "Epoch 95/100\n",
            "85/85 [==============================] - 42s 486ms/step - d_loss: 0.6890 - g_loss: 0.7753\n",
            "Epoch 96/100\n",
            "85/85 [==============================] - 43s 495ms/step - d_loss: 0.6907 - g_loss: 0.7697\n",
            "Epoch 97/100\n",
            "85/85 [==============================] - 42s 490ms/step - d_loss: 0.6908 - g_loss: 0.7619\n",
            "Epoch 98/100\n",
            "85/85 [==============================] - 43s 496ms/step - d_loss: 0.6920 - g_loss: 0.7620\n",
            "Epoch 99/100\n",
            "85/85 [==============================] - 43s 498ms/step - d_loss: 0.6933 - g_loss: 0.7567\n",
            "Epoch 100/100\n",
            "85/85 [==============================] - 43s 495ms/step - d_loss: 0.6944 - g_loss: 0.7582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "epochs = 100  # In practice, use ~100 epochs \n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/COVID-19_Augmented_COVID_CTModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/COVID-19_Augmented_COVID_CTModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CT Non COVID"
      ],
      "metadata": {
        "id": "AoKCk7WXpA-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load images\n",
        "image_size = (128,128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    covid19_dataset + '/CT/Non-COVID/', label_mode=None, image_size=image_size, batch_size=64,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqE5Czr6pEDY",
        "outputId": "c0ee9036-24a5-4f6d-c9cc-61510875f524"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2627 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 100"
      ],
      "metadata": {
        "id": "hUNd0_vzqp1Q"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(16 * 16 * 128),\n",
        "        layers.Reshape((16, 16, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uXvjGylqqZU",
        "outputId": "897db29b-e971-4d34-fb0f-44730c19ee54"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_32 (Conv2D)          (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu_50 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_51 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_52 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_17 (Dense)            (None, 32768)             3309568   \n",
            "                                                                 \n",
            " reshape_8 (Reshape)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_26 (Conv2D  (None, 32, 32, 256)      524544    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_53 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_27 (Conv2D  (None, 64, 64, 512)      2097664   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_54 (LeakyReLU)  (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_28 (Conv2D  (None, 128, 128, 1024)   8389632   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_55 (LeakyReLU)  (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 128, 128, 3)       49155     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,370,563\n",
            "Trainable params: 14,370,563\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_NON_COVID_CT')\n",
        "\n"
      ],
      "metadata": {
        "id": "5IfuY-ZEqqmR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "TfCFY6DvZpNp"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_COVID_CT_Non_COVID' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100  # In practice, use ~100 epochs \n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_COVID_CTModelNonCovid/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_COVID_CTModelNonCovid/Discriminator',save_format='tf')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QJk4Zwip5yv",
        "outputId": "7b73760f-915b-4f85-f466-73423afdda73"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "42/42 [==============================] - 20s 399ms/step - d_loss: 0.5517 - g_loss: 0.6872\n",
            "Epoch 2/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.4608 - g_loss: 0.6385\n",
            "Epoch 3/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.4884 - g_loss: 0.5905\n",
            "Epoch 4/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.5075 - g_loss: 0.5908\n",
            "Epoch 5/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.4830 - g_loss: 0.6633\n",
            "Epoch 6/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.4240 - g_loss: 0.7692\n",
            "Epoch 7/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.3804 - g_loss: 0.8234\n",
            "Epoch 8/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.3627 - g_loss: 0.8384\n",
            "Epoch 9/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.3512 - g_loss: 0.8497\n",
            "Epoch 10/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.3351 - g_loss: 0.8848\n",
            "Epoch 11/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.3054 - g_loss: 0.9605\n",
            "Epoch 12/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.2557 - g_loss: 1.1248\n",
            "Epoch 13/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.1984 - g_loss: 1.3756\n",
            "Epoch 14/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.1462 - g_loss: 1.7194\n",
            "Epoch 15/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.0984 - g_loss: 2.2149\n",
            "Epoch 16/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.0684 - g_loss: 2.7149\n",
            "Epoch 17/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.0771 - g_loss: 2.7507\n",
            "Epoch 18/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.3187 - g_loss: 1.3422\n",
            "Epoch 19/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.4176 - g_loss: 1.2234\n",
            "Epoch 20/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.4376 - g_loss: 1.3753\n",
            "Epoch 21/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6610 - g_loss: 1.0499\n",
            "Epoch 22/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7969 - g_loss: 0.7940\n",
            "Epoch 23/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.9079 - g_loss: 0.6743\n",
            "Epoch 24/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 1.0140 - g_loss: 0.5615\n",
            "Epoch 25/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.8863 - g_loss: 0.6822\n",
            "Epoch 26/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7548 - g_loss: 0.8840\n",
            "Epoch 27/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7698 - g_loss: 0.9848\n",
            "Epoch 28/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.7749 - g_loss: 1.0039\n",
            "Epoch 29/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7813 - g_loss: 0.9896\n",
            "Epoch 30/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.8111 - g_loss: 0.9150\n",
            "Epoch 31/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.8431 - g_loss: 0.8163\n",
            "Epoch 32/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.8365 - g_loss: 0.7990\n",
            "Epoch 33/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.8308 - g_loss: 0.8134\n",
            "Epoch 34/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.8269 - g_loss: 0.8327\n",
            "Epoch 35/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7986 - g_loss: 0.8099\n",
            "Epoch 36/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7711 - g_loss: 0.8373\n",
            "Epoch 37/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.7532 - g_loss: 0.8555\n",
            "Epoch 38/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7710 - g_loss: 0.8179\n",
            "Epoch 39/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7573 - g_loss: 0.8231\n",
            "Epoch 40/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.7207 - g_loss: 0.8565\n",
            "Epoch 41/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7316 - g_loss: 0.8400\n",
            "Epoch 42/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7460 - g_loss: 0.8030\n",
            "Epoch 43/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7252 - g_loss: 0.8339\n",
            "Epoch 44/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7039 - g_loss: 0.8770\n",
            "Epoch 45/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7197 - g_loss: 0.8576\n",
            "Epoch 46/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7308 - g_loss: 0.8124\n",
            "Epoch 47/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6989 - g_loss: 0.8294\n",
            "Epoch 48/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6931 - g_loss: 0.8448\n",
            "Epoch 49/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7161 - g_loss: 0.8030\n",
            "Epoch 50/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6916 - g_loss: 0.8225\n",
            "Epoch 51/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6864 - g_loss: 0.8486\n",
            "Epoch 52/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6931 - g_loss: 0.8280\n",
            "Epoch 53/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6875 - g_loss: 0.8380\n",
            "Epoch 54/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6804 - g_loss: 0.8170\n",
            "Epoch 55/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6803 - g_loss: 0.8277\n",
            "Epoch 56/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6757 - g_loss: 0.8317\n",
            "Epoch 57/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6679 - g_loss: 0.8516\n",
            "Epoch 58/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6781 - g_loss: 0.8413\n",
            "Epoch 59/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6797 - g_loss: 0.8304\n",
            "Epoch 60/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6815 - g_loss: 0.8331\n",
            "Epoch 61/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6800 - g_loss: 0.8248\n",
            "Epoch 62/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6919 - g_loss: 0.8208\n",
            "Epoch 63/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7029 - g_loss: 0.8027\n",
            "Epoch 64/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6973 - g_loss: 0.8339\n",
            "Epoch 65/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7011 - g_loss: 0.8142\n",
            "Epoch 66/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7057 - g_loss: 0.7952\n",
            "Epoch 67/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7035 - g_loss: 0.7972\n",
            "Epoch 68/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6844 - g_loss: 0.8102\n",
            "Epoch 69/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6874 - g_loss: 0.8204\n",
            "Epoch 70/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6869 - g_loss: 0.8037\n",
            "Epoch 71/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6721 - g_loss: 0.8152\n",
            "Epoch 72/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6781 - g_loss: 0.8152\n",
            "Epoch 73/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6962 - g_loss: 0.7826\n",
            "Epoch 74/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6861 - g_loss: 0.7929\n",
            "Epoch 75/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6827 - g_loss: 0.8109\n",
            "Epoch 76/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6951 - g_loss: 0.7804\n",
            "Epoch 77/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6913 - g_loss: 0.7897\n",
            "Epoch 78/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6772 - g_loss: 0.8197\n",
            "Epoch 79/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6953 - g_loss: 0.8135\n",
            "Epoch 80/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7007 - g_loss: 0.7828\n",
            "Epoch 81/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6863 - g_loss: 0.8129\n",
            "Epoch 82/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6860 - g_loss: 0.8209\n",
            "Epoch 83/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6982 - g_loss: 0.7974\n",
            "Epoch 84/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6906 - g_loss: 0.7812\n",
            "Epoch 85/100\n",
            "42/42 [==============================] - 20s 453ms/step - d_loss: 0.6714 - g_loss: 0.8143\n",
            "Epoch 86/100\n",
            "42/42 [==============================] - 19s 444ms/step - d_loss: 0.6768 - g_loss: 0.8081\n",
            "Epoch 87/100\n",
            "42/42 [==============================] - 19s 449ms/step - d_loss: 0.6715 - g_loss: 0.8023\n",
            "Epoch 88/100\n",
            "42/42 [==============================] - 19s 450ms/step - d_loss: 0.6507 - g_loss: 0.8182\n",
            "Epoch 89/100\n",
            "42/42 [==============================] - 19s 447ms/step - d_loss: 0.6605 - g_loss: 0.8100\n",
            "Epoch 90/100\n",
            "42/42 [==============================] - 20s 472ms/step - d_loss: 0.6690 - g_loss: 0.7894\n",
            "Epoch 91/100\n",
            "42/42 [==============================] - 19s 446ms/step - d_loss: 0.6652 - g_loss: 0.7723\n",
            "Epoch 92/100\n",
            "42/42 [==============================] - 19s 446ms/step - d_loss: 0.6720 - g_loss: 0.7879\n",
            "Epoch 93/100\n",
            "42/42 [==============================] - 19s 448ms/step - d_loss: 0.6827 - g_loss: 0.7636\n",
            "Epoch 94/100\n",
            "42/42 [==============================] - 19s 447ms/step - d_loss: 0.6774 - g_loss: 0.7598\n",
            "Epoch 95/100\n",
            "42/42 [==============================] - 20s 457ms/step - d_loss: 0.6754 - g_loss: 0.7675\n",
            "Epoch 96/100\n",
            "42/42 [==============================] - 20s 452ms/step - d_loss: 0.6801 - g_loss: 0.7738\n",
            "Epoch 97/100\n",
            "42/42 [==============================] - 19s 447ms/step - d_loss: 0.6815 - g_loss: 0.7606\n",
            "Epoch 98/100\n",
            "42/42 [==============================] - 19s 445ms/step - d_loss: 0.6912 - g_loss: 0.7715\n",
            "Epoch 99/100\n",
            "42/42 [==============================] - 21s 492ms/step - d_loss: 0.6833 - g_loss: 0.7890\n",
            "Epoch 100/100\n",
            "42/42 [==============================] - 19s 446ms/step - d_loss: 0.6643 - g_loss: 0.8297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nA0cqo3src2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}