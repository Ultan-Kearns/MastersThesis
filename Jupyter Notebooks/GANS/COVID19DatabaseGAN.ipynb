{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xjyfa03-ZQT",
        "outputId": "7267b09f-5abf-4aa8-cc61-a5b732032e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n",
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons, install\n",
            "Successfully installed install-1.3.5 tensorflow-addons-0.19.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install pip install tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOY_iJ28-tjq",
        "outputId": "8e596e1d-623b-4827-bf38-03b2ecf8fb5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CT  X-ray\n"
          ]
        }
      ],
      "source": [
        "import keras \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from enum import Enum\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "# for reproducibility - ref https://machinelearningmastery.com/reproducible-results-neural-networks-keras/ and https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "np.random.seed(9)\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "# loading data from gdrive\n",
        "covid19_dataset = os.path.abspath(\"/content/gdrive/My Drive/COVID-19 Dataset/\")\n",
        "\n",
        "!ls \"/content/gdrive/My Drive/COVID-19 Dataset/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Y1hJwcCMAwmM",
        "outputId": "e3876f9e-70b7-4dcc-a85c-defc75636249"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#@markdown #**Anti-Disconnect for Google Colab**\n",
        "#@markdown ## Run this to stop it from disconnecting automatically \n",
        "#@markdown  **(It will anyhow disconnect after 6 - 12 hrs for using the free version of Colab.)**\n",
        "#@markdown  *(Colab Pro users will get about 24 hrs usage time)*\n",
        "#@markdown ---\n",
        "# taken from https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP_%28z%2Bquantize_method_with_augmentations%2C_user_friendly_interface%29.ipynb#scrollTo=XHyPd4oxVp_l stops colab disconnecting\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "IPython.display.Javascript(js_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qFzgcC1_AnH",
        "outputId": "fbfc347a-8087-47c7-9688-8b8437291eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4044 files belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128,128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    covid19_dataset + '/X-ray/COVID/', label_mode=None, image_size=image_size, batch_size=16,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6M1w4bJp_tAF"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVdcu5MM_vbv",
        "outputId": "8eba8915-d7d6-4398-d771-472f79ae757a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "257 4\n"
          ]
        }
      ],
      "source": [
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "print(generator_in_channels, discriminator_in_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRcx8khQ_1Mg",
        "outputId": "99479aaf-11ca-42a9-f2f5-315a173c4ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_14 (Conv2D)          (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu_22 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_23 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_24 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 32768)             8421376   \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_11 (Conv2D  (None, 32, 32, 256)      524544    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_25 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_12 (Conv2D  (None, 64, 64, 512)      2097664   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_26 (LeakyReLU)  (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_13 (Conv2D  (None, 128, 128, 1024)   8389632   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_27 (LeakyReLU)  (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 128, 128, 3)       49155     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,482,371\n",
            "Trainable params: 19,482,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(16 * 16 * 128),\n",
        "        layers.Reshape((16, 16, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ueLYo9Ww_3m1"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_COVID_Xray')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bQZGmdVT_8K4"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/COVID-19 Data Augmented COVID X-Ray' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNH_74E2_-Bp",
        "outputId": "e41dd918-4c42-4b69-d752-cf23ef7d676b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "253/253 [==============================] - 31s 109ms/step - d_loss: 0.6906 - g_loss: 0.7612\n",
            "Epoch 2/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6922 - g_loss: 0.7454\n",
            "Epoch 3/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6912 - g_loss: 0.7542\n",
            "Epoch 4/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6895 - g_loss: 0.7526\n",
            "Epoch 5/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6872 - g_loss: 0.7603\n",
            "Epoch 6/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6870 - g_loss: 0.7561\n",
            "Epoch 7/200\n",
            "253/253 [==============================] - 28s 109ms/step - d_loss: 0.6883 - g_loss: 0.7535\n",
            "Epoch 8/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6888 - g_loss: 0.7562\n",
            "Epoch 9/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6900 - g_loss: 0.7492\n",
            "Epoch 10/200\n",
            "253/253 [==============================] - 28s 109ms/step - d_loss: 0.6914 - g_loss: 0.7529\n",
            "Epoch 11/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6915 - g_loss: 0.7492\n",
            "Epoch 12/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6915 - g_loss: 0.7509\n",
            "Epoch 13/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6909 - g_loss: 0.7500\n",
            "Epoch 14/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6919 - g_loss: 0.7491\n",
            "Epoch 15/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6931 - g_loss: 0.7456\n",
            "Epoch 16/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6926 - g_loss: 0.7525\n",
            "Epoch 17/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6925 - g_loss: 0.7497\n",
            "Epoch 18/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6925 - g_loss: 0.7510\n",
            "Epoch 19/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6935 - g_loss: 0.7504\n",
            "Epoch 20/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6939 - g_loss: 0.7491\n",
            "Epoch 21/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6944 - g_loss: 0.7487\n",
            "Epoch 22/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6948 - g_loss: 0.7499\n",
            "Epoch 23/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6942 - g_loss: 0.7516\n",
            "Epoch 24/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6953 - g_loss: 0.7524\n",
            "Epoch 25/200\n",
            "253/253 [==============================] - 28s 109ms/step - d_loss: 0.6951 - g_loss: 0.7522\n",
            "Epoch 26/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6968 - g_loss: 0.7470\n",
            "Epoch 27/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6973 - g_loss: 0.7450\n",
            "Epoch 28/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6965 - g_loss: 0.7469\n",
            "Epoch 29/200\n",
            "253/253 [==============================] - 28s 109ms/step - d_loss: 0.6965 - g_loss: 0.7474\n",
            "Epoch 30/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6973 - g_loss: 0.7444\n",
            "Epoch 31/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6977 - g_loss: 0.7458\n",
            "Epoch 32/200\n",
            "253/253 [==============================] - 28s 110ms/step - d_loss: 0.6966 - g_loss: 0.7480\n",
            "Epoch 33/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6964 - g_loss: 0.7493\n",
            "Epoch 34/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6966 - g_loss: 0.7489\n",
            "Epoch 35/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6967 - g_loss: 0.7493\n",
            "Epoch 36/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6965 - g_loss: 0.7496\n",
            "Epoch 37/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6977 - g_loss: 0.7493\n",
            "Epoch 38/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6986 - g_loss: 0.7490\n",
            "Epoch 39/200\n",
            "253/253 [==============================] - 31s 118ms/step - d_loss: 0.6997 - g_loss: 0.7464\n",
            "Epoch 40/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7008 - g_loss: 0.7467\n",
            "Epoch 41/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7013 - g_loss: 0.7448\n",
            "Epoch 42/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7035 - g_loss: 0.7421\n",
            "Epoch 43/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7044 - g_loss: 0.7412\n",
            "Epoch 44/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7052 - g_loss: 0.7418\n",
            "Epoch 45/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7049 - g_loss: 0.7407\n",
            "Epoch 46/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7038 - g_loss: 0.7426\n",
            "Epoch 47/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7031 - g_loss: 0.7447\n",
            "Epoch 48/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7020 - g_loss: 0.7453\n",
            "Epoch 49/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7015 - g_loss: 0.7444\n",
            "Epoch 50/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7009 - g_loss: 0.7463\n",
            "Epoch 51/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7008 - g_loss: 0.7458\n",
            "Epoch 52/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6995 - g_loss: 0.7458\n",
            "Epoch 53/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6998 - g_loss: 0.7486\n",
            "Epoch 54/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7009 - g_loss: 0.7434\n",
            "Epoch 55/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.7005 - g_loss: 0.7452\n",
            "Epoch 56/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7000 - g_loss: 0.7474\n",
            "Epoch 57/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6999 - g_loss: 0.7479\n",
            "Epoch 58/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.7000 - g_loss: 0.7458\n",
            "Epoch 59/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6994 - g_loss: 0.7461\n",
            "Epoch 60/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6995 - g_loss: 0.7465\n",
            "Epoch 61/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6990 - g_loss: 0.7496\n",
            "Epoch 62/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6985 - g_loss: 0.7480\n",
            "Epoch 63/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6987 - g_loss: 0.7452\n",
            "Epoch 64/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6979 - g_loss: 0.7477\n",
            "Epoch 65/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6986 - g_loss: 0.7471\n",
            "Epoch 66/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6984 - g_loss: 0.7489\n",
            "Epoch 67/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6987 - g_loss: 0.7471\n",
            "Epoch 68/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6987 - g_loss: 0.7491\n",
            "Epoch 69/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6983 - g_loss: 0.7470\n",
            "Epoch 70/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6982 - g_loss: 0.7475\n",
            "Epoch 71/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6975 - g_loss: 0.7482\n",
            "Epoch 72/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6975 - g_loss: 0.7486\n",
            "Epoch 73/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6971 - g_loss: 0.7504\n",
            "Epoch 74/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6966 - g_loss: 0.7491\n",
            "Epoch 75/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6967 - g_loss: 0.7501\n",
            "Epoch 76/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6964 - g_loss: 0.7511\n",
            "Epoch 77/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6960 - g_loss: 0.7496\n",
            "Epoch 78/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6959 - g_loss: 0.7506\n",
            "Epoch 79/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6958 - g_loss: 0.7500\n",
            "Epoch 80/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6957 - g_loss: 0.7522\n",
            "Epoch 81/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6955 - g_loss: 0.7486\n",
            "Epoch 82/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6954 - g_loss: 0.7511\n",
            "Epoch 83/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6955 - g_loss: 0.7493\n",
            "Epoch 84/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6945 - g_loss: 0.7506\n",
            "Epoch 85/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6947 - g_loss: 0.7509\n",
            "Epoch 86/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6948 - g_loss: 0.7497\n",
            "Epoch 87/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6942 - g_loss: 0.7508\n",
            "Epoch 88/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6940 - g_loss: 0.7513\n",
            "Epoch 89/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6937 - g_loss: 0.7507\n",
            "Epoch 90/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6938 - g_loss: 0.7517\n",
            "Epoch 91/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6933 - g_loss: 0.7511\n",
            "Epoch 92/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6936 - g_loss: 0.7513\n",
            "Epoch 93/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6929 - g_loss: 0.7521\n",
            "Epoch 94/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6932 - g_loss: 0.7515\n",
            "Epoch 95/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6928 - g_loss: 0.7496\n",
            "Epoch 96/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6930 - g_loss: 0.7522\n",
            "Epoch 97/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6927 - g_loss: 0.7509\n",
            "Epoch 98/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6924 - g_loss: 0.7525\n",
            "Epoch 99/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6924 - g_loss: 0.7512\n",
            "Epoch 100/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6923 - g_loss: 0.7529\n",
            "Epoch 101/200\n",
            "253/253 [==============================] - 32s 124ms/step - d_loss: 0.6915 - g_loss: 0.7524\n",
            "Epoch 102/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6917 - g_loss: 0.7517\n",
            "Epoch 103/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6910 - g_loss: 0.7537\n",
            "Epoch 104/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6916 - g_loss: 0.7516\n",
            "Epoch 105/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6913 - g_loss: 0.7538\n",
            "Epoch 106/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6910 - g_loss: 0.7533\n",
            "Epoch 107/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6908 - g_loss: 0.7524\n",
            "Epoch 108/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6910 - g_loss: 0.7530\n",
            "Epoch 109/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6903 - g_loss: 0.7538\n",
            "Epoch 110/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6905 - g_loss: 0.7545\n",
            "Epoch 111/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6900 - g_loss: 0.7547\n",
            "Epoch 112/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6896 - g_loss: 0.7529\n",
            "Epoch 113/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6898 - g_loss: 0.7565\n",
            "Epoch 114/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6897 - g_loss: 0.7531\n",
            "Epoch 115/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6897 - g_loss: 0.7536\n",
            "Epoch 116/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6900 - g_loss: 0.7567\n",
            "Epoch 117/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6900 - g_loss: 0.7538\n",
            "Epoch 118/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6895 - g_loss: 0.7548\n",
            "Epoch 119/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6897 - g_loss: 0.7557\n",
            "Epoch 120/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6892 - g_loss: 0.7537\n",
            "Epoch 121/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6892 - g_loss: 0.7544\n",
            "Epoch 122/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6892 - g_loss: 0.7562\n",
            "Epoch 123/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6892 - g_loss: 0.7538\n",
            "Epoch 124/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6885 - g_loss: 0.7561\n",
            "Epoch 125/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6892 - g_loss: 0.7545\n",
            "Epoch 126/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6888 - g_loss: 0.7555\n",
            "Epoch 127/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6886 - g_loss: 0.7564\n",
            "Epoch 128/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6889 - g_loss: 0.7539\n",
            "Epoch 129/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6887 - g_loss: 0.7566\n",
            "Epoch 130/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6882 - g_loss: 0.7551\n",
            "Epoch 131/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6891 - g_loss: 0.7558\n",
            "Epoch 132/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6883 - g_loss: 0.7544\n",
            "Epoch 133/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6886 - g_loss: 0.7578\n",
            "Epoch 134/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6880 - g_loss: 0.7569\n",
            "Epoch 135/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6883 - g_loss: 0.7574\n",
            "Epoch 136/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6882 - g_loss: 0.7559\n",
            "Epoch 137/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6886 - g_loss: 0.7549\n",
            "Epoch 138/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6885 - g_loss: 0.7565\n",
            "Epoch 139/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6883 - g_loss: 0.7571\n",
            "Epoch 140/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6883 - g_loss: 0.7566\n",
            "Epoch 141/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6879 - g_loss: 0.7566\n",
            "Epoch 142/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6877 - g_loss: 0.7561\n",
            "Epoch 143/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6877 - g_loss: 0.7565\n",
            "Epoch 144/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6877 - g_loss: 0.7587\n",
            "Epoch 145/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6875 - g_loss: 0.7570\n",
            "Epoch 146/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6874 - g_loss: 0.7557\n",
            "Epoch 147/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6876 - g_loss: 0.7568\n",
            "Epoch 148/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6875 - g_loss: 0.7595\n",
            "Epoch 149/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6877 - g_loss: 0.7569\n",
            "Epoch 150/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6880 - g_loss: 0.7573\n",
            "Epoch 151/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6878 - g_loss: 0.7576\n",
            "Epoch 152/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6876 - g_loss: 0.7571\n",
            "Epoch 153/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6872 - g_loss: 0.7559\n",
            "Epoch 154/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6876 - g_loss: 0.7575\n",
            "Epoch 155/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6879 - g_loss: 0.7582\n",
            "Epoch 156/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6870 - g_loss: 0.7590\n",
            "Epoch 157/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6869 - g_loss: 0.7592\n",
            "Epoch 158/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6877 - g_loss: 0.7579\n",
            "Epoch 159/200\n",
            "253/253 [==============================] - 31s 119ms/step - d_loss: 0.6871 - g_loss: 0.7582\n",
            "Epoch 160/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6876 - g_loss: 0.7563\n",
            "Epoch 161/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6871 - g_loss: 0.7592\n",
            "Epoch 162/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6872 - g_loss: 0.7593\n",
            "Epoch 163/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6874 - g_loss: 0.7557\n",
            "Epoch 164/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6878 - g_loss: 0.7574\n",
            "Epoch 165/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6871 - g_loss: 0.7589\n",
            "Epoch 166/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6868 - g_loss: 0.7571\n",
            "Epoch 167/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6869 - g_loss: 0.7583\n",
            "Epoch 168/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6872 - g_loss: 0.7607\n",
            "Epoch 169/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6869 - g_loss: 0.7599\n",
            "Epoch 170/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6869 - g_loss: 0.7574\n",
            "Epoch 171/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6867 - g_loss: 0.7601\n",
            "Epoch 172/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6869 - g_loss: 0.7584\n",
            "Epoch 173/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6866 - g_loss: 0.7572\n",
            "Epoch 174/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6870 - g_loss: 0.7596\n",
            "Epoch 175/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6869 - g_loss: 0.7595\n",
            "Epoch 176/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6865 - g_loss: 0.7586\n",
            "Epoch 177/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6869 - g_loss: 0.7595\n",
            "Epoch 178/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6866 - g_loss: 0.7582\n",
            "Epoch 179/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6871 - g_loss: 0.7581\n",
            "Epoch 180/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6865 - g_loss: 0.7585\n",
            "Epoch 181/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6864 - g_loss: 0.7583\n",
            "Epoch 182/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6868 - g_loss: 0.7594\n",
            "Epoch 183/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6862 - g_loss: 0.7575\n",
            "Epoch 184/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6866 - g_loss: 0.7609\n",
            "Epoch 185/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6862 - g_loss: 0.7592\n",
            "Epoch 186/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6869 - g_loss: 0.7629\n",
            "Epoch 187/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6863 - g_loss: 0.7606\n",
            "Epoch 188/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6861 - g_loss: 0.7579\n",
            "Epoch 189/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6861 - g_loss: 0.7600\n",
            "Epoch 190/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6869 - g_loss: 0.7590\n",
            "Epoch 191/200\n",
            "253/253 [==============================] - 30s 116ms/step - d_loss: 0.6868 - g_loss: 0.7586\n",
            "Epoch 192/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6867 - g_loss: 0.7586\n",
            "Epoch 193/200\n",
            "253/253 [==============================] - 30s 118ms/step - d_loss: 0.6859 - g_loss: 0.7619\n",
            "Epoch 194/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6860 - g_loss: 0.7600\n",
            "Epoch 195/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6860 - g_loss: 0.7591\n",
            "Epoch 196/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6856 - g_loss: 0.7615\n",
            "Epoch 197/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6859 - g_loss: 0.7591\n",
            "Epoch 198/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6853 - g_loss: 0.7607\n",
            "Epoch 199/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6864 - g_loss: 0.7589\n",
            "Epoch 200/200\n",
            "253/253 [==============================] - 30s 117ms/step - d_loss: 0.6864 - g_loss: 0.7615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "epochs = 200  # In practice, use ~100 epochs \n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/COVID-19_Augmented_COVID_XRayModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/COVID-19_Augmented_COVID_XRayModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIHW_gktBCi3",
        "outputId": "df9680d8-c65a-4b24-f43a-c396c52b5dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5427 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128,128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    covid19_dataset + '/CT/COVID/', label_mode=None, image_size=image_size, batch_size=64,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dRhSaoghZHaO"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "url2vqiYZMdf",
        "outputId": "0165065f-bc3d-4138-dffa-92bf2adb023e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "257 4\n"
          ]
        }
      ],
      "source": [
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "print(generator_in_channels, discriminator_in_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cITyyvXVZPr2",
        "outputId": "592693c0-86bb-44a9-a40d-c88e0f10354e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_18 (Conv2D)          (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu_28 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_29 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_30 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 32768)             8421376   \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_14 (Conv2D  (None, 32, 32, 256)      524544    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_31 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_15 (Conv2D  (None, 64, 64, 512)      2097664   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_32 (LeakyReLU)  (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_16 (Conv2D  (None, 128, 128, 1024)   8389632   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_33 (LeakyReLU)  (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 128, 128, 3)       49155     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,482,371\n",
            "Trainable params: 19,482,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(16 * 16 * 128),\n",
        "        layers.Reshape((16, 16, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "go-V8ASVZSua"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_COVID_CT')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9o0hdV5GZV_k"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/COVID-19 Data Augmented COVID CT' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnpBNr4AZYf_",
        "outputId": "09d00dee-1d70-4e39-fb61-a67408729725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "85/85 [==============================] - 116s 490ms/step - d_loss: 0.5481 - g_loss: 0.6431\n",
            "Epoch 2/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.5500 - g_loss: 0.6027\n",
            "Epoch 3/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.4797 - g_loss: 0.7369\n",
            "Epoch 4/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.4191 - g_loss: 0.7852\n",
            "Epoch 5/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.3950 - g_loss: 0.8115\n",
            "Epoch 6/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.3510 - g_loss: 0.9145\n",
            "Epoch 7/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.3298 - g_loss: 1.0371\n",
            "Epoch 8/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.4548 - g_loss: 1.0025\n",
            "Epoch 9/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6733 - g_loss: 0.7800\n",
            "Epoch 10/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.7224 - g_loss: 0.8164\n",
            "Epoch 11/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6720 - g_loss: 1.1622\n",
            "Epoch 12/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.7412 - g_loss: 1.1553\n",
            "Epoch 13/100\n",
            "85/85 [==============================] - 37s 431ms/step - d_loss: 0.7815 - g_loss: 1.0549\n",
            "Epoch 14/100\n",
            "85/85 [==============================] - 37s 431ms/step - d_loss: 0.7069 - g_loss: 1.1157\n",
            "Epoch 15/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.6342 - g_loss: 1.1730\n",
            "Epoch 16/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6205 - g_loss: 1.1344\n",
            "Epoch 17/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6284 - g_loss: 1.1227\n",
            "Epoch 18/100\n",
            "85/85 [==============================] - 37s 432ms/step - d_loss: 0.6311 - g_loss: 1.0690\n",
            "Epoch 19/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6274 - g_loss: 1.0093\n",
            "Epoch 20/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6392 - g_loss: 0.8675\n",
            "Epoch 21/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6524 - g_loss: 0.7372\n",
            "Epoch 22/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6610 - g_loss: 0.6848\n",
            "Epoch 23/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6640 - g_loss: 0.7260\n",
            "Epoch 24/100\n",
            "85/85 [==============================] - 37s 431ms/step - d_loss: 0.6674 - g_loss: 0.7830\n",
            "Epoch 25/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6707 - g_loss: 0.8151\n",
            "Epoch 26/100\n",
            "85/85 [==============================] - 38s 439ms/step - d_loss: 0.6662 - g_loss: 0.8198\n",
            "Epoch 27/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6688 - g_loss: 0.8137\n",
            "Epoch 28/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.6664 - g_loss: 0.8103\n",
            "Epoch 29/100\n",
            "85/85 [==============================] - 37s 425ms/step - d_loss: 0.6621 - g_loss: 0.8188\n",
            "Epoch 30/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6517 - g_loss: 0.8409\n",
            "Epoch 31/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.6483 - g_loss: 0.8298\n",
            "Epoch 32/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6474 - g_loss: 0.8644\n",
            "Epoch 33/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.6452 - g_loss: 0.8356\n",
            "Epoch 34/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.6527 - g_loss: 0.8087\n",
            "Epoch 35/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.6673 - g_loss: 0.7737\n",
            "Epoch 36/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6763 - g_loss: 0.7729\n",
            "Epoch 37/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6835 - g_loss: 0.7572\n",
            "Epoch 38/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6903 - g_loss: 0.7457\n",
            "Epoch 39/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6794 - g_loss: 0.7791\n",
            "Epoch 40/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.6860 - g_loss: 0.7673\n",
            "Epoch 41/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6838 - g_loss: 0.7752\n",
            "Epoch 42/100\n",
            "85/85 [==============================] - 37s 426ms/step - d_loss: 0.6775 - g_loss: 0.7736\n",
            "Epoch 43/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6760 - g_loss: 0.7731\n",
            "Epoch 44/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6687 - g_loss: 0.7876\n",
            "Epoch 45/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6686 - g_loss: 0.7768\n",
            "Epoch 46/100\n",
            "85/85 [==============================] - 37s 426ms/step - d_loss: 0.6624 - g_loss: 0.7799\n",
            "Epoch 47/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6707 - g_loss: 0.7653\n",
            "Epoch 48/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6615 - g_loss: 0.7899\n",
            "Epoch 49/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6726 - g_loss: 0.7696\n",
            "Epoch 50/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6810 - g_loss: 0.7471\n",
            "Epoch 51/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6758 - g_loss: 0.7646\n",
            "Epoch 52/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6812 - g_loss: 0.7494\n",
            "Epoch 53/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6821 - g_loss: 0.7480\n",
            "Epoch 54/100\n",
            "85/85 [==============================] - 37s 430ms/step - d_loss: 0.6733 - g_loss: 0.7727\n",
            "Epoch 55/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6710 - g_loss: 0.7860\n",
            "Epoch 56/100\n",
            "85/85 [==============================] - 38s 434ms/step - d_loss: 0.6747 - g_loss: 0.7679\n",
            "Epoch 57/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6670 - g_loss: 0.7930\n",
            "Epoch 58/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6596 - g_loss: 0.8038\n",
            "Epoch 59/100\n",
            "85/85 [==============================] - 37s 433ms/step - d_loss: 0.6617 - g_loss: 0.7909\n",
            "Epoch 60/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6681 - g_loss: 0.7917\n",
            "Epoch 61/100\n",
            "85/85 [==============================] - 37s 428ms/step - d_loss: 0.6739 - g_loss: 0.7575\n",
            "Epoch 62/100\n",
            "85/85 [==============================] - 37s 427ms/step - d_loss: 0.6768 - g_loss: 0.7752\n",
            "Epoch 63/100\n",
            "85/85 [==============================] - 37s 429ms/step - d_loss: 0.6736 - g_loss: 0.7797\n",
            "Epoch 64/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6773 - g_loss: 0.7711\n",
            "Epoch 65/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6683 - g_loss: 0.7846\n",
            "Epoch 66/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6706 - g_loss: 0.7850\n",
            "Epoch 67/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6716 - g_loss: 0.7839\n",
            "Epoch 68/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6696 - g_loss: 0.7819\n",
            "Epoch 69/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6726 - g_loss: 0.7777\n",
            "Epoch 70/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6726 - g_loss: 0.7789\n",
            "Epoch 71/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6736 - g_loss: 0.7760\n",
            "Epoch 72/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6708 - g_loss: 0.7844\n",
            "Epoch 73/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6689 - g_loss: 0.7881\n",
            "Epoch 74/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6667 - g_loss: 0.7836\n",
            "Epoch 75/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6665 - g_loss: 0.7895\n",
            "Epoch 76/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6672 - g_loss: 0.7822\n",
            "Epoch 77/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6661 - g_loss: 0.7840\n",
            "Epoch 78/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6711 - g_loss: 0.7719\n",
            "Epoch 79/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6702 - g_loss: 0.7766\n",
            "Epoch 80/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6693 - g_loss: 0.7784\n",
            "Epoch 81/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6663 - g_loss: 0.7830\n",
            "Epoch 82/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6623 - g_loss: 0.7938\n",
            "Epoch 83/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6636 - g_loss: 0.7829\n",
            "Epoch 84/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6671 - g_loss: 0.7840\n",
            "Epoch 85/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6646 - g_loss: 0.7921\n",
            "Epoch 86/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6637 - g_loss: 0.7937\n",
            "Epoch 87/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6688 - g_loss: 0.7745\n",
            "Epoch 88/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6582 - g_loss: 0.8045\n",
            "Epoch 89/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6647 - g_loss: 0.7914\n",
            "Epoch 90/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6714 - g_loss: 0.7724\n",
            "Epoch 91/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6650 - g_loss: 0.7962\n",
            "Epoch 92/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6795 - g_loss: 0.7715\n",
            "Epoch 93/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6702 - g_loss: 0.7832\n",
            "Epoch 94/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6702 - g_loss: 0.7891\n",
            "Epoch 95/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6709 - g_loss: 0.7874\n",
            "Epoch 96/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6680 - g_loss: 0.7915\n",
            "Epoch 97/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6728 - g_loss: 0.7838\n",
            "Epoch 98/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6761 - g_loss: 0.7785\n",
            "Epoch 99/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6763 - g_loss: 0.7707\n",
            "Epoch 100/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6698 - g_loss: 0.7847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "epochs = 100  # In practice, use ~100 epochs \n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/COVID-19_Augmented_COVID_CTModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/COVID-19_Augmented_COVID_CTModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfCFY6DvZpNp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}