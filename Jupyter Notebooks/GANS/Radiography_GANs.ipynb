{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-KAqwHSkFqu",
        "outputId": "673cd2fe-1209-42b7-a290-81e256e7a8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            " COVID\t\t\t      Normal.metadata.xlsx\n",
            " COVID.metadata.xlsx\t      README.md.txt\n",
            " Lung_Opacity.metadata.xlsx  'Viral Pneumonia'\n",
            " Normal\t\t\t     'Viral Pneumonia.metadata.xlsx'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n",
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons, install\n",
            "Successfully installed install-1.3.5 tensorflow-addons-0.19.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls \"/content/gdrive/My Drive/COVID-19_Radiography_Dataset\"\n",
        "!pip install pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u4JUZUnIjArU"
      },
      "outputs": [],
      "source": [
        "import keras \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from enum import Enum\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "# for reproducibility - ref https://machinelearningmastery.com/reproducible-results-neural-networks-keras/ and https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "np.random.seed(9)\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "# loading data from gdrive\n",
        "chest_xray_dataset = os.path.abspath(\"/content/gdrive/My Drive/COVID 19 CHEST XRAY/images\")\n",
        "chest_xray_dataset_annotations = os.path.abspath(\"/content/gdrive/My Drive/COVID 19 CHEST XRAY/metadata.csv\")\n",
        "radiography_dataset = os.path.abspath(\"/content/gdrive/My Drive/COVID-19_Radiography_Dataset/\")\n",
        "xray_covid19_dataset = os.path.abspath(\"/content/gdrive/My Drive/xray_dataset_covid19/\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Gy_GXINDq0-8",
        "outputId": "8da6953a-2a84-4c39-b8a1-ac30b01b1d70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#@markdown #**Anti-Disconnect for Google Colab**\n",
        "#@markdown ## Run this to stop it from disconnecting automatically \n",
        "#@markdown  **(It will anyhow disconnect after 6 - 12 hrs for using the free version of Colab.)**\n",
        "#@markdown  *(Colab Pro users will get about 24 hrs usage time)*\n",
        "#@markdown ---\n",
        "# taken from https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP_%28z%2Bquantize_method_with_augmentations%2C_user_friendly_interface%29.ipynb#scrollTo=XHyPd4oxVp_l stops colab disconnecting\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "IPython.display.Javascript(js_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSpMb9aHjQT4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "2e76ad01-5ebe-4291-fd80-40f9abb04c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7232 files belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-aac433845448>:9: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  ax = plt.subplot(2, 2, i + 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128, 128)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "dataset = keras.utils.image_dataset_from_directory(\n",
        "    radiography_dataset + '/COVID', label_mode=None, image_size=image_size, batch_size=32\n",
        ")\n",
        "for images in dataset.take(4):\n",
        "  for i in range(1):\n",
        "      ax = plt.subplot(2, 2, i + 1)\n",
        "      plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "      plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TndIIeA7R5IK"
      },
      "source": [
        "# Code taken from ref https://keras.io/examples/generative/vae/ and refactored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NKKXZDF3P5o"
      },
      "outputs": [],
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRqfi82oD8q3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aefda5be-c4a0-436f-d941-e863e64e43f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 32)   896         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 16, 16, 64)   18496       ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 8, 8, 128)    73856       ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 4, 4, 256)    295168      ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 2, 2, 512)    1180160     ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 728)    3355352     ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 1024)   6710272     ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 1, 1, 2048)   18876416    ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 1, 1, 4096)   75501568    ['conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 4096)         0           ['conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           65552       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 2)            34          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 2)            34          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 106,077,804\n",
            "Trainable params: 106,077,804\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_dim = 2\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(64, 64, 3))\n",
        "x = layers.Conv2D(32, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(128, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(256, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(512, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(728, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(1024, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(2048, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(4096, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"LeakyReLU\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl-lUO8PnvxI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb662fb2-db56-485c-c337-013e96192080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 2)]               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              3072      \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 4, 4, 64)          0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 8, 8, 256)        147712    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 16, 16, 512)      1180160   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 32, 32, 768)      3539712   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 64, 64, 1024)     7078912   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 64, 64, 1)        9217      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,958,785\n",
            "Trainable params: 11,958,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(4 * 4 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((4, 4, 64))(x)\n",
        "x = layers.Conv2DTranspose(256, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(512, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(768, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(1024, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\")(x)\n",
        "\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crjHnHiplTek"
      },
      "outputs": [],
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.mae(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "    def get_vae():\n",
        "      return VAE(name='VAERADIOGRAPHYCOVID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "15ivoe-vnzsC",
        "outputId": "e03f8bda-164e-4d73-c53c-b61195a34fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/226\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-18ae96ed001e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m226\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/DataAugmentedRadiography_CovidModel/VAE.tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d793ac89fadc>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             reconstruction_loss = tf.reduce_mean(\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-8-d793ac89fadc>\", line 22, in train_step\n        z_mean, z_log_var, z = self.encoder(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"encoder\" is incompatible with the layer: expected shape=(None, 64, 64, 3), found shape=(None, 128, 128, 3)\n"
          ]
        }
      ],
      "source": [
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(1e-3))\n",
        "vae.fit(dataset, epochs=226 , steps_per_epoch=1)\n",
        "model = vae.get_vae();\n",
        "vae.save('/content/gdrive/My Drive/DataAugmentedRadiography_CovidModel/VAE.tf')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "COVID_imgs = vae.inference(1000)\n",
        "\n",
        "for i in range(COVID_imgs):\n",
        "  "
      ],
      "metadata": {
        "id": "pWJ4_a6a5iae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZTn2W0YR9WW"
      },
      "source": [
        "# Code taken from https://keras.io/examples/generative/dcgan_overriding_train_step/ and refactored  Radiography DCGANs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DCGAN COVID"
      ],
      "metadata": {
        "id": "21sz_kgVG234"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LpzVOLGUZHP",
        "outputId": "d54b461a-9176-4086-fb4b-fe6800356bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7232 files belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128, 128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    radiography_dataset + '/COVID/', label_mode=None, image_size=image_size, batch_size=16,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xqBHZFeEvnL1"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpdanrS_Sf3K",
        "outputId": "d9743a10-034a-4b92-b41b-e51ede0ad9d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 4\n"
          ]
        }
      ],
      "source": [
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "print(generator_in_channels, discriminator_in_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi6lG4V7Siri",
        "outputId": "da3b0d5a-f1b8-4149-d4a5-5507a26d4b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 8192)              827392    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 256)      524544    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 1024)     8389632   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 64, 64, 1024)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 128, 128, 64)     1048640   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 3)       3075      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,890,947\n",
            "Trainable params: 12,890,947\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ySIPqDcuSmcD"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_RADIOGRAPHY_COVID')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ELzgXu-lSrZg"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_Radiography_COVID' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8hcEc6lNL_q",
        "outputId": "d640d111-0fdf-4d78-af59-4ad1377e02ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/452\n",
            "1/1 [==============================] - 61s 61s/step - d_loss: 0.7080 - g_loss: 0.6947\n",
            "Epoch 2/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5554 - g_loss: 0.6952\n",
            "Epoch 3/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4893 - g_loss: 0.6932\n",
            "Epoch 4/452\n",
            "1/1 [==============================] - 7s 7s/step - d_loss: 0.4373 - g_loss: 0.6867\n",
            "Epoch 5/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4345 - g_loss: 0.6766\n",
            "Epoch 6/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4456 - g_loss: 0.6634\n",
            "Epoch 7/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4428 - g_loss: 0.6549\n",
            "Epoch 8/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4418 - g_loss: 0.6490\n",
            "Epoch 9/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4436 - g_loss: 0.6385\n",
            "Epoch 10/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4628 - g_loss: 0.6334\n",
            "Epoch 11/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4731 - g_loss: 0.6275\n",
            "Epoch 12/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4751 - g_loss: 0.6265\n",
            "Epoch 13/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4860 - g_loss: 0.6177\n",
            "Epoch 14/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4751 - g_loss: 0.6333\n",
            "Epoch 15/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4930 - g_loss: 0.5981\n",
            "Epoch 16/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5090 - g_loss: 0.6745\n",
            "Epoch 17/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5337 - g_loss: 0.4960\n",
            "Epoch 18/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6096 - g_loss: 0.6389\n",
            "Epoch 19/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5618 - g_loss: 0.5616\n",
            "Epoch 20/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5759 - g_loss: 0.6511\n",
            "Epoch 21/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6030 - g_loss: 0.5292\n",
            "Epoch 22/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6000 - g_loss: 0.6425\n",
            "Epoch 23/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5540 - g_loss: 0.6995\n",
            "Epoch 24/452\n",
            "1/1 [==============================] - 7s 7s/step - d_loss: 0.5757 - g_loss: 0.5453\n",
            "Epoch 25/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5871 - g_loss: 0.8474\n",
            "Epoch 26/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6156 - g_loss: 0.5019\n",
            "Epoch 27/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6309 - g_loss: 0.7834\n",
            "Epoch 28/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5820 - g_loss: 0.6106\n",
            "Epoch 29/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5456 - g_loss: 0.8317\n",
            "Epoch 30/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5246 - g_loss: 0.7403\n",
            "Epoch 31/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5583 - g_loss: 0.6408\n",
            "Epoch 32/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5430 - g_loss: 0.9594\n",
            "Epoch 33/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6661 - g_loss: 0.5111\n",
            "Epoch 34/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5999 - g_loss: 0.8917\n",
            "Epoch 35/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5901 - g_loss: 0.7058\n",
            "Epoch 36/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4882 - g_loss: 0.8728\n",
            "Epoch 37/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5590 - g_loss: 0.6577\n",
            "Epoch 38/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4946 - g_loss: 0.9271\n",
            "Epoch 39/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5112 - g_loss: 0.7283\n",
            "Epoch 40/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4996 - g_loss: 0.7452\n",
            "Epoch 41/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5143 - g_loss: 0.8113\n",
            "Epoch 42/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4592 - g_loss: 0.8488\n",
            "Epoch 43/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4716 - g_loss: 0.6906\n",
            "Epoch 44/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5080 - g_loss: 0.9308\n",
            "Epoch 45/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4778 - g_loss: 0.6289\n",
            "Epoch 46/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5365 - g_loss: 0.9034\n",
            "Epoch 47/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4488 - g_loss: 0.6821\n",
            "Epoch 48/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4851 - g_loss: 1.1103\n",
            "Epoch 49/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6442 - g_loss: 0.5064\n",
            "Epoch 50/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5901 - g_loss: 0.8042\n",
            "Epoch 51/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5133 - g_loss: 0.6916\n",
            "Epoch 52/452\n",
            "1/1 [==============================] - 12s 12s/step - d_loss: 0.4710 - g_loss: 0.9161\n",
            "Epoch 53/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4878 - g_loss: 0.6848\n",
            "Epoch 54/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5378 - g_loss: 0.7324\n",
            "Epoch 55/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5055 - g_loss: 0.7288\n",
            "Epoch 56/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4866 - g_loss: 0.8795\n",
            "Epoch 57/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4064 - g_loss: 1.0699\n",
            "Epoch 58/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5057 - g_loss: 0.5607\n",
            "Epoch 59/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5393 - g_loss: 0.9822\n",
            "Epoch 60/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5629 - g_loss: 0.5415\n",
            "Epoch 61/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5434 - g_loss: 0.9322\n",
            "Epoch 62/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5980 - g_loss: 0.5606\n",
            "Epoch 63/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5675 - g_loss: 0.8167\n",
            "Epoch 64/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6185 - g_loss: 0.5842\n",
            "Epoch 65/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5258 - g_loss: 0.9616\n",
            "Epoch 66/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5202 - g_loss: 0.7006\n",
            "Epoch 67/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4778 - g_loss: 0.9028\n",
            "Epoch 68/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4556 - g_loss: 0.7998\n",
            "Epoch 69/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5070 - g_loss: 0.8243\n",
            "Epoch 70/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5038 - g_loss: 0.7823\n",
            "Epoch 71/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4522 - g_loss: 0.9975\n",
            "Epoch 72/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5707 - g_loss: 0.4624\n",
            "Epoch 73/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6988 - g_loss: 1.2232\n",
            "Epoch 74/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5817 - g_loss: 0.6647\n",
            "Epoch 75/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5633 - g_loss: 0.7616\n",
            "Epoch 76/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5223 - g_loss: 0.6949\n",
            "Epoch 77/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5237 - g_loss: 0.9743\n",
            "Epoch 78/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5971 - g_loss: 0.5018\n",
            "Epoch 79/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6287 - g_loss: 1.0827\n",
            "Epoch 80/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5618 - g_loss: 0.7177\n",
            "Epoch 81/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5434 - g_loss: 0.9202\n",
            "Epoch 82/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5639 - g_loss: 0.6826\n",
            "Epoch 83/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5332 - g_loss: 0.9676\n",
            "Epoch 84/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4814 - g_loss: 0.8791\n",
            "Epoch 85/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5309 - g_loss: 0.6779\n",
            "Epoch 86/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4880 - g_loss: 1.1669\n",
            "Epoch 87/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5548 - g_loss: 0.6187\n",
            "Epoch 88/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5378 - g_loss: 1.2441\n",
            "Epoch 89/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6045 - g_loss: 0.5676\n",
            "Epoch 90/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5856 - g_loss: 1.0661\n",
            "Epoch 91/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5545 - g_loss: 0.6196\n",
            "Epoch 92/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5551 - g_loss: 1.0069\n",
            "Epoch 93/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5158 - g_loss: 0.6696\n",
            "Epoch 94/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5509 - g_loss: 1.1355\n",
            "Epoch 95/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4924 - g_loss: 0.8282\n",
            "Epoch 96/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4972 - g_loss: 1.0395\n",
            "Epoch 97/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5191 - g_loss: 0.6667\n",
            "Epoch 98/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5472 - g_loss: 1.3607\n",
            "Epoch 99/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5193 - g_loss: 0.7491\n",
            "Epoch 100/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4922 - g_loss: 1.1414\n",
            "Epoch 101/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5106 - g_loss: 0.7145\n",
            "Epoch 102/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5947 - g_loss: 1.3261\n",
            "Epoch 103/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4963 - g_loss: 0.7820\n",
            "Epoch 104/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4643 - g_loss: 1.1756\n",
            "Epoch 105/452\n",
            "1/1 [==============================] - 7s 7s/step - d_loss: 0.5072 - g_loss: 0.7332\n",
            "Epoch 106/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4791 - g_loss: 1.4226\n",
            "Epoch 107/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5635 - g_loss: 0.7849\n",
            "Epoch 108/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5172 - g_loss: 1.3133\n",
            "Epoch 109/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4809 - g_loss: 0.8102\n",
            "Epoch 110/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4151 - g_loss: 1.2746\n",
            "Epoch 111/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4677 - g_loss: 0.8176\n",
            "Epoch 112/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4509 - g_loss: 1.2520\n",
            "Epoch 113/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4761 - g_loss: 0.7452\n",
            "Epoch 114/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4924 - g_loss: 1.6357\n",
            "Epoch 115/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4846 - g_loss: 0.7730\n",
            "Epoch 116/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4462 - g_loss: 1.4343\n",
            "Epoch 117/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4079 - g_loss: 0.9027\n",
            "Epoch 118/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4646 - g_loss: 1.4720\n",
            "Epoch 119/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4604 - g_loss: 0.7668\n",
            "Epoch 120/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4806 - g_loss: 1.8177\n",
            "Epoch 121/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4819 - g_loss: 0.7302\n",
            "Epoch 122/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5461 - g_loss: 1.6623\n",
            "Epoch 123/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5585 - g_loss: 0.8507\n",
            "Epoch 124/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5008 - g_loss: 1.2253\n",
            "Epoch 125/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.3947 - g_loss: 0.9764\n",
            "Epoch 126/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4160 - g_loss: 1.5065\n",
            "Epoch 127/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4243 - g_loss: 0.8421\n",
            "Epoch 128/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4622 - g_loss: 1.7172\n",
            "Epoch 129/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4752 - g_loss: 0.8539\n",
            "Epoch 130/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5219 - g_loss: 1.3791\n",
            "Epoch 131/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4314 - g_loss: 1.1615\n",
            "Epoch 132/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5242 - g_loss: 1.1639\n",
            "Epoch 133/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4607 - g_loss: 0.8011\n",
            "Epoch 134/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4742 - g_loss: 1.8111\n",
            "Epoch 135/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5459 - g_loss: 0.6427\n",
            "Epoch 136/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5574 - g_loss: 1.6868\n",
            "Epoch 137/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6526 - g_loss: 0.6245\n",
            "Epoch 138/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5837 - g_loss: 1.2727\n",
            "Epoch 139/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6310 - g_loss: 0.7474\n",
            "Epoch 140/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5817 - g_loss: 1.2195\n",
            "Epoch 141/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.4662 - g_loss: 0.9305\n",
            "Epoch 142/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4794 - g_loss: 1.2519\n",
            "Epoch 143/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.4939 - g_loss: 0.9047\n",
            "Epoch 144/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5385 - g_loss: 1.0838\n",
            "Epoch 145/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5004 - g_loss: 1.2096\n",
            "Epoch 146/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5138 - g_loss: 0.6169\n",
            "Epoch 147/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6231 - g_loss: 1.6300\n",
            "Epoch 148/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5626 - g_loss: 0.8257\n",
            "Epoch 149/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6466 - g_loss: 0.7382\n",
            "Epoch 150/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5197 - g_loss: 1.2588\n",
            "Epoch 151/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6735 - g_loss: 0.4364\n",
            "Epoch 152/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6991 - g_loss: 1.3797\n",
            "Epoch 153/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7178 - g_loss: 0.6316\n",
            "Epoch 154/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5771 - g_loss: 1.0557\n",
            "Epoch 155/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6912 - g_loss: 0.6153\n",
            "Epoch 156/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6305 - g_loss: 1.1491\n",
            "Epoch 157/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6718 - g_loss: 0.6125\n",
            "Epoch 158/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6960 - g_loss: 0.7900\n",
            "Epoch 159/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6369 - g_loss: 0.6184\n",
            "Epoch 160/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6803 - g_loss: 1.1198\n",
            "Epoch 161/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6464 - g_loss: 0.6224\n",
            "Epoch 162/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7120 - g_loss: 0.9328\n",
            "Epoch 163/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7196 - g_loss: 0.4811\n",
            "Epoch 164/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7775 - g_loss: 1.3009\n",
            "Epoch 165/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.8749 - g_loss: 0.5453\n",
            "Epoch 166/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6847 - g_loss: 0.8698\n",
            "Epoch 167/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7248 - g_loss: 0.6033\n",
            "Epoch 168/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7228 - g_loss: 0.9278\n",
            "Epoch 169/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6507 - g_loss: 0.5803\n",
            "Epoch 170/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7114 - g_loss: 0.7420\n",
            "Epoch 171/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6958 - g_loss: 0.6389\n",
            "Epoch 172/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6824 - g_loss: 1.0024\n",
            "Epoch 173/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7251 - g_loss: 0.5749\n",
            "Epoch 174/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7800 - g_loss: 0.9348\n",
            "Epoch 175/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6701 - g_loss: 0.8684\n",
            "Epoch 176/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6674 - g_loss: 0.6510\n",
            "Epoch 177/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7125 - g_loss: 0.9311\n",
            "Epoch 178/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.8127 - g_loss: 0.4816\n",
            "Epoch 179/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.8748 - g_loss: 1.0848\n",
            "Epoch 180/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7347 - g_loss: 0.6314\n",
            "Epoch 181/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6629 - g_loss: 0.7247\n",
            "Epoch 182/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7073 - g_loss: 0.8481\n",
            "Epoch 183/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6860 - g_loss: 0.5989\n",
            "Epoch 184/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7884 - g_loss: 1.3934\n",
            "Epoch 185/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.8272 - g_loss: 0.8250\n",
            "Epoch 186/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6702 - g_loss: 0.7395\n",
            "Epoch 187/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6378 - g_loss: 0.6422\n",
            "Epoch 188/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6514 - g_loss: 1.0219\n",
            "Epoch 189/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6291 - g_loss: 0.8546\n",
            "Epoch 190/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6612 - g_loss: 0.6298\n",
            "Epoch 191/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7426 - g_loss: 1.4887\n",
            "Epoch 192/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6939 - g_loss: 0.7051\n",
            "Epoch 193/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6756 - g_loss: 1.1782\n",
            "Epoch 194/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7107 - g_loss: 0.8019\n",
            "Epoch 195/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6474 - g_loss: 1.2318\n",
            "Epoch 196/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6208 - g_loss: 0.8567\n",
            "Epoch 197/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7851 - g_loss: 2.2648\n",
            "Epoch 198/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7709 - g_loss: 0.9788\n",
            "Epoch 199/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6886 - g_loss: 1.1268\n",
            "Epoch 200/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7697 - g_loss: 1.2439\n",
            "Epoch 201/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7780 - g_loss: 1.3121\n",
            "Epoch 202/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6548 - g_loss: 1.2231\n",
            "Epoch 203/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7276 - g_loss: 0.8714\n",
            "Epoch 204/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6479 - g_loss: 1.0652\n",
            "Epoch 205/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6174 - g_loss: 1.0330\n",
            "Epoch 206/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6710 - g_loss: 1.3831\n",
            "Epoch 207/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7533 - g_loss: 0.8560\n",
            "Epoch 208/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6650 - g_loss: 1.0509\n",
            "Epoch 209/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6051 - g_loss: 1.3111\n",
            "Epoch 210/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6548 - g_loss: 0.8665\n",
            "Epoch 211/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6708 - g_loss: 1.5761\n",
            "Epoch 212/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6554 - g_loss: 0.9725\n",
            "Epoch 213/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6851 - g_loss: 1.0502\n",
            "Epoch 214/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6672 - g_loss: 0.9840\n",
            "Epoch 215/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6416 - g_loss: 0.7954\n",
            "Epoch 216/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6996 - g_loss: 1.3441\n",
            "Epoch 217/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6839 - g_loss: 0.8473\n",
            "Epoch 218/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6484 - g_loss: 0.7690\n",
            "Epoch 219/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6212 - g_loss: 0.9927\n",
            "Epoch 220/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5987 - g_loss: 0.7190\n",
            "Epoch 221/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7342 - g_loss: 1.3926\n",
            "Epoch 222/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6633 - g_loss: 1.1230\n",
            "Epoch 223/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6467 - g_loss: 1.1141\n",
            "Epoch 224/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7257 - g_loss: 1.1926\n",
            "Epoch 225/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6356 - g_loss: 0.9126\n",
            "Epoch 226/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6523 - g_loss: 1.0854\n",
            "Epoch 227/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6339 - g_loss: 0.9386\n",
            "Epoch 228/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7281 - g_loss: 1.0324\n",
            "Epoch 229/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6522 - g_loss: 0.9517\n",
            "Epoch 230/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6398 - g_loss: 0.8810\n",
            "Epoch 231/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6783 - g_loss: 0.7729\n",
            "Epoch 232/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6772 - g_loss: 0.8203\n",
            "Epoch 233/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6052 - g_loss: 1.0262\n",
            "Epoch 234/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6662 - g_loss: 0.8368\n",
            "Epoch 235/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7030 - g_loss: 0.9357\n",
            "Epoch 236/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6703 - g_loss: 0.8855\n",
            "Epoch 237/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6521 - g_loss: 0.7975\n",
            "Epoch 238/452\n",
            "1/1 [==============================] - 8s 8s/step - d_loss: 0.6583 - g_loss: 0.9306\n",
            "Epoch 239/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6226 - g_loss: 0.9048\n",
            "Epoch 240/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6333 - g_loss: 0.8405\n",
            "Epoch 241/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6052 - g_loss: 0.7702\n",
            "Epoch 242/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6395 - g_loss: 0.7159\n",
            "Epoch 243/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6726 - g_loss: 1.1824\n",
            "Epoch 244/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7442 - g_loss: 0.7085\n",
            "Epoch 245/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6852 - g_loss: 1.0035\n",
            "Epoch 246/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6275 - g_loss: 0.8782\n",
            "Epoch 247/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6628 - g_loss: 0.8434\n",
            "Epoch 248/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6407 - g_loss: 0.8271\n",
            "Epoch 249/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6881 - g_loss: 0.8844\n",
            "Epoch 250/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6401 - g_loss: 0.8729\n",
            "Epoch 251/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6297 - g_loss: 0.9285\n",
            "Epoch 252/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6240 - g_loss: 0.9542\n",
            "Epoch 253/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6170 - g_loss: 0.8271\n",
            "Epoch 254/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6269 - g_loss: 0.8364\n",
            "Epoch 255/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6825 - g_loss: 0.8732\n",
            "Epoch 256/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6708 - g_loss: 1.0227\n",
            "Epoch 257/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6706 - g_loss: 0.6519\n",
            "Epoch 258/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6767 - g_loss: 0.8574\n",
            "Epoch 259/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6821 - g_loss: 0.8529\n",
            "Epoch 260/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6824 - g_loss: 0.7206\n",
            "Epoch 261/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6931 - g_loss: 0.9918\n",
            "Epoch 262/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6388 - g_loss: 0.7856\n",
            "Epoch 263/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6563 - g_loss: 0.8241\n",
            "Epoch 264/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7305 - g_loss: 0.9129\n",
            "Epoch 265/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7137 - g_loss: 0.7707\n",
            "Epoch 266/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6716 - g_loss: 0.7992\n",
            "Epoch 267/452\n",
            "1/1 [==============================] - 10s 10s/step - d_loss: 0.6985 - g_loss: 0.7553\n",
            "Epoch 268/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6715 - g_loss: 0.8693\n",
            "Epoch 269/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6854 - g_loss: 0.7044\n",
            "Epoch 270/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6880 - g_loss: 0.9144\n",
            "Epoch 271/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6419 - g_loss: 0.6925\n",
            "Epoch 272/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6779 - g_loss: 0.8123\n",
            "Epoch 273/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6399 - g_loss: 0.8748\n",
            "Epoch 274/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6547 - g_loss: 0.7552\n",
            "Epoch 275/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6559 - g_loss: 0.7708\n",
            "Epoch 276/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6704 - g_loss: 0.7888\n",
            "Epoch 277/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6739 - g_loss: 0.7187\n",
            "Epoch 278/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6795 - g_loss: 0.8812\n",
            "Epoch 279/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6741 - g_loss: 0.7769\n",
            "Epoch 280/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6860 - g_loss: 0.8738\n",
            "Epoch 281/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6478 - g_loss: 0.8393\n",
            "Epoch 282/452\n",
            "1/1 [==============================] - 32s 32s/step - d_loss: 0.6508 - g_loss: 0.8668\n",
            "Epoch 283/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6700 - g_loss: 0.8078\n",
            "Epoch 284/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6797 - g_loss: 0.6467\n",
            "Epoch 285/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6844 - g_loss: 1.0753\n",
            "Epoch 286/452\n",
            "1/1 [==============================] - 7s 7s/step - d_loss: 0.6723 - g_loss: 0.6905\n",
            "Epoch 287/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6191 - g_loss: 0.6831\n",
            "Epoch 288/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6332 - g_loss: 0.8711\n",
            "Epoch 289/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7007 - g_loss: 0.6397\n",
            "Epoch 290/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6580 - g_loss: 0.7095\n",
            "Epoch 291/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6321 - g_loss: 0.7787\n",
            "Epoch 292/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6513 - g_loss: 0.7990\n",
            "Epoch 293/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6547 - g_loss: 0.8257\n",
            "Epoch 294/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6123 - g_loss: 0.7596\n",
            "Epoch 295/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6567 - g_loss: 1.0378\n",
            "Epoch 296/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7062 - g_loss: 0.5638\n",
            "Epoch 297/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7312 - g_loss: 0.8720\n",
            "Epoch 298/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6449 - g_loss: 0.7202\n",
            "Epoch 299/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6293 - g_loss: 0.8143\n",
            "Epoch 300/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6045 - g_loss: 0.8766\n",
            "Epoch 301/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6805 - g_loss: 0.7433\n",
            "Epoch 302/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6531 - g_loss: 0.7425\n",
            "Epoch 303/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6702 - g_loss: 0.7300\n",
            "Epoch 304/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6571 - g_loss: 0.8369\n",
            "Epoch 305/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6204 - g_loss: 0.7109\n",
            "Epoch 306/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6545 - g_loss: 0.6787\n",
            "Epoch 307/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6688 - g_loss: 0.9723\n",
            "Epoch 308/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6854 - g_loss: 0.6901\n",
            "Epoch 309/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6828 - g_loss: 0.8783\n",
            "Epoch 310/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6477 - g_loss: 0.8711\n",
            "Epoch 311/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6325 - g_loss: 1.0076\n",
            "Epoch 312/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6524 - g_loss: 0.7951\n",
            "Epoch 313/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7012 - g_loss: 0.6297\n",
            "Epoch 314/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6593 - g_loss: 0.8763\n",
            "Epoch 315/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6529 - g_loss: 0.9663\n",
            "Epoch 316/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6946 - g_loss: 0.7166\n",
            "Epoch 317/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6699 - g_loss: 0.8964\n",
            "Epoch 318/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6680 - g_loss: 0.7868\n",
            "Epoch 319/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6263 - g_loss: 0.8610\n",
            "Epoch 320/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6421 - g_loss: 0.8017\n",
            "Epoch 321/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6364 - g_loss: 0.8946\n",
            "Epoch 322/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6700 - g_loss: 0.9273\n",
            "Epoch 323/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6945 - g_loss: 0.8218\n",
            "Epoch 324/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6852 - g_loss: 0.6353\n",
            "Epoch 325/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6539 - g_loss: 0.9087\n",
            "Epoch 326/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6440 - g_loss: 0.8092\n",
            "Epoch 327/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6509 - g_loss: 0.9282\n",
            "Epoch 328/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6577 - g_loss: 0.8166\n",
            "Epoch 329/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6235 - g_loss: 0.7426\n",
            "Epoch 330/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5819 - g_loss: 0.9035\n",
            "Epoch 331/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6849 - g_loss: 0.7049\n",
            "Epoch 332/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6423 - g_loss: 0.6805\n",
            "Epoch 333/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6492 - g_loss: 1.0151\n",
            "Epoch 334/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7503 - g_loss: 0.6208\n",
            "Epoch 335/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6219 - g_loss: 0.8307\n",
            "Epoch 336/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6526 - g_loss: 0.7866\n",
            "Epoch 337/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6310 - g_loss: 0.9250\n",
            "Epoch 338/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6110 - g_loss: 0.8447\n",
            "Epoch 339/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6494 - g_loss: 0.5656\n",
            "Epoch 340/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6384 - g_loss: 0.7846\n",
            "Epoch 341/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7572 - g_loss: 0.7632\n",
            "Epoch 342/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6186 - g_loss: 0.9102\n",
            "Epoch 343/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6669 - g_loss: 0.8090\n",
            "Epoch 344/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6484 - g_loss: 0.8649\n",
            "Epoch 345/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6190 - g_loss: 0.8336\n",
            "Epoch 346/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6233 - g_loss: 0.7907\n",
            "Epoch 347/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5688 - g_loss: 0.9627\n",
            "Epoch 348/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7090 - g_loss: 0.7021\n",
            "Epoch 349/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6878 - g_loss: 1.0686\n",
            "Epoch 350/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6781 - g_loss: 0.7459\n",
            "Epoch 351/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6372 - g_loss: 0.7989\n",
            "Epoch 352/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6762 - g_loss: 0.9679\n",
            "Epoch 353/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6380 - g_loss: 0.7421\n",
            "Epoch 354/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7044 - g_loss: 1.1391\n",
            "Epoch 355/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6838 - g_loss: 0.6580\n",
            "Epoch 356/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7128 - g_loss: 0.8233\n",
            "Epoch 357/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6505 - g_loss: 0.7699\n",
            "Epoch 358/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6379 - g_loss: 0.9605\n",
            "Epoch 359/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6331 - g_loss: 0.8996\n",
            "Epoch 360/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6499 - g_loss: 0.7719\n",
            "Epoch 361/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6642 - g_loss: 0.8914\n",
            "Epoch 362/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6451 - g_loss: 0.6496\n",
            "Epoch 363/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6989 - g_loss: 1.0676\n",
            "Epoch 364/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6554 - g_loss: 0.7740\n",
            "Epoch 365/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6744 - g_loss: 1.0164\n",
            "Epoch 366/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6522 - g_loss: 0.8317\n",
            "Epoch 367/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6538 - g_loss: 0.9936\n",
            "Epoch 368/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6380 - g_loss: 0.6991\n",
            "Epoch 369/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6551 - g_loss: 0.8170\n",
            "Epoch 370/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6183 - g_loss: 0.9973\n",
            "Epoch 371/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6693 - g_loss: 0.8194\n",
            "Epoch 372/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6803 - g_loss: 1.0333\n",
            "Epoch 373/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6308 - g_loss: 0.7424\n",
            "Epoch 374/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6548 - g_loss: 1.0890\n",
            "Epoch 375/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6332 - g_loss: 0.7624\n",
            "Epoch 376/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6179 - g_loss: 0.7465\n",
            "Epoch 377/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6182 - g_loss: 0.7577\n",
            "Epoch 378/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6503 - g_loss: 0.6740\n",
            "Epoch 379/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6091 - g_loss: 1.0178\n",
            "Epoch 380/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6246 - g_loss: 0.6980\n",
            "Epoch 381/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6966 - g_loss: 1.2074\n",
            "Epoch 382/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7364 - g_loss: 0.5614\n",
            "Epoch 383/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6682 - g_loss: 0.9508\n",
            "Epoch 384/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6093 - g_loss: 0.8324\n",
            "Epoch 385/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6492 - g_loss: 0.8068\n",
            "Epoch 386/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6561 - g_loss: 0.8713\n",
            "Epoch 387/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6407 - g_loss: 1.0041\n",
            "Epoch 388/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6277 - g_loss: 1.1832\n",
            "Epoch 389/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6433 - g_loss: 0.9250\n",
            "Epoch 390/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6679 - g_loss: 0.9361\n",
            "Epoch 391/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6221 - g_loss: 0.7709\n",
            "Epoch 392/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6485 - g_loss: 1.0281\n",
            "Epoch 393/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5817 - g_loss: 1.1679\n",
            "Epoch 394/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.7355 - g_loss: 0.6913\n",
            "Epoch 395/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6411 - g_loss: 0.7717\n",
            "Epoch 396/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6325 - g_loss: 0.9271\n",
            "Epoch 397/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6165 - g_loss: 0.7905\n",
            "Epoch 398/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6422 - g_loss: 0.9160\n",
            "Epoch 399/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6952 - g_loss: 0.7262\n",
            "Epoch 400/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6207 - g_loss: 0.9491\n",
            "Epoch 401/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6101 - g_loss: 1.0397\n",
            "Epoch 402/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6069 - g_loss: 0.6859\n",
            "Epoch 403/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6045 - g_loss: 1.0656\n",
            "Epoch 404/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6216 - g_loss: 0.8170\n",
            "Epoch 405/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6488 - g_loss: 0.9167\n",
            "Epoch 406/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6118 - g_loss: 0.8564\n",
            "Epoch 407/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6541 - g_loss: 0.5945\n",
            "Epoch 408/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7081 - g_loss: 1.2106\n",
            "Epoch 409/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6572 - g_loss: 0.7785\n",
            "Epoch 410/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5750 - g_loss: 0.7628\n",
            "Epoch 411/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5815 - g_loss: 0.9162\n",
            "Epoch 412/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.7219 - g_loss: 1.3282\n",
            "Epoch 413/452\n",
            "1/1 [==============================] - 40s 40s/step - d_loss: 0.6671 - g_loss: 0.7367\n",
            "Epoch 414/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6460 - g_loss: 0.9717\n",
            "Epoch 415/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6330 - g_loss: 0.7431\n",
            "Epoch 416/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6375 - g_loss: 0.8089\n",
            "Epoch 417/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6719 - g_loss: 0.8722\n",
            "Epoch 418/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6100 - g_loss: 0.7443\n",
            "Epoch 419/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6405 - g_loss: 1.0127\n",
            "Epoch 420/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6235 - g_loss: 0.7132\n",
            "Epoch 421/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6074 - g_loss: 0.7762\n",
            "Epoch 422/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6148 - g_loss: 0.8183\n",
            "Epoch 423/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6121 - g_loss: 0.8975\n",
            "Epoch 424/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6405 - g_loss: 0.7994\n",
            "Epoch 425/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6423 - g_loss: 0.9225\n",
            "Epoch 426/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6517 - g_loss: 0.9597\n",
            "Epoch 427/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6235 - g_loss: 0.7961\n",
            "Epoch 428/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6252 - g_loss: 1.3311\n",
            "Epoch 429/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6303 - g_loss: 0.7592\n",
            "Epoch 430/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6495 - g_loss: 0.9947\n",
            "Epoch 431/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6231 - g_loss: 0.5846\n",
            "Epoch 432/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6983 - g_loss: 1.2598\n",
            "Epoch 433/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6652 - g_loss: 0.7162\n",
            "Epoch 434/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6269 - g_loss: 1.0394\n",
            "Epoch 435/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6283 - g_loss: 0.8606\n",
            "Epoch 436/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6436 - g_loss: 0.7043\n",
            "Epoch 437/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6618 - g_loss: 0.9010\n",
            "Epoch 438/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.5888 - g_loss: 0.9586\n",
            "Epoch 439/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6133 - g_loss: 0.9909\n",
            "Epoch 440/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6216 - g_loss: 1.0364\n",
            "Epoch 441/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6330 - g_loss: 0.6789\n",
            "Epoch 442/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6673 - g_loss: 1.2960\n",
            "Epoch 443/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6417 - g_loss: 0.8021\n",
            "Epoch 444/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6406 - g_loss: 0.9820\n",
            "Epoch 445/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6230 - g_loss: 0.6963\n",
            "Epoch 446/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6552 - g_loss: 1.0442\n",
            "Epoch 447/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6093 - g_loss: 0.7862\n",
            "Epoch 448/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5981 - g_loss: 0.9818\n",
            "Epoch 449/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6811 - g_loss: 1.0500\n",
            "Epoch 450/452\n",
            "1/1 [==============================] - 6s 6s/step - d_loss: 0.6171 - g_loss: 1.1007\n",
            "Epoch 451/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.5676 - g_loss: 1.1652\n",
            "Epoch 452/452\n",
            "1/1 [==============================] - 5s 5s/step - d_loss: 0.6527 - g_loss: 0.7740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "epochs = 452  # In practice, use ~100 epochs 452\n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0.001),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0.001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs,steps_per_epoch=1, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_Radiography_COVIDModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_Radiography_COVIDModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmVY9ChyN7K9"
      },
      "source": [
        "# Creating DCGAN to only generate masks - test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1HA-eA19N6DS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d35159-22f3-4e8a-831f-5c8a897c1e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7232 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128, 128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    radiography_dataset + '/COVID/', label_mode=None, image_size=image_size, batch_size=16,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fKa6UiK3OGzZ"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MHZmffsBOHeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff5d85e-0893-47ed-eebc-b8708c4c5dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 8192)              827392    \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 16, 16, 256)      524544    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_5 (Conv2DT  (None, 32, 32, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " re_lu_1 (ReLU)              (None, 32, 32, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_6 (Conv2DT  (None, 64, 64, 1024)     8389632   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 64, 64, 1024)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_7 (Conv2DT  (None, 128, 128, 2048)   33556480  \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 128, 128, 2048)    0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 128, 128, 3)       153603    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 45,549,315\n",
            "Trainable params: 45,549,315\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2DTranspose(2048, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJgeeK-GTaM4"
      },
      "outputs": [],
      "source": [
        "epochs = 226  # In practice, use ~100 epochs 452\n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0.001),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0.001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs,steps_per_epoch=1, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_Radiography_COVIDModel/MaskGenerator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_Radiography_COVIDModel/MaskDiscriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pneumonia DCGAN\n"
      ],
      "metadata": {
        "id": "QZ4c0-LGGww3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b-ZcgZzw-cyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "546aeb8d-8ad9-4ed7-a4f7-cd0c4ddfce20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2690 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128, 128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    radiography_dataset + '/Viral Pneumonia/', label_mode=None, image_size=image_size, batch_size=16,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WAuQy-L3o_Ch"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OrJRP5N-9g7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e72fb7b-9245-4969-c0c3-31689dd9fca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_36 (Conv2D)          (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu_40 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_37 (Conv2D)          (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_41 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_38 (Conv2D)          (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_42 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_19 (Dense)            (None, 2048)              264192    \n",
            "                                                                 \n",
            " reshape_9 (Reshape)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_44 (Conv2D  (None, 8, 8, 128)        262272    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_43 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_45 (Conv2D  (None, 16, 16, 256)      524544    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_44 (LeakyReLU)  (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_46 (Conv2D  (None, 32, 32, 512)      2097664   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_45 (LeakyReLU)  (None, 32, 32, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_47 (Conv2D  (None, 64, 64, 1024)     8389632   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_46 (LeakyReLU)  (None, 64, 64, 1024)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_48 (Conv2D  (None, 128, 128, 64)     1048640   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_47 (LeakyReLU)  (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " conv2d_39 (Conv2D)          (None, 128, 128, 3)       1731      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,588,675\n",
            "Trainable params: 12,588,675\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(4 * 4 * 128),\n",
        "        layers.Reshape((4, 4, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "             layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=3, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YDd9AMmj9iPD"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_RADIOGRAPHY_Pneumonia')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rnY1UG_r199G"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=100):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_Radiography_PNEUMONIA' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "u04njmlW2CUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9803669-b59c-45f3-87d2-2b6836c38725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/168\n",
            "1/1 [==============================] - 4s 4s/step - d_loss: 0.7025 - g_loss: 0.6638\n",
            "Epoch 2/168\n",
            "1/1 [==============================] - 1s 758ms/step - d_loss: 0.7731 - g_loss: 0.9229\n",
            "Epoch 3/168\n",
            "1/1 [==============================] - 1s 764ms/step - d_loss: 4.4265 - g_loss: 0.6864\n",
            "Epoch 4/168\n",
            "1/1 [==============================] - 1s 729ms/step - d_loss: 0.5929 - g_loss: 2.3049\n",
            "Epoch 5/168\n",
            "1/1 [==============================] - 1s 776ms/step - d_loss: 10.2823 - g_loss: 1.7706\n",
            "Epoch 6/168\n",
            "1/1 [==============================] - 1s 737ms/step - d_loss: 0.3491 - g_loss: 2.0849\n",
            "Epoch 7/168\n",
            "1/1 [==============================] - 1s 768ms/step - d_loss: 2.5907 - g_loss: 4.9507\n",
            "Epoch 8/168\n",
            "1/1 [==============================] - 1s 768ms/step - d_loss: 0.3700 - g_loss: 1.5296\n",
            "Epoch 9/168\n",
            "1/1 [==============================] - 1s 750ms/step - d_loss: 9.0148 - g_loss: 4.5993e-06\n",
            "Epoch 10/168\n",
            "1/1 [==============================] - 1s 775ms/step - d_loss: 6.9819 - g_loss: 4.2783\n",
            "Epoch 11/168\n",
            "1/1 [==============================] - 1s 763ms/step - d_loss: 0.7945 - g_loss: 0.8160\n",
            "Epoch 12/168\n",
            "1/1 [==============================] - 1s 756ms/step - d_loss: 0.6851 - g_loss: 3.0502\n",
            "Epoch 13/168\n",
            "1/1 [==============================] - 1s 785ms/step - d_loss: 0.4170 - g_loss: 1.2038\n",
            "Epoch 14/168\n",
            "1/1 [==============================] - 1s 780ms/step - d_loss: 0.5749 - g_loss: 0.8804\n",
            "Epoch 15/168\n",
            "1/1 [==============================] - 1s 783ms/step - d_loss: 0.8351 - g_loss: 1.3697\n",
            "Epoch 16/168\n",
            "1/1 [==============================] - 1s 772ms/step - d_loss: 0.6651 - g_loss: 1.4044\n",
            "Epoch 17/168\n",
            "1/1 [==============================] - 1s 783ms/step - d_loss: 0.7010 - g_loss: 1.2669\n",
            "Epoch 18/168\n",
            "1/1 [==============================] - 1s 756ms/step - d_loss: 0.6588 - g_loss: 0.7014\n",
            "Epoch 19/168\n",
            "1/1 [==============================] - 1s 768ms/step - d_loss: 0.9336 - g_loss: 4.6832\n",
            "Epoch 20/168\n",
            "1/1 [==============================] - 1s 772ms/step - d_loss: 1.2292 - g_loss: 0.8320\n",
            "Epoch 21/168\n",
            "1/1 [==============================] - 1s 761ms/step - d_loss: 0.5775 - g_loss: 0.8855\n",
            "Epoch 22/168\n",
            "1/1 [==============================] - 1s 785ms/step - d_loss: 0.5050 - g_loss: 0.8025\n",
            "Epoch 23/168\n",
            "1/1 [==============================] - 1s 763ms/step - d_loss: 0.5625 - g_loss: 0.6956\n",
            "Epoch 24/168\n",
            "1/1 [==============================] - 1s 766ms/step - d_loss: 0.6461 - g_loss: 0.6845\n",
            "Epoch 25/168\n",
            "1/1 [==============================] - 1s 785ms/step - d_loss: 0.6733 - g_loss: 0.9846\n",
            "Epoch 26/168\n",
            "1/1 [==============================] - 1s 779ms/step - d_loss: 0.6051 - g_loss: 0.9174\n",
            "Epoch 27/168\n",
            "1/1 [==============================] - 1s 747ms/step - d_loss: 0.5753 - g_loss: 0.8583\n",
            "Epoch 28/168\n",
            "1/1 [==============================] - 1s 779ms/step - d_loss: 0.5681 - g_loss: 0.9025\n",
            "Epoch 29/168\n",
            "1/1 [==============================] - 1s 748ms/step - d_loss: 0.5235 - g_loss: 1.0767\n",
            "Epoch 30/168\n",
            "1/1 [==============================] - 1s 750ms/step - d_loss: 0.5370 - g_loss: 0.7299\n",
            "Epoch 31/168\n",
            "1/1 [==============================] - 1s 768ms/step - d_loss: 0.6214 - g_loss: 1.6903\n",
            "Epoch 32/168\n",
            "1/1 [==============================] - 1s 740ms/step - d_loss: 0.6483 - g_loss: 0.8859\n",
            "Epoch 33/168\n",
            "1/1 [==============================] - 1s 747ms/step - d_loss: 0.6133 - g_loss: 0.7879\n",
            "Epoch 34/168\n",
            "1/1 [==============================] - 1s 764ms/step - d_loss: 0.6082 - g_loss: 0.8760\n",
            "Epoch 35/168\n",
            "1/1 [==============================] - 1s 732ms/step - d_loss: 0.5544 - g_loss: 1.1965\n",
            "Epoch 36/168\n",
            "1/1 [==============================] - 1s 772ms/step - d_loss: 0.6120 - g_loss: 0.9597\n",
            "Epoch 37/168\n",
            "1/1 [==============================] - 1s 756ms/step - d_loss: 0.6684 - g_loss: 1.8969\n",
            "Epoch 38/168\n",
            "1/1 [==============================] - 1s 757ms/step - d_loss: 0.5766 - g_loss: 0.8713\n",
            "Epoch 39/168\n",
            "1/1 [==============================] - 1s 735ms/step - d_loss: 0.5187 - g_loss: 0.9512\n",
            "Epoch 40/168\n",
            "1/1 [==============================] - 1s 778ms/step - d_loss: 0.5210 - g_loss: 1.1115\n",
            "Epoch 41/168\n",
            "1/1 [==============================] - 1s 761ms/step - d_loss: 0.4644 - g_loss: 1.2590\n",
            "Epoch 42/168\n",
            "1/1 [==============================] - 1s 730ms/step - d_loss: 0.4562 - g_loss: 0.8949\n",
            "Epoch 43/168\n",
            "1/1 [==============================] - 1s 766ms/step - d_loss: 0.5334 - g_loss: 4.7366\n",
            "Epoch 44/168\n",
            "1/1 [==============================] - 1s 760ms/step - d_loss: 1.4787 - g_loss: 0.7167\n",
            "Epoch 45/168\n",
            "1/1 [==============================] - 1s 727ms/step - d_loss: 0.5704 - g_loss: 1.7451\n",
            "Epoch 46/168\n",
            "1/1 [==============================] - 1s 748ms/step - d_loss: 0.2038 - g_loss: 2.1178\n",
            "Epoch 47/168\n",
            "1/1 [==============================] - 1s 742ms/step - d_loss: 0.4979 - g_loss: 3.0794\n",
            "Epoch 48/168\n",
            "1/1 [==============================] - 1s 724ms/step - d_loss: 0.7484 - g_loss: 0.7688\n",
            "Epoch 49/168\n",
            "1/1 [==============================] - 1s 748ms/step - d_loss: 0.5568 - g_loss: 1.1267\n",
            "Epoch 50/168\n",
            "1/1 [==============================] - 1s 741ms/step - d_loss: 0.4792 - g_loss: 1.1115\n",
            "Epoch 51/168\n",
            "1/1 [==============================] - 1s 723ms/step - d_loss: 0.5999 - g_loss: 1.2153\n",
            "Epoch 52/168\n",
            "1/1 [==============================] - 1s 752ms/step - d_loss: 0.4429 - g_loss: 1.0000\n",
            "Epoch 53/168\n",
            "1/1 [==============================] - 1s 743ms/step - d_loss: 0.6111 - g_loss: 1.6111\n",
            "Epoch 54/168\n",
            "1/1 [==============================] - 1s 724ms/step - d_loss: 0.5247 - g_loss: 0.7623\n",
            "Epoch 55/168\n",
            "1/1 [==============================] - 1s 744ms/step - d_loss: 0.5359 - g_loss: 1.2352\n",
            "Epoch 56/168\n",
            "1/1 [==============================] - 1s 742ms/step - d_loss: 0.6534 - g_loss: 1.3339\n",
            "Epoch 57/168\n",
            "1/1 [==============================] - 1s 726ms/step - d_loss: 0.6290 - g_loss: 0.9077\n",
            "Epoch 58/168\n",
            "1/1 [==============================] - 1s 751ms/step - d_loss: 0.8382 - g_loss: 1.6965\n",
            "Epoch 59/168\n",
            "1/1 [==============================] - 1s 750ms/step - d_loss: 0.7827 - g_loss: 0.7043\n",
            "Epoch 60/168\n",
            "1/1 [==============================] - 1s 727ms/step - d_loss: 0.6399 - g_loss: 1.0789\n",
            "Epoch 61/168\n",
            "1/1 [==============================] - 1s 746ms/step - d_loss: 0.6266 - g_loss: 1.1140\n",
            "Epoch 62/168\n",
            "1/1 [==============================] - 1s 750ms/step - d_loss: 0.6260 - g_loss: 1.1713\n",
            "Epoch 63/168\n",
            "1/1 [==============================] - 1s 726ms/step - d_loss: 0.5995 - g_loss: 1.1331\n",
            "Epoch 64/168\n",
            "1/1 [==============================] - 1s 750ms/step - d_loss: 0.5829 - g_loss: 0.9427\n",
            "Epoch 65/168\n",
            "1/1 [==============================] - 1s 739ms/step - d_loss: 0.7073 - g_loss: 1.0815\n",
            "Epoch 66/168\n",
            "1/1 [==============================] - 1s 721ms/step - d_loss: 0.7052 - g_loss: 0.7485\n",
            "Epoch 67/168\n",
            "1/1 [==============================] - 1s 742ms/step - d_loss: 0.6738 - g_loss: 1.9701\n",
            "Epoch 68/168\n",
            "1/1 [==============================] - 1s 729ms/step - d_loss: 1.1072 - g_loss: 1.0129\n",
            "Epoch 69/168\n",
            "1/1 [==============================] - 1s 719ms/step - d_loss: 0.6874 - g_loss: 2.2176\n",
            "Epoch 70/168\n",
            "1/1 [==============================] - 1s 739ms/step - d_loss: 0.6298 - g_loss: 1.4165\n",
            "Epoch 71/168\n",
            "1/1 [==============================] - 1s 732ms/step - d_loss: 0.8309 - g_loss: 1.1320\n",
            "Epoch 72/168\n",
            "1/1 [==============================] - 1s 719ms/step - d_loss: 0.6803 - g_loss: 0.8229\n",
            "Epoch 73/168\n",
            "1/1 [==============================] - 1s 737ms/step - d_loss: 0.6564 - g_loss: 1.3056\n",
            "Epoch 74/168\n",
            "1/1 [==============================] - 1s 735ms/step - d_loss: 0.7035 - g_loss: 0.5368\n",
            "Epoch 75/168\n",
            "1/1 [==============================] - 1s 729ms/step - d_loss: 0.6932 - g_loss: 1.1595\n",
            "Epoch 76/168\n",
            "1/1 [==============================] - 1s 753ms/step - d_loss: 0.6108 - g_loss: 0.8326\n",
            "Epoch 77/168\n",
            "1/1 [==============================] - 1s 743ms/step - d_loss: 0.6788 - g_loss: 0.9385\n",
            "Epoch 78/168\n",
            "1/1 [==============================] - 1s 724ms/step - d_loss: 0.6271 - g_loss: 0.9387\n",
            "Epoch 79/168\n",
            "1/1 [==============================] - 1s 747ms/step - d_loss: 0.6776 - g_loss: 0.8952\n",
            "Epoch 80/168\n",
            "1/1 [==============================] - 1s 738ms/step - d_loss: 0.7039 - g_loss: 1.0686\n",
            "Epoch 81/168\n",
            "1/1 [==============================] - 1s 723ms/step - d_loss: 0.6466 - g_loss: 0.7110\n",
            "Epoch 82/168\n",
            "1/1 [==============================] - 1s 743ms/step - d_loss: 0.5999 - g_loss: 0.8316\n",
            "Epoch 83/168\n",
            "1/1 [==============================] - 1s 743ms/step - d_loss: 0.5596 - g_loss: 1.2720\n",
            "Epoch 84/168\n",
            "1/1 [==============================] - 1s 727ms/step - d_loss: 0.6561 - g_loss: 0.6843\n",
            "Epoch 85/168\n",
            "1/1 [==============================] - 1s 749ms/step - d_loss: 0.7905 - g_loss: 1.1642\n",
            "Epoch 86/168\n",
            "1/1 [==============================] - 1s 745ms/step - d_loss: 0.7055 - g_loss: 0.8013\n",
            "Epoch 87/168\n",
            "1/1 [==============================] - 1s 780ms/step - d_loss: 0.6588 - g_loss: 1.0204\n",
            "Epoch 88/168\n",
            "1/1 [==============================] - 1s 827ms/step - d_loss: 0.5624 - g_loss: 0.9951\n",
            "Epoch 89/168\n",
            "1/1 [==============================] - 1s 749ms/step - d_loss: 0.6806 - g_loss: 1.0685\n",
            "Epoch 90/168\n",
            "1/1 [==============================] - 1s 724ms/step - d_loss: 0.6486 - g_loss: 1.1935\n",
            "Epoch 91/168\n",
            "1/1 [==============================] - 1s 758ms/step - d_loss: 0.6759 - g_loss: 0.7128\n",
            "Epoch 92/168\n",
            "1/1 [==============================] - 1s 749ms/step - d_loss: 0.6995 - g_loss: 1.1506\n",
            "Epoch 93/168\n",
            "1/1 [==============================] - 1s 739ms/step - d_loss: 0.6013 - g_loss: 1.0864\n",
            "Epoch 94/168\n",
            "1/1 [==============================] - 1s 749ms/step - d_loss: 0.6592 - g_loss: 0.9315\n",
            "Epoch 95/168\n",
            "1/1 [==============================] - 1s 755ms/step - d_loss: 0.6132 - g_loss: 0.6705\n",
            "Epoch 96/168\n",
            "1/1 [==============================] - 1s 727ms/step - d_loss: 0.8052 - g_loss: 1.3094\n",
            "Epoch 97/168\n",
            "1/1 [==============================] - 1s 728ms/step - d_loss: 0.8923 - g_loss: 1.1728\n",
            "Epoch 98/168\n",
            "1/1 [==============================] - 1s 746ms/step - d_loss: 0.6684 - g_loss: 0.7010\n",
            "Epoch 99/168\n",
            "1/1 [==============================] - 1s 729ms/step - d_loss: 0.6482 - g_loss: 3.1375\n",
            "Epoch 100/168\n",
            "1/1 [==============================] - 1s 740ms/step - d_loss: 1.0738 - g_loss: 0.8046\n",
            "Epoch 101/168\n",
            "1/1 [==============================] - 1s 744ms/step - d_loss: 0.6139 - g_loss: 0.6237\n",
            "Epoch 102/168\n",
            "1/1 [==============================] - 1s 735ms/step - d_loss: 0.9897 - g_loss: 1.3565\n",
            "Epoch 103/168\n",
            "1/1 [==============================] - 1s 744ms/step - d_loss: 0.7240 - g_loss: 0.1283\n",
            "Epoch 104/168\n",
            "1/1 [==============================] - 1s 763ms/step - d_loss: 1.9726 - g_loss: 0.8393\n",
            "Epoch 105/168\n",
            "1/1 [==============================] - 1s 753ms/step - d_loss: 0.6750 - g_loss: 1.5364\n",
            "Epoch 106/168\n",
            "1/1 [==============================] - 1s 732ms/step - d_loss: 0.7536 - g_loss: 0.6951\n",
            "Epoch 107/168\n",
            "1/1 [==============================] - 1s 767ms/step - d_loss: 0.6630 - g_loss: 1.0606\n",
            "Epoch 108/168\n",
            "1/1 [==============================] - 1s 752ms/step - d_loss: 0.7075 - g_loss: 0.6922\n",
            "Epoch 109/168\n",
            "1/1 [==============================] - 1s 740ms/step - d_loss: 0.6728 - g_loss: 0.9544\n",
            "Epoch 110/168\n",
            "1/1 [==============================] - 1s 759ms/step - d_loss: 0.6679 - g_loss: 0.8798\n",
            "Epoch 111/168\n",
            "1/1 [==============================] - 1s 742ms/step - d_loss: 0.6772 - g_loss: 0.8333\n",
            "Epoch 112/168\n",
            "1/1 [==============================] - 1s 734ms/step - d_loss: 0.6351 - g_loss: 0.8339\n",
            "Epoch 113/168\n",
            "1/1 [==============================] - 1s 756ms/step - d_loss: 0.6136 - g_loss: 1.0485\n",
            "Epoch 114/168\n",
            "1/1 [==============================] - 1s 736ms/step - d_loss: 0.6573 - g_loss: 0.9220\n",
            "Epoch 115/168\n",
            "1/1 [==============================] - 1s 738ms/step - d_loss: 0.6870 - g_loss: 0.8100\n",
            "Epoch 116/168\n",
            "1/1 [==============================] - 1s 751ms/step - d_loss: 0.6618 - g_loss: 0.9068\n",
            "Epoch 117/168\n",
            "1/1 [==============================] - 1s 739ms/step - d_loss: 0.6604 - g_loss: 0.9515\n",
            "Epoch 118/168\n",
            "1/1 [==============================] - 1s 737ms/step - d_loss: 0.6244 - g_loss: 0.7582\n",
            "Epoch 119/168\n",
            "1/1 [==============================] - 1s 760ms/step - d_loss: 0.7329 - g_loss: 1.3620\n",
            "Epoch 120/168\n",
            "1/1 [==============================] - 1s 743ms/step - d_loss: 0.6986 - g_loss: 0.9000\n",
            "Epoch 121/168\n",
            "1/1 [==============================] - 1s 742ms/step - d_loss: 0.7009 - g_loss: 1.1957\n",
            "Epoch 122/168\n",
            "1/1 [==============================] - 1s 764ms/step - d_loss: 0.8094 - g_loss: 0.9224\n",
            "Epoch 123/168\n",
            "1/1 [==============================] - 1s 740ms/step - d_loss: 0.6625 - g_loss: 0.8758\n",
            "Epoch 124/168\n",
            "1/1 [==============================] - 1s 770ms/step - d_loss: 0.6199 - g_loss: 1.0366\n",
            "Epoch 125/168\n",
            "1/1 [==============================] - 1s 766ms/step - d_loss: 0.7719 - g_loss: 1.0602\n",
            "Epoch 126/168\n",
            "1/1 [==============================] - 1s 748ms/step - d_loss: 0.6788 - g_loss: 0.7201\n",
            "Epoch 127/168\n",
            "1/1 [==============================] - 1s 770ms/step - d_loss: 0.6771 - g_loss: 1.0669\n",
            "Epoch 128/168\n",
            "1/1 [==============================] - 1s 760ms/step - d_loss: 0.6657 - g_loss: 1.0021\n",
            "Epoch 129/168\n",
            "1/1 [==============================] - 1s 743ms/step - d_loss: 0.6316 - g_loss: 0.8160\n",
            "Epoch 130/168\n",
            "1/1 [==============================] - 1s 767ms/step - d_loss: 0.6631 - g_loss: 1.2011\n",
            "Epoch 131/168\n",
            "1/1 [==============================] - 1s 758ms/step - d_loss: 0.6135 - g_loss: 0.7112\n",
            "Epoch 132/168\n",
            "1/1 [==============================] - 1s 748ms/step - d_loss: 0.7104 - g_loss: 1.2117\n",
            "Epoch 133/168\n",
            "1/1 [==============================] - 1s 764ms/step - d_loss: 0.7110 - g_loss: 0.9364\n",
            "Epoch 134/168\n",
            "1/1 [==============================] - 1s 761ms/step - d_loss: 0.6528 - g_loss: 0.9595\n",
            "Epoch 135/168\n",
            "1/1 [==============================] - 1s 741ms/step - d_loss: 0.6270 - g_loss: 1.1364\n",
            "Epoch 136/168\n",
            "1/1 [==============================] - 1s 761ms/step - d_loss: 0.6746 - g_loss: 0.8043\n",
            "Epoch 137/168\n",
            "1/1 [==============================] - 1s 764ms/step - d_loss: 0.6466 - g_loss: 1.4225\n",
            "Epoch 138/168\n",
            "1/1 [==============================] - 1s 743ms/step - d_loss: 0.7077 - g_loss: 0.7951\n",
            "Epoch 139/168\n",
            "1/1 [==============================] - 1s 759ms/step - d_loss: 0.7224 - g_loss: 0.9252\n",
            "Epoch 140/168\n",
            "1/1 [==============================] - 1s 789ms/step - d_loss: 0.7718 - g_loss: 2.2644\n",
            "Epoch 141/168\n",
            "1/1 [==============================] - 1s 756ms/step - d_loss: 0.8672 - g_loss: 0.1409\n",
            "Epoch 142/168\n",
            "1/1 [==============================] - 1s 775ms/step - d_loss: 1.8047 - g_loss: 0.8853\n",
            "Epoch 143/168\n",
            "1/1 [==============================] - 1s 773ms/step - d_loss: 0.7824 - g_loss: 1.9850\n",
            "Epoch 144/168\n",
            "1/1 [==============================] - 1s 740ms/step - d_loss: 1.2416 - g_loss: 0.6785\n",
            "Epoch 145/168\n",
            "1/1 [==============================] - 1s 770ms/step - d_loss: 0.7576 - g_loss: 0.8355\n",
            "Epoch 146/168\n",
            "1/1 [==============================] - 1s 763ms/step - d_loss: 0.6068 - g_loss: 0.8965\n",
            "Epoch 147/168\n",
            "1/1 [==============================] - 1s 740ms/step - d_loss: 0.6732 - g_loss: 0.8527\n",
            "Epoch 148/168\n",
            "1/1 [==============================] - 1s 757ms/step - d_loss: 0.6893 - g_loss: 1.0290\n",
            "Epoch 149/168\n",
            "1/1 [==============================] - 1s 747ms/step - d_loss: 0.6644 - g_loss: 0.8576\n",
            "Epoch 150/168\n",
            "1/1 [==============================] - 1s 733ms/step - d_loss: 0.7127 - g_loss: 1.0137\n",
            "Epoch 151/168\n",
            "1/1 [==============================] - 1s 752ms/step - d_loss: 0.6697 - g_loss: 0.7786\n",
            "Epoch 152/168\n",
            "1/1 [==============================] - 1s 753ms/step - d_loss: 0.6771 - g_loss: 1.0969\n",
            "Epoch 153/168\n",
            "1/1 [==============================] - 1s 737ms/step - d_loss: 0.6804 - g_loss: 0.7392\n",
            "Epoch 154/168\n",
            "1/1 [==============================] - 1s 745ms/step - d_loss: 0.6380 - g_loss: 1.1163\n",
            "Epoch 155/168\n",
            "1/1 [==============================] - 1s 756ms/step - d_loss: 0.6811 - g_loss: 0.6632\n",
            "Epoch 156/168\n",
            "1/1 [==============================] - 1s 734ms/step - d_loss: 0.6567 - g_loss: 1.0484\n",
            "Epoch 157/168\n",
            "1/1 [==============================] - 1s 763ms/step - d_loss: 0.6360 - g_loss: 0.7495\n",
            "Epoch 158/168\n",
            "1/1 [==============================] - 1s 756ms/step - d_loss: 0.6372 - g_loss: 0.9952\n",
            "Epoch 159/168\n",
            "1/1 [==============================] - 1s 758ms/step - d_loss: 0.6057 - g_loss: 1.1015\n",
            "Epoch 160/168\n",
            "1/1 [==============================] - 1s 735ms/step - d_loss: 0.6268 - g_loss: 1.1628\n",
            "Epoch 161/168\n",
            "1/1 [==============================] - 1s 749ms/step - d_loss: 0.7719 - g_loss: 0.8842\n",
            "Epoch 162/168\n",
            "1/1 [==============================] - 1s 732ms/step - d_loss: 0.6062 - g_loss: 1.4723\n",
            "Epoch 163/168\n",
            "1/1 [==============================] - 1s 742ms/step - d_loss: 0.7027 - g_loss: 0.4590\n",
            "Epoch 164/168\n",
            "1/1 [==============================] - 1s 749ms/step - d_loss: 0.8113 - g_loss: 1.1918\n",
            "Epoch 165/168\n",
            "1/1 [==============================] - 1s 729ms/step - d_loss: 0.6868 - g_loss: 0.6052\n",
            "Epoch 166/168\n",
            "1/1 [==============================] - 1s 735ms/step - d_loss: 0.6882 - g_loss: 1.0997\n",
            "Epoch 167/168\n",
            "1/1 [==============================] - 1s 754ms/step - d_loss: 0.6304 - g_loss: 0.7301\n",
            "Epoch 168/168\n",
            "1/1 [==============================] - 1s 744ms/step - d_loss: 0.6223 - g_loss: 0.8765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "epochs = 168  # In practice, use ~100 epochs\n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.001,momentum=0.001),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.001,momentum=0.001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs,steps_per_epoch=1, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        " \n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_Radiography_PNEUMONIAModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_Radiography_PNEUMONIAModel/Discriminator',save_format='tf')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI2ALLkCR2vt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}