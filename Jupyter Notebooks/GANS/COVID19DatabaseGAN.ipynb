{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xjyfa03-ZQT",
        "outputId": "ea647b7e-9ca6-4a48-b492-10fef609c138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (22.0.4)\n",
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (23.0)\n",
            "Collecting typeguard>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, install, tensorflow-addons\n",
            "Successfully installed install-1.3.5 tensorflow-addons-0.19.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install pip install tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOY_iJ28-tjq",
        "outputId": "4ae9b908-17fd-4fd5-fcc3-d66a02e1fc2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CT  X-ray\n"
          ]
        }
      ],
      "source": [
        "import keras \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from enum import Enum\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "# for reproducibility - ref https://machinelearningmastery.com/reproducible-results-neural-networks-keras/ and https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "np.random.seed(9)\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "# loading data from gdrive\n",
        "covid19_dataset = os.path.abspath(\"/content/gdrive/My Drive/COVID-19 Dataset/\")\n",
        "\n",
        "!ls \"/content/gdrive/My Drive/COVID-19 Dataset/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Y1hJwcCMAwmM",
        "outputId": "26e0ae0a-358d-48d5-84c1-3c6eb5a23536"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#@markdown #**Anti-Disconnect for Google Colab**\n",
        "#@markdown ## Run this to stop it from disconnecting automatically \n",
        "#@markdown  **(It will anyhow disconnect after 6 - 12 hrs for using the free version of Colab.)**\n",
        "#@markdown  *(Colab Pro users will get about 24 hrs usage time)*\n",
        "#@markdown ---\n",
        "# taken from https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP_%28z%2Bquantize_method_with_augmentations%2C_user_friendly_interface%29.ipynb#scrollTo=XHyPd4oxVp_l stops colab disconnecting\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "IPython.display.Javascript(js_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xray COVID"
      ],
      "metadata": {
        "id": "vdGobPGzow-X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qFzgcC1_AnH",
        "outputId": "bf475905-3c9f-4fe6-82c2-99602cadac87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4044 files belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128,128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    covid19_dataset + '/X-ray/COVID/', label_mode=None, image_size=image_size, batch_size=16,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M1w4bJp_tAF"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVdcu5MM_vbv",
        "outputId": "985f7c92-9b75-4bef-b9fb-20ed598b71d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "257 4\n"
          ]
        }
      ],
      "source": [
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "print(generator_in_channels, discriminator_in_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRcx8khQ_1Mg",
        "outputId": "5d329a5c-4ff4-4845-e9c5-25e9d4132351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 8192)              2105344   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 128)      262272    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 256)      524544    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 128, 128, 1024)   8389632   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 3)       49155     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,428,611\n",
            "Trainable params: 13,428,611\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(A\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.3),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.3),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.3),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.3),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueLYo9Ww_3m1"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_COVID_Xray')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQZGmdVT_8K4"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/COVID-19 Data Augmented COVID X-Ray' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNH_74E2_-Bp",
        "outputId": "722a3ee9-2036-4e52-9630-da23073fad01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "253/253 [==============================] - 133s 233ms/step - d_loss: 0.5081 - g_loss: 0.6379\n",
            "Epoch 2/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.4262 - g_loss: 0.8379\n",
            "Epoch 3/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.3544 - g_loss: 1.1402\n",
            "Epoch 4/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.4209 - g_loss: 1.2938\n",
            "Epoch 5/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.9504 - g_loss: 0.6536\n",
            "Epoch 6/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7354 - g_loss: 1.0003\n",
            "Epoch 7/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6814 - g_loss: 1.0150\n",
            "Epoch 8/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6497 - g_loss: 0.9612\n",
            "Epoch 9/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6756 - g_loss: 0.7771\n",
            "Epoch 10/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6883 - g_loss: 0.7137\n",
            "Epoch 11/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6837 - g_loss: 0.7961\n",
            "Epoch 12/200\n",
            "253/253 [==============================] - 37s 142ms/step - d_loss: 0.6790 - g_loss: 0.7761\n",
            "Epoch 13/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6903 - g_loss: 0.7577\n",
            "Epoch 14/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6910 - g_loss: 0.7681\n",
            "Epoch 15/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6826 - g_loss: 0.7788\n",
            "Epoch 16/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6782 - g_loss: 0.7725\n",
            "Epoch 17/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6836 - g_loss: 0.7827\n",
            "Epoch 18/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6803 - g_loss: 0.7794\n",
            "Epoch 19/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6792 - g_loss: 0.7686\n",
            "Epoch 20/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6765 - g_loss: 0.7789\n",
            "Epoch 21/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6755 - g_loss: 0.7772\n",
            "Epoch 22/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6827 - g_loss: 0.7640\n",
            "Epoch 23/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6862 - g_loss: 0.7662\n",
            "Epoch 24/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6833 - g_loss: 0.7743\n",
            "Epoch 25/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6804 - g_loss: 0.7786\n",
            "Epoch 26/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6824 - g_loss: 0.7648\n",
            "Epoch 27/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6829 - g_loss: 0.7644\n",
            "Epoch 28/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6788 - g_loss: 0.7710\n",
            "Epoch 29/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6788 - g_loss: 0.7728\n",
            "Epoch 30/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6814 - g_loss: 0.7711\n",
            "Epoch 31/200\n",
            "253/253 [==============================] - 37s 142ms/step - d_loss: 0.6835 - g_loss: 0.7657\n",
            "Epoch 32/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6864 - g_loss: 0.7586\n",
            "Epoch 33/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6871 - g_loss: 0.7646\n",
            "Epoch 34/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6845 - g_loss: 0.7659\n",
            "Epoch 35/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6845 - g_loss: 0.7676\n",
            "Epoch 36/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6865 - g_loss: 0.7625\n",
            "Epoch 37/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6868 - g_loss: 0.7612\n",
            "Epoch 38/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6860 - g_loss: 0.7601\n",
            "Epoch 39/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6866 - g_loss: 0.7637\n",
            "Epoch 40/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6863 - g_loss: 0.7584\n",
            "Epoch 41/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6877 - g_loss: 0.7618\n",
            "Epoch 42/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6870 - g_loss: 0.7579\n",
            "Epoch 43/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6873 - g_loss: 0.7585\n",
            "Epoch 44/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6890 - g_loss: 0.7476\n",
            "Epoch 45/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6885 - g_loss: 0.7546\n",
            "Epoch 46/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6904 - g_loss: 0.7592\n",
            "Epoch 47/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6893 - g_loss: 0.7570\n",
            "Epoch 48/200\n",
            "253/253 [==============================] - 37s 144ms/step - d_loss: 0.6898 - g_loss: 0.7501\n",
            "Epoch 49/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6915 - g_loss: 0.7474\n",
            "Epoch 50/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6913 - g_loss: 0.7591\n",
            "Epoch 51/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6909 - g_loss: 0.7560\n",
            "Epoch 52/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6900 - g_loss: 0.7558\n",
            "Epoch 53/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6900 - g_loss: 0.7605\n",
            "Epoch 54/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6898 - g_loss: 0.7580\n",
            "Epoch 55/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6896 - g_loss: 0.7546\n",
            "Epoch 56/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6896 - g_loss: 0.7631\n",
            "Epoch 57/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6879 - g_loss: 0.7614\n",
            "Epoch 58/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6892 - g_loss: 0.7533\n",
            "Epoch 59/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6894 - g_loss: 0.7599\n",
            "Epoch 60/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6891 - g_loss: 0.7545\n",
            "Epoch 61/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6891 - g_loss: 0.7622\n",
            "Epoch 62/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6895 - g_loss: 0.7546\n",
            "Epoch 63/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6890 - g_loss: 0.7609\n",
            "Epoch 64/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6890 - g_loss: 0.7604\n",
            "Epoch 65/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6893 - g_loss: 0.7577\n",
            "Epoch 66/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6918 - g_loss: 0.7544\n",
            "Epoch 67/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6920 - g_loss: 0.7554\n",
            "Epoch 68/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6924 - g_loss: 0.7545\n",
            "Epoch 69/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6915 - g_loss: 0.7578\n",
            "Epoch 70/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6922 - g_loss: 0.7547\n",
            "Epoch 71/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6931 - g_loss: 0.7536\n",
            "Epoch 72/200\n",
            "253/253 [==============================] - 36s 138ms/step - d_loss: 0.6947 - g_loss: 0.7539\n",
            "Epoch 73/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6947 - g_loss: 0.7537\n",
            "Epoch 74/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6948 - g_loss: 0.7516\n",
            "Epoch 75/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6964 - g_loss: 0.7505\n",
            "Epoch 76/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6982 - g_loss: 0.7465\n",
            "Epoch 77/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6981 - g_loss: 0.7483\n",
            "Epoch 78/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.7005 - g_loss: 0.7451\n",
            "Epoch 79/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.7011 - g_loss: 0.7435\n",
            "Epoch 80/200\n",
            "253/253 [==============================] - 37s 142ms/step - d_loss: 0.7007 - g_loss: 0.7467\n",
            "Epoch 81/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.7021 - g_loss: 0.7453\n",
            "Epoch 82/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7032 - g_loss: 0.7456\n",
            "Epoch 83/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.7023 - g_loss: 0.7459\n",
            "Epoch 84/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.7037 - g_loss: 0.7434\n",
            "Epoch 85/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.7052 - g_loss: 0.7422\n",
            "Epoch 86/200\n",
            "253/253 [==============================] - 35s 138ms/step - d_loss: 0.7065 - g_loss: 0.7425\n",
            "Epoch 87/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7076 - g_loss: 0.7371\n",
            "Epoch 88/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.7070 - g_loss: 0.7394\n",
            "Epoch 89/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7069 - g_loss: 0.7392\n",
            "Epoch 90/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7081 - g_loss: 0.7387\n",
            "Epoch 91/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.7085 - g_loss: 0.7376\n",
            "Epoch 92/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7077 - g_loss: 0.7391\n",
            "Epoch 93/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.7085 - g_loss: 0.7360\n",
            "Epoch 94/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.7080 - g_loss: 0.7402\n",
            "Epoch 95/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7065 - g_loss: 0.7383\n",
            "Epoch 96/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7065 - g_loss: 0.7391\n",
            "Epoch 97/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7057 - g_loss: 0.7385\n",
            "Epoch 98/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7073 - g_loss: 0.7380\n",
            "Epoch 99/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.7071 - g_loss: 0.7372\n",
            "Epoch 100/200\n",
            "253/253 [==============================] - 37s 142ms/step - d_loss: 0.7064 - g_loss: 0.7413\n",
            "Epoch 101/200\n",
            "253/253 [==============================] - 37s 142ms/step - d_loss: 0.7053 - g_loss: 0.7414\n",
            "Epoch 102/200\n",
            "253/253 [==============================] - 37s 145ms/step - d_loss: 0.7046 - g_loss: 0.7405\n",
            "Epoch 103/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.7034 - g_loss: 0.7424\n",
            "Epoch 104/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.7022 - g_loss: 0.7421\n",
            "Epoch 105/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.7008 - g_loss: 0.7451\n",
            "Epoch 106/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6998 - g_loss: 0.7447\n",
            "Epoch 107/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6994 - g_loss: 0.7456\n",
            "Epoch 108/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6985 - g_loss: 0.7467\n",
            "Epoch 109/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6977 - g_loss: 0.7496\n",
            "Epoch 110/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6972 - g_loss: 0.7495\n",
            "Epoch 111/200\n",
            "253/253 [==============================] - 37s 142ms/step - d_loss: 0.6981 - g_loss: 0.7488\n",
            "Epoch 112/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6973 - g_loss: 0.7495\n",
            "Epoch 113/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6986 - g_loss: 0.7519\n",
            "Epoch 114/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6986 - g_loss: 0.7487\n",
            "Epoch 115/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6989 - g_loss: 0.7495\n",
            "Epoch 116/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7000 - g_loss: 0.7493\n",
            "Epoch 117/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7002 - g_loss: 0.7493\n",
            "Epoch 118/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7016 - g_loss: 0.7471\n",
            "Epoch 119/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7012 - g_loss: 0.7455\n",
            "Epoch 120/200\n",
            "253/253 [==============================] - 36s 138ms/step - d_loss: 0.7015 - g_loss: 0.7482\n",
            "Epoch 121/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7017 - g_loss: 0.7480\n",
            "Epoch 122/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.7024 - g_loss: 0.7483\n",
            "Epoch 123/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.7019 - g_loss: 0.7486\n",
            "Epoch 124/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.7017 - g_loss: 0.7478\n",
            "Epoch 125/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.7010 - g_loss: 0.7481\n",
            "Epoch 126/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.7010 - g_loss: 0.7467\n",
            "Epoch 127/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.7008 - g_loss: 0.7467\n",
            "Epoch 128/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.7000 - g_loss: 0.7496\n",
            "Epoch 129/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6994 - g_loss: 0.7494\n",
            "Epoch 130/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6991 - g_loss: 0.7479\n",
            "Epoch 131/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6986 - g_loss: 0.7499\n",
            "Epoch 132/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6978 - g_loss: 0.7485\n",
            "Epoch 133/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6983 - g_loss: 0.7499\n",
            "Epoch 134/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6975 - g_loss: 0.7505\n",
            "Epoch 135/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6972 - g_loss: 0.7515\n",
            "Epoch 136/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6975 - g_loss: 0.7497\n",
            "Epoch 137/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6968 - g_loss: 0.7507\n",
            "Epoch 138/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6969 - g_loss: 0.7516\n",
            "Epoch 139/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6964 - g_loss: 0.7507\n",
            "Epoch 140/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6962 - g_loss: 0.7513\n",
            "Epoch 141/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6952 - g_loss: 0.7502\n",
            "Epoch 142/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6956 - g_loss: 0.7501\n",
            "Epoch 143/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6944 - g_loss: 0.7516\n",
            "Epoch 144/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6947 - g_loss: 0.7527\n",
            "Epoch 145/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6939 - g_loss: 0.7509\n",
            "Epoch 146/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6939 - g_loss: 0.7523\n",
            "Epoch 147/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6937 - g_loss: 0.7535\n",
            "Epoch 148/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6928 - g_loss: 0.7535\n",
            "Epoch 149/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6935 - g_loss: 0.7525\n",
            "Epoch 150/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6925 - g_loss: 0.7514\n",
            "Epoch 151/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6928 - g_loss: 0.7552\n",
            "Epoch 152/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6929 - g_loss: 0.7546\n",
            "Epoch 153/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6922 - g_loss: 0.7537\n",
            "Epoch 154/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6928 - g_loss: 0.7528\n",
            "Epoch 155/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6931 - g_loss: 0.7540\n",
            "Epoch 156/200\n",
            "253/253 [==============================] - 37s 142ms/step - d_loss: 0.6920 - g_loss: 0.7554\n",
            "Epoch 157/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6927 - g_loss: 0.7529\n",
            "Epoch 158/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6924 - g_loss: 0.7543\n",
            "Epoch 159/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6918 - g_loss: 0.7562\n",
            "Epoch 160/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6919 - g_loss: 0.7538\n",
            "Epoch 161/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6923 - g_loss: 0.7564\n",
            "Epoch 162/200\n",
            "253/253 [==============================] - 37s 142ms/step - d_loss: 0.6914 - g_loss: 0.7545\n",
            "Epoch 163/200\n",
            "253/253 [==============================] - 37s 144ms/step - d_loss: 0.6924 - g_loss: 0.7546\n",
            "Epoch 164/200\n",
            "253/253 [==============================] - 37s 144ms/step - d_loss: 0.6919 - g_loss: 0.7539\n",
            "Epoch 165/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6916 - g_loss: 0.7546\n",
            "Epoch 166/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6919 - g_loss: 0.7550\n",
            "Epoch 167/200\n",
            "253/253 [==============================] - 37s 144ms/step - d_loss: 0.6918 - g_loss: 0.7552\n",
            "Epoch 168/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6920 - g_loss: 0.7557\n",
            "Epoch 169/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6914 - g_loss: 0.7544\n",
            "Epoch 170/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6915 - g_loss: 0.7539\n",
            "Epoch 171/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6917 - g_loss: 0.7566\n",
            "Epoch 172/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6918 - g_loss: 0.7540\n",
            "Epoch 173/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6913 - g_loss: 0.7557\n",
            "Epoch 174/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6919 - g_loss: 0.7554\n",
            "Epoch 175/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6918 - g_loss: 0.7546\n",
            "Epoch 176/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6910 - g_loss: 0.7564\n",
            "Epoch 177/200\n",
            "253/253 [==============================] - 37s 144ms/step - d_loss: 0.6913 - g_loss: 0.7557\n",
            "Epoch 178/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6915 - g_loss: 0.7531\n",
            "Epoch 179/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6914 - g_loss: 0.7544\n",
            "Epoch 180/200\n",
            "253/253 [==============================] - 36s 139ms/step - d_loss: 0.6914 - g_loss: 0.7562\n",
            "Epoch 181/200\n",
            "253/253 [==============================] - 37s 144ms/step - d_loss: 0.6913 - g_loss: 0.7556\n",
            "Epoch 182/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6909 - g_loss: 0.7549\n",
            "Epoch 183/200\n",
            "253/253 [==============================] - 37s 144ms/step - d_loss: 0.6907 - g_loss: 0.7532\n",
            "Epoch 184/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6908 - g_loss: 0.7543\n",
            "Epoch 185/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6910 - g_loss: 0.7552\n",
            "Epoch 186/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6903 - g_loss: 0.7542\n",
            "Epoch 187/200\n",
            "253/253 [==============================] - 37s 144ms/step - d_loss: 0.6907 - g_loss: 0.7555\n",
            "Epoch 188/200\n",
            "253/253 [==============================] - 36s 140ms/step - d_loss: 0.6904 - g_loss: 0.7567\n",
            "Epoch 189/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6908 - g_loss: 0.7563\n",
            "Epoch 190/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6904 - g_loss: 0.7544\n",
            "Epoch 191/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6900 - g_loss: 0.7573\n",
            "Epoch 192/200\n",
            "253/253 [==============================] - 36s 142ms/step - d_loss: 0.6906 - g_loss: 0.7549\n",
            "Epoch 193/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6910 - g_loss: 0.7570\n",
            "Epoch 194/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6903 - g_loss: 0.7589\n",
            "Epoch 195/200\n",
            "253/253 [==============================] - 38s 147ms/step - d_loss: 0.6906 - g_loss: 0.7554\n",
            "Epoch 196/200\n",
            "253/253 [==============================] - 37s 142ms/step - d_loss: 0.6904 - g_loss: 0.7546\n",
            "Epoch 197/200\n",
            "253/253 [==============================] - 36s 141ms/step - d_loss: 0.6903 - g_loss: 0.7574\n",
            "Epoch 198/200\n",
            "253/253 [==============================] - 37s 143ms/step - d_loss: 0.6897 - g_loss: 0.7577\n",
            "Epoch 199/200\n",
            "253/253 [==============================] - 37s 145ms/step - d_loss: 0.6905 - g_loss: 0.7567\n",
            "Epoch 200/200\n",
            "253/253 [==============================] - 37s 146ms/step - d_loss: 0.6901 - g_loss: 0.7557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "epochs = 200  # In practice, use ~100 epochs \n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Xray_COVID_Model_test_Extensive/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Xray_COVID_Model_test_Extensive/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X-Ray Non covid model"
      ],
      "metadata": {
        "id": "nBmLr_gV6rxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load images\n",
        "image_size = (128,128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    covid19_dataset + '/X-ray/Non-COVID/', label_mode=None, image_size=image_size, batch_size=16,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su7ByhNwo7nz",
        "outputId": "397df95d-814d-4511-a4df-294ec43a2dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5493 files belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 256"
      ],
      "metadata": {
        "id": "LaDBxus7qXgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(16 * 16 * 128),\n",
        "        layers.Reshape((16, 16, 128)),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2fVtNldqkGq",
        "outputId": "955deddb-4765-4af5-8721-ce23f9d8c70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 32768)             8421376   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 32, 32, 256)      524544    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 64, 64, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 128, 128, 1024)   8389632   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 3)       49155     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,482,371\n",
            "Trainable params: 19,482,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_NON_COVID_Xray')\n",
        "\n"
      ],
      "metadata": {
        "id": "BWPqNyImqdbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_COVID_Xray_Non_COVID' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))\n"
      ],
      "metadata": {
        "id": "ZhTfSLMdpHvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100  # In practice, use ~100 epochs \n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.00001,momentum=0),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/COVID-Data_Augmented_COVID_XrayModelNonCovid/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/COVID-Data_Augmented_COVID_XrayModelNonCovid/Discriminator',save_format='tf')\n"
      ],
      "metadata": {
        "id": "m3xnZDIa6tb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10b6d40-8adb-41aa-c695-447bf51a36b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "344/344 [==============================] - 41s 109ms/step - d_loss: 0.6459 - g_loss: 0.9787\n",
            "Epoch 2/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6792 - g_loss: 0.8639\n",
            "Epoch 3/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6997 - g_loss: 0.6929\n",
            "Epoch 4/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6943 - g_loss: 0.7507\n",
            "Epoch 5/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6871 - g_loss: 0.7922\n",
            "Epoch 6/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6799 - g_loss: 0.7965\n",
            "Epoch 7/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6955 - g_loss: 0.7607\n",
            "Epoch 8/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6935 - g_loss: 0.7476\n",
            "Epoch 9/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6914 - g_loss: 0.7570\n",
            "Epoch 10/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6938 - g_loss: 0.7571\n",
            "Epoch 11/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6835 - g_loss: 0.7700\n",
            "Epoch 12/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6880 - g_loss: 0.7650\n",
            "Epoch 13/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6852 - g_loss: 0.7675\n",
            "Epoch 14/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6910 - g_loss: 0.7535\n",
            "Epoch 15/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6936 - g_loss: 0.7502\n",
            "Epoch 16/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6927 - g_loss: 0.7596\n",
            "Epoch 17/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6938 - g_loss: 0.7491\n",
            "Epoch 18/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6925 - g_loss: 0.7514\n",
            "Epoch 19/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6974 - g_loss: 0.7390\n",
            "Epoch 20/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6922 - g_loss: 0.7508\n",
            "Epoch 21/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6924 - g_loss: 0.7519\n",
            "Epoch 22/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6926 - g_loss: 0.7501\n",
            "Epoch 23/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6927 - g_loss: 0.7471\n",
            "Epoch 24/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6937 - g_loss: 0.7468\n",
            "Epoch 25/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6950 - g_loss: 0.7445\n",
            "Epoch 26/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6925 - g_loss: 0.7517\n",
            "Epoch 27/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6918 - g_loss: 0.7497\n",
            "Epoch 28/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6921 - g_loss: 0.7461\n",
            "Epoch 29/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6942 - g_loss: 0.7441\n",
            "Epoch 30/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6940 - g_loss: 0.7548\n",
            "Epoch 31/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6934 - g_loss: 0.7502\n",
            "Epoch 32/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6925 - g_loss: 0.7522\n",
            "Epoch 33/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6937 - g_loss: 0.7474\n",
            "Epoch 34/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6948 - g_loss: 0.7445\n",
            "Epoch 35/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6943 - g_loss: 0.7523\n",
            "Epoch 36/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6942 - g_loss: 0.7494\n",
            "Epoch 37/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6931 - g_loss: 0.7515\n",
            "Epoch 38/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6934 - g_loss: 0.7486\n",
            "Epoch 39/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6943 - g_loss: 0.7437\n",
            "Epoch 40/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6946 - g_loss: 0.7466\n",
            "Epoch 41/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6943 - g_loss: 0.7480\n",
            "Epoch 42/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6940 - g_loss: 0.7489\n",
            "Epoch 43/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6929 - g_loss: 0.7491\n",
            "Epoch 44/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6927 - g_loss: 0.7510\n",
            "Epoch 45/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6933 - g_loss: 0.7538\n",
            "Epoch 46/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6936 - g_loss: 0.7532\n",
            "Epoch 47/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6931 - g_loss: 0.7500\n",
            "Epoch 48/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6940 - g_loss: 0.7532\n",
            "Epoch 49/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6940 - g_loss: 0.7521\n",
            "Epoch 50/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6933 - g_loss: 0.7512\n",
            "Epoch 51/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6926 - g_loss: 0.7529\n",
            "Epoch 52/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6909 - g_loss: 0.7549\n",
            "Epoch 53/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6910 - g_loss: 0.7536\n",
            "Epoch 54/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6914 - g_loss: 0.7559\n",
            "Epoch 55/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6927 - g_loss: 0.7533\n",
            "Epoch 56/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6924 - g_loss: 0.7540\n",
            "Epoch 57/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6927 - g_loss: 0.7550\n",
            "Epoch 58/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6944 - g_loss: 0.7522\n",
            "Epoch 59/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6964 - g_loss: 0.7512\n",
            "Epoch 60/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6995 - g_loss: 0.7490\n",
            "Epoch 61/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.7016 - g_loss: 0.7497\n",
            "Epoch 62/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.7021 - g_loss: 0.7454\n",
            "Epoch 63/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.7019 - g_loss: 0.7470\n",
            "Epoch 64/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.7019 - g_loss: 0.7477\n",
            "Epoch 65/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.7009 - g_loss: 0.7502\n",
            "Epoch 66/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6994 - g_loss: 0.7471\n",
            "Epoch 67/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6986 - g_loss: 0.7494\n",
            "Epoch 68/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6987 - g_loss: 0.7480\n",
            "Epoch 69/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6972 - g_loss: 0.7504\n",
            "Epoch 70/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6981 - g_loss: 0.7512\n",
            "Epoch 71/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6971 - g_loss: 0.7526\n",
            "Epoch 72/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6978 - g_loss: 0.7486\n",
            "Epoch 73/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6977 - g_loss: 0.7500\n",
            "Epoch 74/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6991 - g_loss: 0.7503\n",
            "Epoch 75/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6992 - g_loss: 0.7498\n",
            "Epoch 76/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6977 - g_loss: 0.7500\n",
            "Epoch 77/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6981 - g_loss: 0.7508\n",
            "Epoch 78/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6978 - g_loss: 0.7499\n",
            "Epoch 79/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6985 - g_loss: 0.7490\n",
            "Epoch 80/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6983 - g_loss: 0.7495\n",
            "Epoch 81/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6985 - g_loss: 0.7484\n",
            "Epoch 82/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6982 - g_loss: 0.7488\n",
            "Epoch 83/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6977 - g_loss: 0.7479\n",
            "Epoch 84/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6978 - g_loss: 0.7486\n",
            "Epoch 85/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6973 - g_loss: 0.7497\n",
            "Epoch 86/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6966 - g_loss: 0.7489\n",
            "Epoch 87/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6963 - g_loss: 0.7500\n",
            "Epoch 88/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6956 - g_loss: 0.7515\n",
            "Epoch 89/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6949 - g_loss: 0.7499\n",
            "Epoch 90/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6947 - g_loss: 0.7525\n",
            "Epoch 91/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6947 - g_loss: 0.7520\n",
            "Epoch 92/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6940 - g_loss: 0.7501\n",
            "Epoch 93/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6936 - g_loss: 0.7521\n",
            "Epoch 94/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6934 - g_loss: 0.7514\n",
            "Epoch 95/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6926 - g_loss: 0.7556\n",
            "Epoch 96/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6926 - g_loss: 0.7528\n",
            "Epoch 97/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6913 - g_loss: 0.7549\n",
            "Epoch 98/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6914 - g_loss: 0.7521\n",
            "Epoch 99/100\n",
            "344/344 [==============================] - 38s 109ms/step - d_loss: 0.6912 - g_loss: 0.7563\n",
            "Epoch 100/100\n",
            "344/344 [==============================] - 38s 110ms/step - d_loss: 0.6912 - g_loss: 0.7575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CT COVID"
      ],
      "metadata": {
        "id": "LNcHxahEo9_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIHW_gktBCi3",
        "outputId": "e3448a87-cb47-417e-91df-7fa9c3b87d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5427 files belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "# load images\n",
        "image_size = (128,128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    covid19_dataset + '/CT/COVID/', label_mode=None, image_size=image_size, batch_size=64,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRhSaoghZHaO"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "url2vqiYZMdf",
        "outputId": "c2a59ad3-56e9-4647-b594-9632db0bd524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129 4\n"
          ]
        }
      ],
      "source": [
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "print(generator_in_channels, discriminator_in_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cITyyvXVZPr2",
        "outputId": "4f80b3b1-00a5-4753-b5fb-fe7fc7a49db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 8192)              1056768   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 128)      262272    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 256)      524544    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 128, 128, 1024)   8389632   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 128, 128, 1024)    0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 3)       49155     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,380,035\n",
            "Trainable params: 12,380,035\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go-V8ASVZSua"
      },
      "outputs": [],
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_COVID_CT')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o0hdV5GZV_k"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/COVID-19 Data Augmented COVID CT' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnpBNr4AZYf_",
        "outputId": "b1f1fc71-eea3-4f62-ca7d-02744fed5d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "85/85 [==============================] - 44s 476ms/step - d_loss: 0.5077 - g_loss: 0.7655\n",
            "Epoch 2/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6077 - g_loss: 1.0237\n",
            "Epoch 3/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6522 - g_loss: 0.8763\n",
            "Epoch 4/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6348 - g_loss: 0.8716\n",
            "Epoch 5/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6530 - g_loss: 0.8438\n",
            "Epoch 6/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6408 - g_loss: 0.8569\n",
            "Epoch 7/100\n",
            "85/85 [==============================] - 35s 410ms/step - d_loss: 0.6335 - g_loss: 0.8718\n",
            "Epoch 8/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6314 - g_loss: 0.8751\n",
            "Epoch 9/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6307 - g_loss: 0.8792\n",
            "Epoch 10/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6296 - g_loss: 0.8904\n",
            "Epoch 11/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6432 - g_loss: 0.8963\n",
            "Epoch 12/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6310 - g_loss: 0.9101\n",
            "Epoch 13/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6319 - g_loss: 0.9100\n",
            "Epoch 14/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6256 - g_loss: 0.9276\n",
            "Epoch 15/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6215 - g_loss: 0.9427\n",
            "Epoch 16/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6226 - g_loss: 0.9527\n",
            "Epoch 17/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6217 - g_loss: 0.9411\n",
            "Epoch 18/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6258 - g_loss: 0.9412\n",
            "Epoch 19/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6358 - g_loss: 0.9473\n",
            "Epoch 20/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6491 - g_loss: 0.9261\n",
            "Epoch 21/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6539 - g_loss: 0.9150\n",
            "Epoch 22/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6481 - g_loss: 0.9416\n",
            "Epoch 23/100\n",
            "85/85 [==============================] - 35s 408ms/step - d_loss: 0.6534 - g_loss: 0.9156\n",
            "Epoch 24/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6554 - g_loss: 0.9058\n",
            "Epoch 25/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6509 - g_loss: 0.9193\n",
            "Epoch 26/100\n",
            "85/85 [==============================] - 35s 408ms/step - d_loss: 0.6502 - g_loss: 0.9174\n",
            "Epoch 27/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6509 - g_loss: 0.9177\n",
            "Epoch 28/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6523 - g_loss: 0.9195\n",
            "Epoch 29/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6476 - g_loss: 0.9167\n",
            "Epoch 30/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6493 - g_loss: 0.9166\n",
            "Epoch 31/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6493 - g_loss: 0.9131\n",
            "Epoch 32/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6565 - g_loss: 0.9083\n",
            "Epoch 33/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6503 - g_loss: 0.9123\n",
            "Epoch 34/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6488 - g_loss: 0.9188\n",
            "Epoch 35/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6513 - g_loss: 0.9156\n",
            "Epoch 36/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6484 - g_loss: 0.9178\n",
            "Epoch 37/100\n",
            "85/85 [==============================] - 35s 408ms/step - d_loss: 0.6513 - g_loss: 0.9153\n",
            "Epoch 38/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6470 - g_loss: 0.9188\n",
            "Epoch 39/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6461 - g_loss: 0.9210\n",
            "Epoch 40/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6459 - g_loss: 0.9119\n",
            "Epoch 41/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6448 - g_loss: 0.9254\n",
            "Epoch 42/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6468 - g_loss: 0.9120\n",
            "Epoch 43/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6447 - g_loss: 0.9098\n",
            "Epoch 44/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6428 - g_loss: 0.9165\n",
            "Epoch 45/100\n",
            "85/85 [==============================] - 35s 409ms/step - d_loss: 0.6415 - g_loss: 0.9212\n",
            "Epoch 46/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6444 - g_loss: 0.9306\n",
            "Epoch 47/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6405 - g_loss: 0.9187\n",
            "Epoch 48/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6418 - g_loss: 0.9368\n",
            "Epoch 49/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6414 - g_loss: 0.9227\n",
            "Epoch 50/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6404 - g_loss: 0.9259\n",
            "Epoch 51/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6416 - g_loss: 0.9366\n",
            "Epoch 52/100\n",
            "85/85 [==============================] - 35s 408ms/step - d_loss: 0.6378 - g_loss: 0.9338\n",
            "Epoch 53/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6407 - g_loss: 0.9411\n",
            "Epoch 54/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6392 - g_loss: 0.9363\n",
            "Epoch 55/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6383 - g_loss: 0.9422\n",
            "Epoch 56/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6400 - g_loss: 0.9322\n",
            "Epoch 57/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6364 - g_loss: 0.9339\n",
            "Epoch 58/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6410 - g_loss: 0.9450\n",
            "Epoch 59/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6366 - g_loss: 0.9445\n",
            "Epoch 60/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6408 - g_loss: 0.9485\n",
            "Epoch 61/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6367 - g_loss: 0.9419\n",
            "Epoch 62/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6344 - g_loss: 0.9539\n",
            "Epoch 63/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6377 - g_loss: 0.9486\n",
            "Epoch 64/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6386 - g_loss: 0.9481\n",
            "Epoch 65/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6361 - g_loss: 0.9519\n",
            "Epoch 66/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6366 - g_loss: 0.9508\n",
            "Epoch 67/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6383 - g_loss: 0.9516\n",
            "Epoch 68/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6397 - g_loss: 0.9605\n",
            "Epoch 69/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6384 - g_loss: 0.9611\n",
            "Epoch 70/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6376 - g_loss: 0.9535\n",
            "Epoch 71/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6372 - g_loss: 0.9525\n",
            "Epoch 72/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6388 - g_loss: 0.9555\n",
            "Epoch 73/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6376 - g_loss: 0.9580\n",
            "Epoch 74/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6397 - g_loss: 0.9540\n",
            "Epoch 75/100\n",
            "85/85 [==============================] - 35s 408ms/step - d_loss: 0.6391 - g_loss: 0.9486\n",
            "Epoch 76/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6391 - g_loss: 0.9572\n",
            "Epoch 77/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6380 - g_loss: 0.9621\n",
            "Epoch 78/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6385 - g_loss: 0.9480\n",
            "Epoch 79/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6375 - g_loss: 0.9593\n",
            "Epoch 80/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6382 - g_loss: 0.9583\n",
            "Epoch 81/100\n",
            "85/85 [==============================] - 35s 409ms/step - d_loss: 0.6412 - g_loss: 0.9477\n",
            "Epoch 82/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6396 - g_loss: 0.9501\n",
            "Epoch 83/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6405 - g_loss: 0.9524\n",
            "Epoch 84/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6370 - g_loss: 0.9564\n",
            "Epoch 85/100\n",
            "85/85 [==============================] - 35s 408ms/step - d_loss: 0.6403 - g_loss: 0.9563\n",
            "Epoch 86/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6421 - g_loss: 0.9516\n",
            "Epoch 87/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6429 - g_loss: 0.9433\n",
            "Epoch 88/100\n",
            "85/85 [==============================] - 35s 408ms/step - d_loss: 0.6425 - g_loss: 0.9588\n",
            "Epoch 89/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6444 - g_loss: 0.9527\n",
            "Epoch 90/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6417 - g_loss: 0.9527\n",
            "Epoch 91/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6418 - g_loss: 0.9532\n",
            "Epoch 92/100\n",
            "85/85 [==============================] - 35s 408ms/step - d_loss: 0.6414 - g_loss: 0.9465\n",
            "Epoch 93/100\n",
            "85/85 [==============================] - 35s 408ms/step - d_loss: 0.6436 - g_loss: 0.9606\n",
            "Epoch 94/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6443 - g_loss: 0.9551\n",
            "Epoch 95/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6441 - g_loss: 0.9656\n",
            "Epoch 96/100\n",
            "85/85 [==============================] - 35s 405ms/step - d_loss: 0.6413 - g_loss: 0.9499\n",
            "Epoch 97/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6414 - g_loss: 0.9578\n",
            "Epoch 98/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6418 - g_loss: 0.9612\n",
            "Epoch 99/100\n",
            "85/85 [==============================] - 35s 407ms/step - d_loss: 0.6451 - g_loss: 0.9499\n",
            "Epoch 100/100\n",
            "85/85 [==============================] - 35s 406ms/step - d_loss: 0.6443 - g_loss: 0.9409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "epochs = 100  # In practice, use ~100 epochs \n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/COVID-19_Augmented_COVID_CTModel/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/COVID-19_Augmented_COVID_CTModel/Discriminator',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CT Non COVID"
      ],
      "metadata": {
        "id": "AoKCk7WXpA-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load images\n",
        "image_size = (128,128)\n",
        "img_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    covid19_dataset + '/CT/Non-COVID/', label_mode=None, image_size=image_size, batch_size=64,crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "dataset = img_dataset.map(lambda x: x / 255.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqE5Czr6pEDY",
        "outputId": "a5bf23c6-cd80-495d-dcd1-fc845263a816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2627 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = 3\n",
        "num_classes = 1\n",
        "latent_dim = 256"
      ],
      "metadata": {
        "id": "hUNd0_vzqp1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(4 * 4 * 128),\n",
        "        layers.Reshape((4, 4, 128)),\n",
        "        layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uXvjGylqqZU",
        "outputId": "172bd6e6-4fde-47b2-ea54-322fd7d99143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_56 (Conv2D)          (None, 64, 64, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu_99 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_57 (Conv2D)          (None, 32, 32, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_100 (LeakyReLU)  (None, 32, 32, 128)      0         \n",
            "                                                                 \n",
            " conv2d_58 (Conv2D)          (None, 16, 16, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_101 (LeakyReLU)  (None, 16, 16, 128)      0         \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 32768)             0         \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 429,377\n",
            "Trainable params: 429,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_29 (Dense)            (None, 2048)              526336    \n",
            "                                                                 \n",
            " reshape_14 (Reshape)        (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_57 (Conv2D  (None, 8, 8, 64)         131136    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_102 (LeakyReLU)  (None, 8, 8, 64)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_58 (Conv2D  (None, 16, 16, 128)      131200    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_103 (LeakyReLU)  (None, 16, 16, 128)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_59 (Conv2D  (None, 32, 32, 256)      524544    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_104 (LeakyReLU)  (None, 32, 32, 256)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_60 (Conv2D  (None, 64, 64, 512)      2097664   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_105 (LeakyReLU)  (None, 64, 64, 512)      0         \n",
            "                                                                 \n",
            " conv2d_transpose_61 (Conv2D  (None, 128, 128, 1024)   8389632   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " leaky_re_lu_106 (LeakyReLU)  (None, 128, 128, 1024)   0         \n",
            "                                                                 \n",
            " conv2d_59 (Conv2D)          (None, 128, 128, 3)       49155     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,849,667\n",
            "Trainable params: 11,849,667\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_NON_COVID_CT')\n",
        "\n"
      ],
      "metadata": {
        "id": "5IfuY-ZEqqmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfCFY6DvZpNp"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        imageFolder = 0\n",
        "        for i in range(self.num_img):\n",
        "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save('/content/gdrive/My Drive/Data_Augmented_COVID_CT_Non_COVID' + '/' + \"generated_img_%03d_%d.png\" % (epoch, i))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100  # In practice, use ~100 epochs \n",
        "\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0),\n",
        "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.0001,momentum=0),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "history = gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "\n",
        "model = gan.get_gan\n",
        "# Save the model\n",
        "generator.save('/content/gdrive/My Drive/Data_Augmented_COVID_CTModelNonCovid/Generator',save_format='tf')\n",
        "discriminator.save('/content/gdrive/My Drive/Data_Augmented_COVID_CTModelNonCovid/Discriminator',save_format='tf')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QJk4Zwip5yv",
        "outputId": "731c6413-7581-4c16-d752-e9cfa35e5d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "42/42 [==============================] - 20s 398ms/step - d_loss: 0.4833 - g_loss: 0.7271\n",
            "Epoch 2/100\n",
            "42/42 [==============================] - 17s 396ms/step - d_loss: 0.2744 - g_loss: 1.2979\n",
            "Epoch 3/100\n",
            "42/42 [==============================] - 17s 397ms/step - d_loss: 0.4096 - g_loss: 2.2703\n",
            "Epoch 4/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.3900 - g_loss: 2.5052\n",
            "Epoch 5/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6553 - g_loss: 1.6746\n",
            "Epoch 6/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.7077 - g_loss: 1.2637\n",
            "Epoch 7/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5952 - g_loss: 1.1593\n",
            "Epoch 8/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.5944 - g_loss: 1.0245\n",
            "Epoch 9/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6348 - g_loss: 0.9312\n",
            "Epoch 10/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6154 - g_loss: 0.9556\n",
            "Epoch 11/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6286 - g_loss: 0.9210\n",
            "Epoch 12/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6193 - g_loss: 0.9498\n",
            "Epoch 13/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6059 - g_loss: 0.9430\n",
            "Epoch 14/100\n",
            "42/42 [==============================] - 18s 401ms/step - d_loss: 0.5949 - g_loss: 0.9733\n",
            "Epoch 15/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6007 - g_loss: 0.9783\n",
            "Epoch 16/100\n",
            "42/42 [==============================] - 17s 401ms/step - d_loss: 0.6005 - g_loss: 1.0051\n",
            "Epoch 17/100\n",
            "42/42 [==============================] - 18s 401ms/step - d_loss: 0.5889 - g_loss: 1.0348\n",
            "Epoch 18/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6044 - g_loss: 1.0485\n",
            "Epoch 19/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5954 - g_loss: 1.0551\n",
            "Epoch 20/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5940 - g_loss: 1.0897\n",
            "Epoch 21/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.5932 - g_loss: 1.0946\n",
            "Epoch 22/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.5853 - g_loss: 1.1228\n",
            "Epoch 23/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.5832 - g_loss: 1.1448\n",
            "Epoch 24/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.5785 - g_loss: 1.1258\n",
            "Epoch 25/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5772 - g_loss: 1.1723\n",
            "Epoch 26/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5657 - g_loss: 1.1906\n",
            "Epoch 27/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.5719 - g_loss: 1.2045\n",
            "Epoch 28/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5448 - g_loss: 1.2361\n",
            "Epoch 29/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5487 - g_loss: 1.2460\n",
            "Epoch 30/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5377 - g_loss: 1.2213\n",
            "Epoch 31/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5408 - g_loss: 1.2695\n",
            "Epoch 32/100\n",
            "42/42 [==============================] - 18s 402ms/step - d_loss: 0.5351 - g_loss: 1.3018\n",
            "Epoch 33/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.5312 - g_loss: 1.3049\n",
            "Epoch 34/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5381 - g_loss: 1.3220\n",
            "Epoch 35/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.5417 - g_loss: 1.3136\n",
            "Epoch 36/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.5463 - g_loss: 1.3002\n",
            "Epoch 37/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.5683 - g_loss: 1.3026\n",
            "Epoch 38/100\n",
            "42/42 [==============================] - 18s 404ms/step - d_loss: 0.5809 - g_loss: 1.2230\n",
            "Epoch 39/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.5917 - g_loss: 1.1916\n",
            "Epoch 40/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6055 - g_loss: 1.1876\n",
            "Epoch 41/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6005 - g_loss: 1.1852\n",
            "Epoch 42/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6046 - g_loss: 1.1477\n",
            "Epoch 43/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6085 - g_loss: 1.1604\n",
            "Epoch 44/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6064 - g_loss: 1.1337\n",
            "Epoch 45/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6028 - g_loss: 1.1442\n",
            "Epoch 46/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6238 - g_loss: 1.1575\n",
            "Epoch 47/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6056 - g_loss: 1.1404\n",
            "Epoch 48/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6249 - g_loss: 1.1596\n",
            "Epoch 49/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6227 - g_loss: 1.0937\n",
            "Epoch 50/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6284 - g_loss: 1.0748\n",
            "Epoch 51/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6262 - g_loss: 1.0619\n",
            "Epoch 52/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6199 - g_loss: 1.0740\n",
            "Epoch 53/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6281 - g_loss: 1.1087\n",
            "Epoch 54/100\n",
            "42/42 [==============================] - 17s 401ms/step - d_loss: 0.6170 - g_loss: 1.0802\n",
            "Epoch 55/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6203 - g_loss: 1.0756\n",
            "Epoch 56/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6148 - g_loss: 1.0584\n",
            "Epoch 57/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6195 - g_loss: 1.0454\n",
            "Epoch 58/100\n",
            "42/42 [==============================] - 18s 406ms/step - d_loss: 0.6210 - g_loss: 1.0632\n",
            "Epoch 59/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6148 - g_loss: 1.0821\n",
            "Epoch 60/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6263 - g_loss: 1.0290\n",
            "Epoch 61/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6166 - g_loss: 1.0329\n",
            "Epoch 62/100\n",
            "42/42 [==============================] - 17s 401ms/step - d_loss: 0.6155 - g_loss: 1.0433\n",
            "Epoch 63/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6205 - g_loss: 1.0423\n",
            "Epoch 64/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6279 - g_loss: 1.0447\n",
            "Epoch 65/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6234 - g_loss: 1.0239\n",
            "Epoch 66/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6273 - g_loss: 1.0913\n",
            "Epoch 67/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6269 - g_loss: 1.0255\n",
            "Epoch 68/100\n",
            "42/42 [==============================] - 17s 401ms/step - d_loss: 0.6303 - g_loss: 1.0358\n",
            "Epoch 69/100\n",
            "42/42 [==============================] - 18s 402ms/step - d_loss: 0.6324 - g_loss: 1.0418\n",
            "Epoch 70/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6197 - g_loss: 1.0607\n",
            "Epoch 71/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6279 - g_loss: 1.0467\n",
            "Epoch 72/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6248 - g_loss: 1.0142\n",
            "Epoch 73/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6251 - g_loss: 1.0554\n",
            "Epoch 74/100\n",
            "42/42 [==============================] - 18s 403ms/step - d_loss: 0.6381 - g_loss: 1.0297\n",
            "Epoch 75/100\n",
            "42/42 [==============================] - 17s 401ms/step - d_loss: 0.6280 - g_loss: 1.0719\n",
            "Epoch 76/100\n",
            "42/42 [==============================] - 18s 401ms/step - d_loss: 0.6422 - g_loss: 1.0389\n",
            "Epoch 77/100\n",
            "42/42 [==============================] - 17s 401ms/step - d_loss: 0.6370 - g_loss: 1.0428\n",
            "Epoch 78/100\n",
            "42/42 [==============================] - 17s 402ms/step - d_loss: 0.6419 - g_loss: 1.0367\n",
            "Epoch 79/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6431 - g_loss: 1.0614\n",
            "Epoch 80/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6390 - g_loss: 1.0300\n",
            "Epoch 81/100\n",
            "42/42 [==============================] - 17s 401ms/step - d_loss: 0.6420 - g_loss: 1.0297\n",
            "Epoch 82/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6406 - g_loss: 1.0458\n",
            "Epoch 83/100\n",
            "42/42 [==============================] - 17s 402ms/step - d_loss: 0.6321 - g_loss: 1.0249\n",
            "Epoch 84/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6489 - g_loss: 1.0505\n",
            "Epoch 85/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6404 - g_loss: 1.0313\n",
            "Epoch 86/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6459 - g_loss: 1.0502\n",
            "Epoch 87/100\n",
            "42/42 [==============================] - 17s 398ms/step - d_loss: 0.6503 - g_loss: 1.0289\n",
            "Epoch 88/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6352 - g_loss: 1.0709\n",
            "Epoch 89/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6511 - g_loss: 1.0212\n",
            "Epoch 90/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6536 - g_loss: 1.0244\n",
            "Epoch 91/100\n",
            "42/42 [==============================] - 17s 401ms/step - d_loss: 0.6421 - g_loss: 1.0331\n",
            "Epoch 92/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6563 - g_loss: 1.0523\n",
            "Epoch 93/100\n",
            "42/42 [==============================] - 17s 401ms/step - d_loss: 0.6484 - g_loss: 1.0066\n",
            "Epoch 94/100\n",
            "42/42 [==============================] - 18s 401ms/step - d_loss: 0.6442 - g_loss: 1.0493\n",
            "Epoch 95/100\n",
            "42/42 [==============================] - 17s 401ms/step - d_loss: 0.6509 - g_loss: 1.0362\n",
            "Epoch 96/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6461 - g_loss: 1.0206\n",
            "Epoch 97/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6532 - g_loss: 1.0071\n",
            "Epoch 98/100\n",
            "42/42 [==============================] - 17s 400ms/step - d_loss: 0.6524 - g_loss: 1.0227\n",
            "Epoch 99/100\n",
            "42/42 [==============================] - 17s 399ms/step - d_loss: 0.6570 - g_loss: 1.0300\n",
            "Epoch 100/100\n",
            "42/42 [==============================] - 18s 403ms/step - d_loss: 0.6522 - g_loss: 1.0048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Models"
      ],
      "metadata": {
        "id": "B-W7758Bwqqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "    def get_gan():\n",
        "      return GAN(name='DC_GAN_COVID19_COVID_Xray')\n",
        "\n"
      ],
      "metadata": {
        "id": "3MFfY20GydMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need to generate 1k COVID images\n",
        "generator = keras.models.load_model('/content/gdrive/My Drive/COVID-19_Augmented_COVID_XRayModel/Generator')\n",
        "discriminator = keras.models.load_model('/content/gdrive/My Drive/COVID-19_Augmented_COVID_XRayModel/Discriminator/')\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=256)\n",
        "gan.compile(\n",
        "    discriminator,\n",
        "    generator,\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "generated_batch = 0\n",
        "generator.summary\n",
        "for i in range(100):\n",
        "  random_latent_vectors = tf.random.normal(shape=(10, 256))\n",
        "  generated_images = gan.generator(random_latent_vectors)\n",
        "  generated_images *= 255\n",
        "  generated_images.numpy()\n",
        "  imageFolder = 0\n",
        "  for j in range(10):\n",
        "    img = tf.keras.preprocessing.image.array_to_img(generated_images[j])\n",
        "    img.save('/content/gdrive/My Drive/COVIDXrayAugmentedDCGANExtensiveFinal' + '/' + \"generated_img_%03d_%d.png\" % (generated_batch,j))\n",
        "  generated_batch += 1\n"
      ],
      "metadata": {
        "id": "nA0cqo3src2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7455bd8b-d664-4d90-908c-0495cdb74756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# meed 2270 images of CT Non COVID\n",
        "generator = keras.models.load_model('/content/gdrive/My Drive/Data_Augmented_COVID_CTModelNonCovid/Generator')\n",
        "discriminator = keras.models.load_model('/content/gdrive/My Drive/Data_Augmented_COVID_CTModelNonCovid/Discriminator/')\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=256)\n",
        "gan.compile(\n",
        "    discriminator,\n",
        "    generator,\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "generated_batch = 0\n",
        "generator.summary\n",
        "for i in range(270):\n",
        "  random_latent_vectors = tf.random.normal(shape=(10, 256))\n",
        "  generated_images = gan.generator(random_latent_vectors)\n",
        "  generated_images *= 255\n",
        "  generated_images.numpy()\n",
        "  imageFolder = 0\n",
        "  for j in range(10):\n",
        "    img = tf.keras.preprocessing.image.array_to_img(generated_images[j])\n",
        "    img.save('/content/gdrive/My Drive/NonCOVIDCTAugmentedDCGANExtensiveFinal' + '/' + \"generated_img_%03d_%d.png\" % (generated_batch,j))\n",
        "  generated_batch += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNqpypNs0SdL",
        "outputId": "7305b308-567a-4447-c7c7-87d7631523c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmented CNN Models\n"
      ],
      "metadata": {
        "id": "-ZCgkUjn4Cok"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFyypaNjysKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}